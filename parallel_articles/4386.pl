Gigabajt
Gigabajt ( skrót GB ) lub gibibajt ( skrót GiB ) – jednostki używane w informatyce oznaczające odpowiednio miliard ( 1 000 000 000 = 109 ) bajtów i 230 , czyli 1 073 741 824 = 10243 bajtów .
Stosowana m.in .
do określania pojemności dużych pamięci masowych .
Współczesne dyski twarde posiadają pojemność liczoną w setkach , a nawet tysiącach gigabajtów .
Zgodnie ze standardem IEC 60027-2 w systemie przedrostków dwójkowych ( binarnych ) obowiązują zależności :
1 GiB = 1024 MiB = 1024 · 1024 KiB = 1024 · 1024 · 1024 B
natomiast w systemie przedrostków dziesiętnych SI :
1 GB = 1000 MB = 1000 · 1000 kB = 1000 · 1000 · 1000 B
Jednak w informatyce przedrostek `` giga '' oznacza często liczbę 10243 = 1 073 741 824 , która jest wynikiem działania 230 ( w odróżnieniu do układu SI , gdzie : 109 = 1 000 000 000 ) .
Formalnie , aby zapewnić jednoznaczność , powinien być używany przedrostek `` gibi '' , który oznacza właśnie 1024 × 1024 × 1024 , a jednostka nosi nazwę `` gibibajt '' i ma skrót `` GiB '' , co zostało zaproponowane przez IEC – w praktyce jednak utarła się nazwa standardu JEDEC .
Ta niekonsekwencja jest często przyczyną nadużyć producentów nośników pamięci czy urządzeń , które je wykorzystują .
Producent może przykładowo podać w specyfikacji , że urządzenie ma pojemność 1 GB , co można odczytać jako 1 073 741 824 bajty , w rzeczywistości zaś produkt ma tylko 1 000 000 000 bajtów , czyli o ok. 70 MiB mniej ; mimo to zgodnie z wartością określoną przez przedrostek SI produkt ma właściwą pojemność .
