<doc id="1454" url="https://pl.wikipedia.org/wiki?curid=1454" title="Eutanazja">
Eutanazja

Eutanazja (od gr. εὐθανασία, "euthanasia" – „dobra śmierć”) – pozbawienie życia osoby nieuleczalnie chorej i cierpiącej, na żądanie jej lub jej najbliższej rodziny. W niektórych kodeksach karnych eutanazję określa się jako rodzaj zabójstwa (w tym w polskim jako "zabójstwo na żądanie i pod wpływem współczucia"). Od połowy XX w. eutanazja jest omawiana w kontekście nauki biotechnologii, bioetyki, prawa, polityki i religii. Termin "eutanazja" stosuje się także w przypadku .
Etymologia słowa i historia.
Pojęcie „eutanazja” po raz pierwszy pojawiło się najprawdopodobniej w V w. p.n.e., w komedii Kratinosa o nieustalonym tytule. Określił on w ten sposób „osobę mającą dobrą śmierć”, nie wyjaśniając sensu tego terminu. Po raz kolejny pojęcie to zostało użyte pod koniec IV w. p.n.e. przez innego greckiego poetę – Menandra. Drugie znaczenie nadane terminowi "euthanatos", określało „łatwą śmierć”, będącą efektem posiadania dystansu do własnego życia.
Także w kulturze Starożytnego Rzymu pojęcie „eutanazji” było obecne. Swetoniusz w "Żywotach cezarów" zawarł opis śmierci cesarza Oktawiana Augusta, który chciał umrzeć w spokoju, bezboleśnie, szybko, a nade wszystko świadomie, tak by móc uporządkować swoje sprawy, co było mu dane. Zatem w powyższych znaczeniach, na początku swojej historii, termin „eutanazja” oznaczał śmierć naturalną, ale w takim stanie zdrowia, by nie była ona powolnym umieraniem połączonym z cierpieniem.
W średniowieczu dominował typ eutanazji samobójczej lub zabójczej. Używano mizerykordii do skracania życia śmiertelnie rannym rycerzom, przeciwnikom, jak również przyjaciołom.
Podział.
Eutanazja jest dzielona na bierną (ortotanazja) i czynną (zabójstwo z litości). Eutanazją również mylnie bywała nazywana eksterminacja osób niepełnosprawnych przez nazistów podczas II wojny światowej (akcja T4), a współcześnie również matanazja.
Dopuszczalność eutanazji jest trudnym zagadnieniem etycznym. Ma ona zarówno zwolenników, jak i przeciwników. Konflikt racji bierze się z różnych systemów wartości, jakimi kierują się obie strony sporu. Przeciwnicy eutanazji uważają życie za święty dar od Boga (głównie przeciwnicy eutanazji czynnej) albo uznają je za najwyższą wartość. Zwolennicy natomiast twierdzą, że ważniejsze są uszanowanie woli chorego, uchronienie go od cierpień oraz jego prawo do zachowania godności w rozumieniu, jakie on przyjmuje.
Uregulowania prawne na świecie.
Holandia.
Eutanazja oraz samobójstwa podejmowane z pomocą lekarzy (ang. "physician-assisted suicide") są otwarcie praktykowane w Holandii od 1973 roku, tj. od kiedy w sprawie lekarki, Geertruidy Postmy, która odebrała życie swojej ciężko chorej matce na jej prośbę za pomocą śmiertelnej dawki morfiny, holenderskie sądownictwo wydało wyrok jednego tygodnia więzienia w zawieszeniu na rok. Po tym incydencie wielu holenderskich lekarzy podpisało list otwarty do ministra sprawiedliwości, w którym deklarowali, że są winni takiego samego „przestępstwa”. Wcześniej Holenderskie Stowarzyszenie Lekarzy sprzeciwiało się samobójstwom z pomocą lekarzy, jednak ww. wydarzenia skłoniły przedstawicieli owego stowarzyszenia do zwrócenia się do sądów o rozstrzygnięcie tego, który z lekarskich obowiązków jest nadrzędny: obowiązek nieodbierania życia czy obowiązek redukowania cierpienia poprzez skracanie życia pacjentom nieuleczalnie chorym.
W 1984 roku Holenderski Sąd Najwyższy wydał orzeczenie, wskutek którego przestano wszczynać postępowania w sprawach o samobójstwa popełnione z pomocą lekarza oraz w sprawach o dobrowolną eutanazję (ang. "voluntary euthanasia"), o ile standardy postępowania medycznego były zachowane. Według owych standardów, doprecyzowanych również w 1984 roku, obie formy zakończenia życia są dopuszczalne w przypadku pacjentów nieuleczalnie chorych, w przypadku których wyczerpano możliwości redukowania ich cierpienia (włączając w to cierpienie psychiczne) do poziomu akceptowalnego przez pacjentów, oraz którzy są zdolni do podejmowania racjonalnych decyzji i wielokrotnie powtarzają prośbę o odebranie im życia, a przy tym są wolni od jakichkolwiek nacisków. Istnieje też obowiązek rejestrowania wszystkich takich przypadków.
Celem zminimalizowania ryzyka nadużyć, w 1993 roku wydano zalecenia dla lekarzy, aby w przypadku takich pacjentów optowali za metodą, w ramach której pacjent sam aplikuje sobie śmiertelną dawkę lekarstwa, zamiast podania jej przez lekarza. Co więcej, zalecono, aby lekarz pomagający w skróceniu życia, wcześniej zasięgnął niezależnej opinii innego lekarza. Pomimo wydania szczegółowych standardów, kwestie te budziły kontrowersje u części holenderskich lekarzy. Jednak zarówno około 80% lekarzy, jak i 80% społeczeństwa holenderskiego popierało owe rozwiązania. Sytuacje kierowania oskarżenia przeciwko medykom z tych powodów należały w Holandii do rzadkości, zaś tylko w jednym przypadku doszło do skazania: w 1986 roku na karę 6 miesięcy więzienia skazano pielęgniarkę, która odebrała życie kilku pacjentom z chorobą nowotworową i zaburzeniami psychicznymi, którzy nie wyrazili zgody na eutanazję (ang. "without their consent").
Dla zobrazowania skali zjawiska można posłużyć się danymi z 1991 roku z Holandii, gdzie łączna liczba zgonów z różnych przyczyn wyniosła 128 tys. W tym samym roku 25 tys. pacjentów podjęło ze swoim lekarzem rozmowę na temat eutanazji, z tego 9 tys. pacjentów jednoznacznie prosiło o eutanazję, z których ostatecznie – po medycznej weryfikacji ich stanu – około 2300 osób odebrało sobie życie z pomocą lekarza lub zakończyło życie w wyniku eutanazji (stanowi to 1,8% ogółu zgonów odnotowanych w 1991 r. w Holandii). Oznacza to, że 26% próśb pacjentów o skrócenie ich życia zostało spełnionych poprzez samobójstwa podejmowane z pomocą lekarzy albo poprzez dobrowolną eutanazję. Taki stan rzeczy utrzymywał się ponad 20 lat, choć formalnie eutanazja była nielegalna i do 1994 roku jej przeprowadzenie skutkowało przesłuchaniem policyjnym i groźbą postawienia w stan oskarżenia. Sugeruje się, że taki stan prawny mógł być przyczyną niezgłaszania wszystkich rzeczywistych przypadków eutanazji. Szacuje się, że rocznie w Holandii w przypadku około 400 terminalnie chorych, którzy nie są w stanie wyrazić swojej woli ze względu na zaburzenia stanu psychicznego towarzyszące nieuleczalnej chorobie, holenderscy lekarze (głównie podstawowej opieki zdrowotnej, ang. "general practitioners") sugerowali skrócenie życia. W 83% tych przypadków konsultowano się z rodziną pacjentów. W przypadku części takich pacjentów została przeprowadzona eutanazja. Przyjmuje się, że przypadki eutanazji wbrew woli terminalnie chorych nie miały miejsca w Holandii lub są zjawiskiem marginalnym i z tego powodu nie zostały odnotowane.
Poza przypadkami eutanazji i samobójstw przy pomocy lekarzy (podobnie jak w innych krajach Europy, w tym w Polsce) odnotowuje się także wiele przypadków powstrzymania się od uporczywej terapii osób terminalnie chorych – w Holandii dotyczy to około 20 tys. osób rocznie, przy czym około 87% z nich nie jest w stanie wyrazić swojej woli, zaś około 13% było w stanie wyrazić swoją wolę, ale rozmowa na temat zaprzestania uporczywej terapii nie została przeprowadzona. Mając na uwadze cierpienie pacjentów, wielu holenderskich lekarzy uznaje procedurę „zaprzestawania utrzymywania pacjenta przy życiu” (ang. "withholding of life-support") i „pozwalania na śmierć” za okrutną i nieakceptowalną.
Od 1 kwietnia 2002 r. w Holandii obowiązuje "Ustawa o zakończeniu życia na żądanie i pomocnictwie w samobójstwie "(ang. "Termination of Life on Request and Assisted Suicide") Jednocześnie znowelizowano Kodeks karny. Akty te łącznie określają warunki wyłączenia karalności sprawcy eutanazji (definiowanej jako odebranie życia na żądanie) i pomocnictwa w samobójstwie.
Art. 2. § 1. "Ustawy o zakończeniu życia na żądanie i pomocnictwie w samobójstwie" podaje 6 kryteriów ostrożności, które holenderski lekarz kończący życie drugiej osoby lub pomagający jej w samobójstwie musi spełnić:
Pacjent poddawany eutanazji musi mieć co najmniej 12 lat (pacjenci w wieku od 12 do 16 lat muszą otrzymać zgodę rodziców). Ustawodawstwo holenderskie uznaje ważność pisemnego oświadczenia woli pacjenta. Oświadczenie takie może być użyte, kiedy pacjent znajdzie się w śpiączce lub w innym stanie, który uniemożliwia wyrażenie zgody na eutanazję.
Luksemburg.
W Luksemburgu 20 lutego 2008 roku parlament przyjął większością 30 z 59 możliwych głosów ustawę legalizującą skracanie na żądanie życia osobom ciężko chorym. Weszła ona w życie po drugim głosowaniu w marcu 2009. Ustawa została silnie skrytykowana przez Kościół katolicki (mający duży wpływ na społeczeństwo luksemburskie), większą część środowisk medycznych i rządzącą Partię Chrześcijańsko-Społeczną. Zwolennicy nowej ustawy argumentowali, że jej wprowadzenie zakończy wyjazdy mieszkańców Luksemburga „po śmierć” do Szwajcarii, gdzie istnieją organizacje legalnie pomagające popełnić samobójstwo.
Zgodnie z projektem decyzję o eutanazji może podjąć wyłącznie pacjent, pod warunkiem że jest "ciężko i nieuleczalnie chory". Decyzję o eutanazji można będzie podjąć, m.in. spisując testament. Eutanazja w Luksemburgu miałaby być dopuszczalna jedynie wobec osób terminalnie lub nieuleczalnie chorych i to tylko pod warunkiem, że sam chory wielokrotnie by się jej domagał. Wymagana byłaby także zgoda dwóch lekarzy i konsylium ekspertów, a każdy przypadek byłby skrupulatnie sprawdzany przez komisję złożoną z lekarzy i przedstawicieli społeczeństwa.
Belgia.
W Belgii parlament przyjął we wrześniu 2002 roku ustawę legalizującą eutanazję, opartą na modelu holenderskim. Dnia 13 lutego 2014 roku Parlament Belgii przyjął ustawę o eutanazji dla nieletnich bez ograniczeń wiekowych. W 2015 roku w Belgii przyznano prawo do eutanazji 24-latce cierpiącej z powodu depresji.
Wraz z rozwojem ustawodawstwa belgijskiego w latach sześćdziesiątych XX wieku narodził się pomysł zmiany regulacji prawnej dotyczącej eutanazji, który został jednak kontynuowany dopiero w latach osiemdziesiątych. Dyskusja nad zmianą miała na celu wyłączenie karalności spowodowania śmierci pacjenta przez lekarza w sytuacji, gdy chory żądał przerwania dalszego leczenia. Koncepcja ta nie została zrealizowana. Do roku 2002 ustawodawstwo belgijskie powoływało się na przepisy kodeksu karnego z 1867 roku, którego treść nie przewidywała uznania zabójstwa eutanatycznego jako uprzywilejowanej formy przestępstwa. Sprowadzała się natomiast do twierdzenia, że umyślne spowodowanie śmierci pacjenta dokonane na jego żądanie to zabójstwo. W szczególnych zaś przypadkach dokonanie owego czynu mogło przybrać nawet formę szczególnego typu zabójstwa.
Przełomowym momentem w kwestii eutanazji było odsunięcie od władzy w wyniku wyborów w 1999 roku flamandzkich i walońskich partii chrześcijańsko-demokratycznych. Wraz z pojawieniem się nowego rządu, składającego się z koalicji socjalistów, liberałów oraz partii ekologicznych, 16 maja 2002 roku uchwalono ostatecznie Ustawę dotyczącą eutanazji. 28 maja 2002 roku opublikowana została w belgijskim dzienniku ustaw, a weszła w życie trzy miesiące później.
W myśl artykułu 2 Ustawy „eutanazja” rozumiana była jako czyn osoby trzeciej, która umyślnie doprowadza do śmierci innego człowieka na jego prośbę. Ustawa ta nie reguluje w sposób dokładny kwestii dotyczących eutanatycznej pomocy w samobójstwie, gdyż nie stanowi ono w świetle prawa belgijskiego czynu zabronionego. Zgodnie z artykułem 3 wyżej wspomnianej ustawy lekarz dokonujący eutanazji, chcąc aby jego czyn nie został zakwalifikowany jako przestępstwo, powinien upewnić się co do określonych warunków, w szczególności, że:
1) pacjent jest osobą pełnoletnią lub osobą małoletnią uznaną za pełnoletnią, która do momentu sformułowania prośby była przytomna i posiadała zdolność do czynności prawnych;
2) pacjent znajduje się w położeniu beznadziejnym, z medycznego punktu widzenia, powstałym na skutek nieuleczalnej choroby lub nieszczęśliwego wypadku;
3) prośba chorego została podjęta dobrowolnie, w przemyślany sposób i co najmniej dwukrotnie została powtórzona;
4) pacjent skarży się na przewlekłe, cielesne lub psychiczne cierpienia, które są nie do zniesienia i których nie można uśmierzyć;
5) przestrzegał przepisanych przez Ustawę warunków oraz sposobów postępowania.
Ponadto, lekarz, chcący dokonać eutanazji, musi spełnić dodatkowe warunki zawarte w artykule 2 Ustawy, między innymi:
1) powinien poinformować pacjenta o stanie jego zdrowia i pozostałym mu okresie życia;
2) powinien omówić z chorym dostępne jeszcze inne sposoby leczenia, które oferuje opieka paliatywna.
Eutanazja zostaje dopuszczona w momencie, gdy lekarz wraz z pacjentem osiągnął konsensus co do położenia, w jakim znajduje się chora osoba. Istotną kwestią jest upewnienie się przez lekarza, że cierpienie zarówno fizyczne, jak i psychiczne jego podopiecznego trwają bezustannie. Ponadto lekarz ma obowiązek skonsultować tę kwestię z innym specjalistą w danej dziedzinie schorzeń, aby i on potwierdził nieuleczalność choroby. Po zakończonej konsultacji pacjent powinien zostać poinformowany o wynikach. W sytuacji, gdy osoba chora pozostaje pod opieką większej grupy osób, to na lekarzu spoczywa obowiązek kontaktowania się z wybranymi osobami, w celu przeprowadzenia wywiadu środowiskowego na temat podopiecznego. Przy zabiegu eutanazji dopuszczalne jest uczestnictwo osób z grona rodziny chorego na jego życzenie. Między decyzją pacjenta o poddaniu się eutanazji a jej wykonaniem musi upłynąć stosowny czas – w tym wypadku minimum jeden miesiąc. Pacjent zobowiązany jest udzielić prośby o dokonanie eutanazji w formie pisemnej. Dokument ten powinien być sporządzony własnoręcznie, opatrzony podpisem oraz datą. W momencie, gdy czynność ta nie może zostać zrealizowana przez samego chorego, wyręczyć go może osoba pełnoletnia, wskazana przez pacjenta. Ważne jest, aby osoba ta nie miała w tym żadnego interesu materialnego oraz aby w czasie sporządzania dokumentu uczestniczył przy tym lekarz. Prośbę tę w postaci dokumentu dołącza się do jego obecnej dokumentacji medycznej. Spisana prośba w każdej chwili może ulec wycofaniu, a tym samym odłączeniu od dokumentacji i zwróceniu choremu.
Ustawa o eutanazji reguluje także kwestię pisemnej deklaracji ujawniającej wolę osoby do przeprowadzenia na niej eutanazji, określanej jako „przedłożone oświadczenie woli”. Składana zostaje w razie zaistnienia sytuacji, kiedy osoba będąc w stanie nieprzytomności nie byłaby zdolna wyrazić swojej woli. Zgodnie z artykułem 4 Ustawy o eutanazji, oświadczenie to może złożyć osoba pełnoletnia lub małoletnia, która prawnie została uznana za pełnoletnią. Warunkiem sine quo non jest to, żeby w momencie złożenia oświadczenia woli posiadała ona zdolność do czynności prawnych. Aby dokonana eutanazja nie stanowiła przestępstwa, lekarz musi stwierdzić, że:
 * Pacjent znajduje się w stanie nieuleczalnego schorzenia, będącego następstwem nieszczęśliwego wypadku lub choroby;
 * Pacjent nie wróci już nigdy do pełnej świadomości;
 * Sytuacja ta jest nieodwracalna.
Artykuł 15 Ustawy przedstawia skutki cywilnoprawne dokonania eutanazji. Wśród nich możemy wskazać, że zastosowanie eutanazji, które zostało przeprowadzone zgodnie z warunkami na osobie, która w jej następstwie zmarła, należy traktować jako wykonanie umowy. Według tego przepisu śmierć tego typu, można uznać za śmierć naturalną.
Lekarz, który dokonał eutanazji, składa w Komisji Rejestracyjno- Kontrolnej stosownie wypełniony dokument rejestracyjny w przeciągu czterech dni od jej przeprowadzenia. Dokument ten jest dwuczęściowy. Pierwsza część ma charakter poufny, a zapoznanie się z jej treścią możliwe jest tylko na podstawie postanowienia Komisji. Wskazana część zawiera między innymi:
 * Imię, nazwisko oraz adres pacjenta;
 * Imię, nazwisko, numer rejestracyjny oraz adres lekarza prowadzącego;
 * Imię, nazwisko, numer rejestracyjny, miejsce zamieszkania lekarzy, którzy brali udział w konsultacjach odnośnie do dokonania eutanazji;
 * Imię, nazwisko, adres innych osób, które zostały poproszone przez lekarza prowadzącego leczenie o konsultacje oraz datę tej konsultacji;
Druga część dokumentu składa się z:
 * Płeć, data, miejsce urodzin pacjenta;
 * Data, miejsce, godzina jego śmierci;
 * Ustalenie, że schorzenie było nieuleczalne;
 * Stwierdzenie, że chory doświadczał długotrwałego cierpienia;
 * Przyczyny uniemożliwiające uśmierzenie bólu;
 * Okoliczności, na podstawie których lekarze mieli pewność, że prośba została sformułowana w sposób prawidłowy – przemyślany, bezprzymusowy, dobrowolny, dwukrotnie powtórzony;
 * Informacja o wystawieniu diagnozy, iż śmierć nastąpiłaby w krótkim czasie;
 * Informacja, o tym iż eutanazja nastąpiła poprzez oświadczenie woli pacjenta;
 * Postępowanie lekarza;
 * Specjalizacja lekarza, treść wydanej opinii, data odbycia konsultacji lekarskich;
 * Sposób i rodzaj dokonania eutanazji;
Zadaniem Komisji jest zbadanie realizacji przesłanek wskazanych wyżej w dokumencie rejestracyjnym. W sytuacji, gdy nastąpi wątpliwość Komisja może postanowić zwykłą większością głosów o zniesieniu anonimowości dokumentu. Rozstrzygnięcie danego przypadku podjęte zostaje w ciągu dwóch miesięcy. Jeżeli członkowie Komisji uznają, że warunki konieczne do dokonania eutanazji nie zostały spełnione, przesyłają akta sprawy właściwemu prokuratorowi. Prokurator ten może wnieść przeciwko lekarzowi prowadzącemu akt oskarżenia o dokonanie zabójstwa.
Opublikowane dane statystyczne wskazują, że od 23 września – czyli od momentu wejścia ustawy w życie – do początku 2004 roku zostało przeprowadzonych 259 zabiegów eutanazji w Belgii. Według opinii Wima Distelmansa – specjalisty lekarza onkologa, przewodniczącego Komisji Rejestracyjno-Kontrolnej liczba przypadków dokonania eutanazji, zarówno legalnej, jak i nielegalnej, nie powinna przekraczać tysiąca przypadków rocznie. Odpowiada ona ilości „ciemnej liczby” zabiegów eutanatycznych dokonywanych przed legalizacją eutanazji w 2002 roku.
Niemcy.
25 czerwca 2010 roku Niemiecki Trybunał Federalny wydał orzeczenie, z którego wynika, że wspomagane samobójstwo jest legalne, jeśli pacjent wyraźnie zażyczył sobie zakończenia sztucznego podtrzymywania przy życiu. Trybunał podkreślił, że nadal nielegalna pozostaje aktywna pomoc w samobójstwie.
Polska.
Eutanazja czynna w polskim kodeksie karnym określona jest jako występująca zawsze pod wpływem współczucia dla cierpiącej osoby i na jej żądanie. Jest ona zabroniona; traktuje się ją jak rodzaj zabójstwa karanego w łagodniejszy sposób. Osoba jej dokonująca jest obecnie zagrożona karą pozbawienia wolności od 3 miesięcy do 5 lat. Wyjątkowo sąd może zastosować złagodzenie kary, a nawet odstąpić od jej wymierzenia. Przepis art. 150 kodeksu karnego, który określa przestępstwo zabójstwa eutanatycznego, nie wymaga natomiast wprost, aby osoba żądająca go była śmiertelne chora, jednak wymóg taki wprowadza orzecznictwo. Specyficznym typem przestępstwa związanego z szeroko pojętą eutanazją jest pomoc do samobójstwa (art. 151 kodeksu karnego), która obejmuje także tzw. eutanatyczną pomoc do samobójstwa, czyli ułatwienie osobie śmiertelnie chorej samobójstwa.
Eutanazja zwierząt.
W Polsce jedyną osobą uprawnioną do przeprowadzania eutanazji jest lekarz weterynarii, który może ją wykonać jeżeli wszystkie inne metody leczenia zwierzęcia zawiodą. Eutanazja zwierząt na życzenie jest nielegalna. Zabieg ten jest powszechnie wykonywany, rozumiany jako akt humanitarnego uśmiercenia z ograniczeniem bólu, strachu i dyskomfortu zwierzęcia do minimum. Środki farmakologiczne wykorzystywane do eutanazji ograniczają świadomość zwierzęcia, niwelują niepokój i ból wyzwalający reakcje obronne. Do wykonania zabiegu uśmiercenia lekarz stosuje zastrzyk pentobarbitalu lub embutramidu w preparatach złożonych. Posiadanie i stosowanie wyżej wymienionych środków podlega ustawie o przeciwdziałaniu narkomanii i wiąże się z koniecznością uzyskania odpowiednich pozwoleń. Na środki eutanazyjne lekarz weterynarii wypisuje receptę ze specjalnym numerem identyfikującym.
Opinia społeczna.
Większość Hiszpanów, Czechów i Szkotów popiera zalegalizowanie eutanazji. Według sondy telefonicznej z 2009 roku 48% Polaków, a według sondy telewizyjnej z tego samego roku 77%, jest za prawem do eutanazji. W 2013 roku 65% Polaków było za zaprzestaniem podtrzymywania życia nieprzytomnych pacjentów z uszkodzonym mózgiem, a 53% za przyspieszeniem śmierci nieuleczalnie chorych na ich prośbę.
Debata o legalności eutanazji.
W coraz większej liczbie periodyków medycznych (w tym „Journal of the American Medical Association”, „New England Journal of Medicine” czy „The British Medical Journal”) ukazują się prace popierające eutanazję. 60% francuskich lekarzy popiera jej legalizację.
Według zwolenników legalizacji, eutanazja rzekomo nie skutkuje większą liczbą przyspieszania zgonów na życzenie. Jednak od 2002 roku w Belgii wykonuje się ok. 1,4 tys. zabiegów eutanazji rocznie, a w 2013 r. wykonano ich już 1,8 tys. Natomiast w roku 2016 wykonano ich aż 6091. Średni wiek osób poddających się eutanazji to 70 lat, co oznacza, że nie jest ona wymuszana na osobach starszych (więcej niż 80 lat), czyli będących większym kłopotem dla ich rodzin i służby zdrowia. Państwo legalizując eutanazję może zapewnić jej bezpieczeństwo i dobrowolność poprzez:
Przeciwnicy eutanazji podają, że diagnozy, na podstawie których podejmuje się decyzję o niej, mogą być błędne.
Doktryny religijne.
Judaizm.
Żydowskie prawo odrzuca wszelkie środki, które zmierzają do skrócenia ludzkiego życia. Mimo to dyskutuje się nad pewnymi wyjątkami, np.: gdy ludzie są nieuleczalnie chorzy i dzięki eutanazji można by uniknąć nadmiernych cierpień. Midrasz nawiązuje do historii króla Saula, który rzucił się na swój miecz, aby uniknąć tortur. Średniowieczne autorytety debatowały nad kwestią, czy dozwolona jest modlitwa o szybką śmierć nieuleczalnie chorego. Modlitwę taką ocenia się zasadniczo pozytywnie, ale zakazuje się bezpośredniej pomocy w umieraniu.
Współczesne autorytety rabinackie zezwalają na podawanie środków uśmierzających ból, chociaż w znacznym stopniu skracają one życie śmiertelnie choremu. Zakazują jednak iniekcji czy podawania środków, które przyspieszają śmierć. Tak samo jednak odrzucają medyczne interwencje, które sztucznie przedłużają życie.
Chrześcijaństwo.
Etyka katolicka stanowczo odrzuca eutanazję bezpośrednią, o czym mówi paragraf 2277 Katechizmu Kościoła katolickiego: „Eutanazja bezpośrednia, niezależnie od motywów i środków polega na położeniu kresu życiu osób upośledzonych, chorych lub umierających. Jest ona moralnie niedopuszczalna”.
W 1995 papież Jan Paweł II w swojej encyklice „Evangelium Vitae” ponownie odrzucił eutanazję bezpośrednią, lecz oddzielił od niej decyzję o rezygnacji z „uporczywej terapii” („pewne zabiegi medyczne, które przestały być adekwatne do realnej sytuacji chorego, ponieważ nie są już współmierne do rezultatów, jakich można by oczekiwać, lub też są zbyt uciążliwe dla samego chorego i dla jego rodziny”). Taką decyzję o zaprzestaniu terapii papież wyraźnie oddzielił od eutanazji i samobójstwa.
Analogiczne rozważania podejmuje się w ramach etyki prawosławnej i protestanckiej. Wspólnota Kościołów Ewangelickich w Europie opowiada się za ochroną praw osób śmiertelnie chorych i umierających, podkreślając zarówno prawo do życia do samego końca, jak i do rezygnacji z uporczywej terapii. Wspólnota odrzuca teologiczno-etyczne usprawiedliwienie eutanazji oraz asystencji przy samobójstwie. Stoi też na stanowisku, że eutanazja nie jest wyłącznie kwestią indywidualnego sumienia, a legalizacja prowadzi do uznania jej za normalną praktykę medyczną.
Świadkowie Jehowy życie uważają za cenny dar od Boga i dlatego niedopuszczalne ich zdaniem jest rozmyślne odebranie życia przez eutanazję.
Islam.
Na Pierwszej Międzynarodowej Konferencji Medycyny Muzułmańskiej (Kuwejt, 1981) zostały potępione samobójstwo i eutanazja. Zgromadzenie to opowiedziało się także za rezygnacją z metod sztucznego przedłużania życia.
Buddyzm.
Buddyzm nie aprobuje eutanazji zarówno ze względu na swą naukę o karmie, jak też z punktu widzenia psychologii. Zła karma, która warunkuje cierpienia człowieka (chorego), będzie tak długo trwać, dopóki się nie wyczerpie i nie przestanie wpływać na kolejne narodziny. Jeśli się przerwie cierpienie, zacznie się ono na nowo w kolejnym wcieleniu, aż do wyczerpania. Jeśli się jednak zniesie cierpienie jako rezultat złej, negatywnej karmy, można się ponownie odrodzić w lepszej egzystencji.
Buddyjska psychologia wychodzi z faktu, że także śmierć zadana ze współczucia przepełniona jest nienawiścią i negatywnymi uczuciami do cierpień pacjenta. Nawet jeśli pierwotnym motywem jest uśmierzenie chwilowego cierpienia, dobra intencja przeobraża się w momencie decyzji w działanie nacechowane odrazą. Chociaż lekarz sądzi, że uśmierca z litości, to jednak w rzeczywistości działa z odrazy do cierpienia. Tym samym wzbudza on dla siebie i dla swego pacjenta negatywną energię karmiczną.
Lekarze buddyjscy dyskutują również na temat dokładnego momentu śmierci, tj. kiedy można wyłączyć aparaturę, która jedynie sztucznie przedłuża życie, gdy tymczasem jest ona potrzebna innym pacjentom. Bardzo ważne jest w tym kontekście pojęcie prany ("prana" – tchnienie życia). Pojawia się ono już w upaniszadach i oznacza „siłę życiową” człowieka, tkwiącą w sercu. Gdy zaniknie, lekarzowi wolno zaniechać swych wysiłków.
Hinduizm.
Eutanazja jest postrzegana jako śmierć z wyboru i aprobowana jako ucieczka od przywiązania do własnego „ja”, które cierpi (np. z powodu starości); daje możliwość uwolnienia się od niewygodnej formy życia i przejścia do następnego życia (reinkarnacja). Ci, którzy podejmują decyzję samobójczą, jak i ci, dokonują eutanazji, winni się uwolnić od złych zamierzeń, namiętności i egoizmu.

</doc>
<doc id="1455" url="https://pl.wikipedia.org/wiki?curid=1455" title="Ekspozycja (fotografia)">
Ekspozycja (fotografia)

Ekspozycja – ilość światła padającego na materiał światłoczuły (np. film lub sensor elektroniczny) konieczna dla prawidłowego zrobienia zdjęcia fotograficznego. Uzyskuje się ją poprzez ustalenie wartości przysłony i czasu naświetlania w stosunku do wybranej czułości filmu (lub jego odpowiednika w aparacie cyfrowym) lub też czułości materiału odbitkowego (papier fotograficzny, kopia diapozytywowa), a także poprzez odpowiednie dozowanie oświetlenia (lampy błyskowe, lampy o świetle ciągłym, odbłyśniki, ekrany, żaluzje, rozpraszacze światła itp.).
Rodzaje ekspozycji w przypadku pracy z urządzeniem rejestrującym obraz (np. aparat fotograficzny, kamera filmowa):
W bardziej zaawansowanych aparatach istnieje możliwość ręcznego korygowania ustawionej przez automatykę wielkości ekspozycji (jest to tak zwana kompensacja ekspozycji).

</doc>
<doc id="1456" url="https://pl.wikipedia.org/wiki?curid=1456" title="Władcy Egiptu">
Władcy Egiptu

Władcy Egiptu – chronologiczny wykaz władców starożytnego, średniowiecznego i nowożytnego Egiptu.
Egipt Starożytny.
Informacje i podział władców egipskich na dynastie zawdzięczamy Manethonowi, zhellenizowanemu egipskiemu kapłanowi z Heliopolis. Jego pisma zaginęły, a kopie obarczone są licznymi błędami. Prace nad opisaniem historii starożytnego Egiptu zaczął prawdopodobnie na polecenie Ptolemeusza I. Ustalenie kolejności i dat panowania władców Egiptu wciąż wywołuje wiele sporów. W poniższych tabelach zastosowano zmodyfikowane datowanie z "Cambridge Ancient History".
Okres predynastyczny i protodynastyczny – ok. 5500–3150 p.n.e..
Do tej grupy zaliczani są bezimienni właściciele elitarnych grobów w Nechen, które zostały odkryte i przebadane w l. 1997–2011. Badania cmentarza HK6 trwają nadal, dlatego można się spodziewać kolejnych „władców” z tej najwcześniejszej „dynastii”. Wodzowie ci panowali nad miastem Nechen i najbliższą okolicą.
Nazwa „Dynastia 00” nie jest powszechnie uznana przez egiptologów. Ma ona obejmować niespokrewnionych ze sobą, bezimiennych królów lokalnych z okresu od Nagada IIC do IIIA2.
Zalicza się do niej właściciela „malowanego grobu” nr 100 na cmentarzu elity HK31 w Nechen (ok. 3500 p.n.e., Nagada IIC), władców z cmentarza T w Nubt (Nagadzie) oraz władców z cmentarza U w Abydos.

</doc>
<doc id="1457" url="https://pl.wikipedia.org/wiki?curid=1457" title="Emacs">
Emacs

Emacs – rodzina edytorów tekstu, znana ze swojej rozszerzalności. Podręcznik najpopularniejszego wariantu GNU Emacs opisuje go jako „rozszerzalny, dostosowywalny, samodokumentujący się, edytor wyświetlany w czasie rzeczywistym”. Rozwój pierwszych Emacsów rozpoczął się w połowie lat siedemdziesiątych, a prace nad GNU Emacsem są kontynuowane (2022).
Wprowadzenie.
Pierwotna wersja Emacsa została napisana w 1976 roku przez programistę z MIT, Richarda Stallmana, jako zestaw makr dla innego edytora o nazwie TECO (nazwa wzięła się od słów i ). W roku 1984 Stallman zaczął pisać nową wersję Emacsa, GNU Emacs, który stał się pierwszym programem projektu GNU. W rok później ukazała się wersja GNU Emacs 15.34, która była już oficjalnie dystrybuowana. 23 lutego 2008 roku Richard Stallman napisał na emacs-devel, iż (po około 20 latach opieki nad programem) chciałby powierzyć projekt Stefanowi Monnier i Chongowi Yidong.
Emacs jest przykładem wolnego oprogramowania, a nie jedynie oprogramowania open source.
Emacs składa się z niewielkiego i wydajnego jądra napisanego w C, zawierającego też interpreter dialektu Lispu zwanego Emacs Lisp, oraz z ogromnej nadbudowy napisanej w Lispie i wykonywanej przez to jądro.
Dzięki takiej konstrukcji Emacs jest elastyczny, a jego zachowanie można w pełni kontrolować przy użyciu Emacs Lispa.
Rozszerzeniami typowymi dla edytorów programisty, są podświetlanie i automatyczne formatowanie kodu źródłowego, oraz integracja z make, systemami kontroli wersji i kompilatorami.
Mniej typowe zastosowania to przeglądanie katalogów, obsługa urządzeń typu modemy,
aż do zupełnie nietypowych, jak: graficzna przeglądarka stron WWW, klient poczty elektronicznej, gry komputerowe czy implementacja ELIZY.
Istnieją dwie wersje Emacsa wywodzące się z pierwotnego kodu, znane jako GNU Emacs i XEmacs, oraz ogromna liczba edytorów i innych narzędzi wzorowanych na Emacsie.
Dzięki dodatkowym narzędziom etags i ctags istnieje możliwość szybkiego „poruszania” się po dużych projektach programistycznych.
Tryby uruchomieniowe.
GNU Emacs może być uruchamiany w dwóch trybach – tekstowym i graficznym. Uruchamiając Emacsa w Terminalu bez środowiska X uruchomi się tryb tekstowy. Natomiast wewnątrz X Window uruchomi się tryb graficzny. Istnieje także możliwość uruchomienia Emacsa w trybie tekstowym w emulatorze terminala w Interfejsie graficznym. Istnieje także możliwość uruchomienia Emacsa jako serwera, w którym wszystkie klienty współdzielą te same bufory.
Sekwencje i skróty klawiszowe.
W Emacsie każde pojedyncze wciśnięcie klawisza lub sekwencji klawiszy wywołuje funkcję napisaną w języku Emacs Lisp. W większości trybów pojedyncze wciśnięcie klawisza powoduje wywołanie funkcji "self-insert-command", która wstawia pojedynczy znak do bufora. Oprócz podstawowych skrótów klawiszowych istnieją także sekwencję rozpoczynające się od C-x (Control + X) lub C+c. Skrót M-x (znak meta lub alt) umożliwia wywołanie dowolnej funkcji w Emacs Lispie, która została utworzona z możliwością wywołania interaktywnego (wewnątrz funkcji musi być wywołanie codice_1).
Przykładowe skróty klawiszowe.
Poniższa tabela zawiera kilka podstawowych skrótów klawiaturowych i sekwencji oraz ich standardowe wiązania do funkcji w Emacs Lispie.
Tryby Emacsa.
Są to biblioteki programistyczne, które dodają jakąś dodatkową funkcjonalność do edytora, zazwyczaj uruchamiane są automatycznie dla określonego typu pliku. Istnieją dwa typy trybów Emacsa główny (Major) i pomniejszy (Minor). Istnieją tryby główne dla większości języków programowania, które m.in. kolorują składnie, dodają nowe funkcje, nowe kombinacje klawiszy lub inteligentne „wcinanie” kodu źródłowego. Pomniejsze tryby służą zazwyczaj do dodania jakiejś jednej funkcji np. wyświetlanie zegarka, czy liczby wierszy. Dla pojedynczego bufora może być wywołany tylko jeden tryb głównych oraz wiele pomniejszych. Istnieje także tryb polymode, który umożliwia uruchomienie wielu trybów głównych, dla różnych części bufora.
Bufory.
W Emacsie wszystkie operacje wykonywane są na buforach. Bufor jest to zazwyczaj plik na dysku, który można wyświetlić wewnątrz okna, który znajduje się w pamięci Emacsa. Buforem może być też zawartość katalogu, strona internetowa czy inna treść stworzona przez edytor np. gra.
Minibufor.
Jest to obszar na dole okna wewnątrz którego wyświetlane są wyniki działania komend oraz służy do wprowadzania dodatkowych argumentów dla funkcji.
Okna.
Oknem (ang. Window) w Emacsie określa się obszar, w którym wyświetlany jest Bufor. Nie należy go mylić z oknem programu uruchomionego w trybie GUI, na które w Emacsie mówi się „ramka”. Wewnątrz pojedynczej ramki programu można wyświetlać wiele okien z wyświetlanym buforem. Do dzielenia okna w poziomie służy sekwencja C-x 3 (wywołująca funkcję "split-window-horizontally") natomiast do dzielenia w pionie służy sekwencja C-x 2 (funkcja "split-window-vertically"), do usunięcia aktualnego okna służy sekwencja C-x 0 (funkcja "delete-window"). W Emacsie można mieć uruchomione dwa okna wyświetlające ten sam bufor – modyfikacja bufora w jednym oknie natychmiastowo uaktualnia drugi bufor. Okna mogą mieć różny rozmiar. Istnieje możliwość uruchomienia emulatora terminala wewnątrz okna.
Ramki.
Ramką (ang. Frame) określa się okno programu w trybie graficznym, w którym może być wiele okien. W trybie tekstowym wyświetlana jest naraz tylko jedna ramka.
Hooki.
Hooki Emacsa służą do wywoływania kodu użytkownika w odpowiednim momencie, np. gdy uruchomiony jest określony tryb. Przykładowo wewnątrz Hooka dla trybu głównego "lisp-mode" (uruchamianego dla programów w języku Lisp), można utworzyć dodatkowe wiązania funkcji (ang. binding) do skrótów klawiszowych, ułatwiające prace z plikami w tym języku lub uruchomić tryby pomniejsze, które powinny być włączone tylko dla tego trybu głównego.
Etags i ctags.
Są to programy, które generują indeksy definicji wewnątrz plików źródłowych. Po wygenerowania pliku Tags dla danego kodu źródłowego, składającego się z wielu plików, można używać skrótu M-. (Meta lub alt i kropka) do przechodzenia z miejsca gdzie wywołano funkcję do miejsca, w którym ta funkcja została zdefiniowana.

</doc>
<doc id="1459" url="https://pl.wikipedia.org/wiki?curid=1459" title="Emotikon">
Emotikon

Emotikon – ideogram złożony z sekwencji znaków typograficznych, służący do wyrażania nastroju w Internecie. Zwykle przedstawia grymas twarzy, obrócony o 90° w kierunku przeciwnym do ruchu wskazówek zegara. Wiele aplikacji przekształca emotikony w tzw. emoji, które to często są potocznie określane jako "emotikony (obrazkowe)", podczas gdy rzeczywiste emotikony określane są jako "emotikony znakowe".
Pochodzenie słowa.
Słowo to powstało z połączenia angielskich słów "emotion" oraz "icon", jednak nim zaczęto używać bitmap, jako ich substytutu, były tylko ciągiem znaków. Obecnie często rozszerza się ideę emotikonów, wprowadzając grafiki reprezentujące przedmioty i czynności, a nie tylko emocje. Według pierwszego wyjaśnienia polska forma powinna brzmieć „emotikon”, a według drugiego – „emotikona”. Właśnie dlatego to słowo jest używane zamiennie w formie żeńskiej i męskiej. Słownik języka polskiego PWN odnotowuje formę „emotikon”.
Historia.
Znaczek codice_1 został użyty po raz pierwszy 19 września 1982 o godzinie 11:43 przez profesora Scotta Fahlmana z Carnegie Mellon University, jednak sam pomysł typograficznych emotikonów złożonych ze znaków przestankowych pojawił się w kilku artykułach prasowych, opublikowanych przed marcem 1881 r., w tym w "Kurjerze warszawskim" z 5 marca 1881 r.
Przykładowe emotikony.
Większość emotikonów posiada warianty z „noskami” i bez nich. Na przykład emotikon „:-)” (z noskiem) jest równoważny „:)” (bez noska). Ze względu na szybkość pisania, częściej spotyka się emotikony bez nosków. Podobnie sytuacja ma się z języczkami, które występują nawet w kilku odmianach („=”, „-”, a także rzadko spotykany wężyk „=-”). Różnica polega na tym, że język zmienia znaczenie. Trudno jest powiedzieć, w jaki sposób; nie istnieją na to żadne szablony. Dwukropek jest też nieraz zastępowany przez znak równości, średnik lub literę „x”, np. zamiast „:)”, „:/” można napisać „=)”, „ =/”, „;)”, „;/” lub „x)”, „x/”. Często też – zamiast nawiasów okrągłych – używa się nawiasów kwadratowych. W niektórych wariantach emotikonów dwukropek występuje z prawej strony.
Przykłady:
"Kaomoji".
Obecnie popularne są także (szczególnie wśród fanów popkultury japońskiej – "otaku") emotikony zwane "kaomoji". Powstałe w Japonii, do ich stworzenia często wykorzystywane są znaki z kanji, katakany i hiragany. Wyraz powstał z kombinacji słów "kao" (顔 – „twarz”), oraz "moji" (文字 – „znak”).
Przykłady:

</doc>
<doc id="1460" url="https://pl.wikipedia.org/wiki?curid=1460" title="Emotikonka">
Emotikonka



</doc>
<doc id="1462" url="https://pl.wikipedia.org/wiki?curid=1462" title="ELIZA">
ELIZA

ELIZA – program symulujący psychoanalityka, napisany w 1966 przez Josepha Weizenbauma.
Program analizuje wzorce w zdaniach, które otrzymuje, a następnie buduje pytania przez przestawienie słów oraz podmianę słów kluczowych. Mimo prostoty program jest tak przekonujący, że powstało wiele anegdot o ludziach silnie angażujących się emocjonalnie w trakcie „rozmów” z ELIZĄ.
Istnieje implementacja ELIZY dla Emacsa.

</doc>
<doc id="1463" url="https://pl.wikipedia.org/wiki?curid=1463" title="Europejski Certyfikat Umiejętności Komputerowych">
Europejski Certyfikat Umiejętności Komputerowych

Europejski Certyfikat Umiejętności Komputerowych (ang. "European Computer Driving Licence", ECDL) – jednolity dla całej Unii Europejskiej, a pod nazwą ICDL ("International Computer Driving Licence") dla reszty świata – certyfikat zaświadczający o posiadaniu podstawowych umiejętności w zakresie korzystania z komputera osobistego. Wcześniej nazywany w Polsce "Europejskim Komputerowym Prawem Jazdy".
Inicjatywa Certyfikatu Komputerowego powstała w 1992 roku w Finlandii. Pierwszy Certyfikat Umiejętności Komputerowych wydano w 1994 roku. Do połowy 1996 roku w liczącej około 5 mln mieszkańców Finlandii ECDL posiadało już ponad 10 000 osób. Docelowo Finowie zakładali, że egzaminom podda się około miliona osób, czyli co piąty obywatel Finlandii. Później ta inicjatywa rozpowszechniła się na cały obszar Unii Europejskiej i poza nią. Reguły egzaminu są identyczne na całym świecie.
Idea Certyfikatu Umiejętności Komputerowych wyszła naprzeciw wymaganiom pracodawców. Jest bowiem jednolitym i obiektywnym miernikiem umiejętności zatrudnionych bądź też zatrudnianych pracowników. Miernik ten jest niezależny od miejsca zdobycia umiejętności, od ukończonych kursów czy też wykształcenia. Pracodawca, zatrudniając posiadacza Certyfikatu Umiejętności Komputerowych, ma pewność, że jego pracownik będzie efektywnie wykorzystywał możliwości jakie niesie sobą technologia informatyczna.
Wzorując się na doświadczeniach fińskich, CEPIS – "Council of European Professional Informatics Societies" (tj. Stowarzyszenie Europejskich Profesjonalnych Towarzystw Informatycznych) na początku 1996 roku podjął inicjatywę upowszechnienie idei Certyfikatu Umiejętności Komputerowych w całej Zjednoczonej Europie. Inicjatywę poparła Rada Europy i włączyła Europejski Certyfikat Umiejętności Komputerowych do pakietu inicjatyw zmierzających do budowy w Europie Społeczeństwa Globalnej Informacji. Wdrożenie Europejskiego Certyfikatu Umiejętności Komputerowych w czasie Europejskiego Forum w Pradze we wrześniu 1996 roku zostało także zalecone krajom Europy Środkowo-Wschodniej jako jedno z działań dostosowawczych.
Do końca 1996 roku szereg krajów Europy rozpoczęło pilotowe wdrożenie programu Europejskiego Certyfikatu Umiejętności Komputerowych. Poza pomysłodawcą, tj. Finlandią, egzaminy według jednolitego programu są przeprowadzane w Szwecji (ponad 6500 do końca 1996 roku), Francji, Irlandii, Danii i Norwegii.
W roku 1997 CEPIS powołało Fundację ECDL z siedzibą w Dublinie (Irlandia), jako ogólnoświatową instytucję certyfikującą ECDL.
Polskie Towarzystwo Informatyczne, jako członek CEPISu, podjęło inicjatywę rozpropagowania idei i wdrożenia Europejskiego Certyfikatu Umiejętności Komputerowych w Polsce. Przygotowane zostały odpowiednie dokumenty i procedury, przetłumaczone pytania egzaminacyjne i przeszkoleni egzaminatorzy, w większości członkowie PTI. W 1997 roku powstało Polskie Biuro ECDL, którego zadaniem jest koordynacja prac, obsługa informacyjna systemu wydawania certyfikatów ECDL i nadzór nad rzetelnością przeprowadzania egzaminów.
Certyfikat ECDL zaświadcza, że jego posiadacz potrafi prawidłowo wykonywać na komputerze osobistym (mikrokomputerze) zadania opisane w sylabusie każdego z modułów egzaminacyjnych. Egzaminy ECDL sprawdzają podstawowe umiejętności potrzebne zarówno w pracy zawodowej, jak i coraz częściej w życiu codziennym każdego obywatela.
Pierwotnie certyfikat ECDL składał się z 7 modułów na poziomie podstawowym. W 2006 roku Polskie Biuro ECDL rozpoczęło wdrażanie certyfikatu na poziomie zaawansowanym – ECDL Advanced. Składa się on z 4 odrębnych egzaminów praktycznych. Obecnie tych modułów jest ponad 20 na poziomie podstawowym, pośrednim (średnio zaawansowanym) i zaawansowanym, z czego w Polsce dostępne są (wg stanu na 1.07.2017):
Moduły na poziomie podstawowym:
Moduły na poziomie pośrednim (średnio zaawansowanym):
Moduły na poziomie zaawansowanym:
Dla osób z ograniczoną wiedzą z zakresu informatyki i mechanizmów Internetu wprowadzono certyfikat e-Citizen (w wersji polskiej e-Obywatel).
Istnieją także certyfikaty zaakceptowane przez Fundację ECDL w programie EPP ("Endorsed Product Program"), do których w Polsce należą:
W 2014 roku system certyfikacji ECDL został zmieniony. W nowym systemie nie ma już certyfikatów ECDL Core obejmującego 7 modułów i certyfikatu ECDL Start obejmującego 4 moduły. Zostały one zastąpione przez: ECDL Standard oraz ECDL Base. Największą elastyczność system uzyskał poprzez wprowadzenie koncepcji ECDL Profile. ECDL Profile umożliwia kandydatom zbudowanie swojej ścieżki certyfikacyjnej, która najbardziej odpowiada ich potrzebom, czy potrzebom ich pracodawców. W ten sposób można walidować (certyfikować) te umiejętności, które najlepiej pasują do profilu zawodowego czy profilu edukacji.
Zdobywanie kolejnych, zaplanowanych kwalifikacji i zdawanie egzaminów z odpowiadających im modułów jest realizacją idei ECDL Profile. Jest to – zgodnie z założeniami Europejskich Ram Kwalifikacyjnych – program uczenia się przez całe życie; nigdy się nie kończy a kandydat może chcieć uaktualniać dotychczasowe kwalifikacje, zdobywać nowe umiejętności, związane z nowymi technologiami lub z nowymi obszarami, jako że Fundacja ECDL wprowadza w życie coraz to nowe moduły i uaktualnia dotychczasowe. ECDL Profile zachęca do ciągłego uczenia się i do stałego rozwoju kompetencji cyfrowych. Na certyfikacie ECDL Profile można umieścić informacje o wszystkich zdanych egzaminach ECDL, niezależnie od chwili, kiedy były zdawane – na jednym certyfikacie mogą więc pojawić się kolejne wersje tego samego egzaminu, świadczące o stałym aktualizowaniu posiadanych kompetencji, jak i egzaminy z kolejnych modułów, rozszerzających kompetencje cyfrowe ich posiadacza.
Certyfikat ECDL Profile można uzyskać po zdaniu dowolnej liczby egzaminów ECDL lub EPP, pod warunkiem że co najmniej jeden z nich należy do grupy ECDL.
W roku 2015 wprowadzono w Polsce rodzinę 21 certyfikatów ECDL Profile DIGCOMP dla certyfikacji różnych zestawów kompetencji DIGCOMP. Wynika to stąd, że Polskie Ministerstwo Rozwoju, jako organ udzielający wsparcia w ramach Europejskiego Funduszu Społecznego, dotyczącego m.in. poprawy kompetencji w zakresie Technologii Informacyjno-Komunikacyjnych (TIK), odniosło się do kwestii niedoboru kompetencji cyfrowych i wydało „Wytyczne w zakresie realizacji przedsięwzięć z udziałem środków Europejskiego Funduszu Społecznego (EFS) w obszarze edukacji na lata 2014–2020”. Standardem kompetencji, które zgodnie z „Wytycznymi…” można zdobywać w ramach szkoleń finansowanych w ramach EFS, stała się rama kompetencji cyfrowych DIGCOMP w wersji 1.0. Komisja Europejska opracowała ramy odniesienia dla rozwoju i rozumienia kompetencji cyfrowych w Europie w ramach projektu DIGCOMP – "Digital Competence Framework". Pierwsze opracowanie ramy DIGCOMP w wersji 1.0 powstało w roku 2013. Wersja 2.0 ramy (etap 1) pojawiła się w roku 2016 i w tymże roku została przetłumaczona na język polski, zaś etap 2, opisany jako wersja 2.1 - w maju 2017 r, w Brukseli.
Rama kompetencji nie jest wykazem czy katalogiem kompetencji, które można zdobywać – to struktura czy opis hierarchii poziomów kompetencji, umożliwiająca porównanie różnych kompetencji szczegółowych z każdego z reprezentowanych obszarów, w ramach każdej kompetencji odniesienia (kompetencji ramowej).
W ramie DIGCOMP zdefiniowano 21 kompetencji odniesienia i zgrupowano je w 5 obszarach – w wersji 1.0 na 3 poziomach zaawansowania, zaś w wersji 2.1 na 8 poziomach, tak jak w Europejskiej Ramie Kwalifikacji.
Zazwyczaj obszar wiedzy i umiejętności z sylabusa modułów ECDL jest znacznie szerszy od obszaru ramy DIGCOMP. W ramach jednego modułowego egzaminu ECDL sprawdzanych jest wiele kompetencji szczegółowych, wpisujących się w kompetencje odniesienia ramy DIGCOMP. Moduły ECDL w różny sposób wpisują się w kompetencje odniesienia ramy DIGCOMP, pokrywając w efekcie całą ramę. W ramach certyfikacji ECDL można potwierdzić kompetencje szczegółowe, które wpisują się we wszystkie 21 kompetencji odniesienia ramy DIGCOMP. W ramach projektu ECDL Profile DIGCOMP w ECDL Polska przygotowano całą serię certyfikatów, które mogą potwierdzać wiele różnych kombinacji kompetencji szczegółowych, wpisujących się w różne kompetencje odniesienia DIGCOMP. Na Światowym Forum na Malcie w maju 2017 projekt ten uzyskał nagrodę główną (szklaną kulę) Best Practice Award.
Od roku 2016 Ministerstwo Rozwoju uznaje certyfikaty ECDL za kwalifikacje.

</doc>
<doc id="1464" url="https://pl.wikipedia.org/wiki?curid=1464" title="Ekumena">
Ekumena

Ekumena (z "oikoumene" – świat zaludniony) – obszary na kuli ziemskiej stale zamieszkane i wykorzystywane gospodarczo przez człowieka.
Początkowo za ekumenę uważano tylko obszar znany ludności kręgu śródziemnomorskiego. Obszary nieznane uważane były błędnie za niezamieszkane i nazywano ją anekumeną. Od wielu wieków, zwłaszcza od epoki wielkich odkryć geograficznych, a także wraz z postępem technicznym, ekumena powiększa się kosztem anekumeny.
Obecnie stałe osiedla ludzkie występują między 54° szerokości geograficznej południowej a 78° szerokości geograficznej północnej.

</doc>
<doc id="1465" url="https://pl.wikipedia.org/wiki?curid=1465" title="Emmy Noether">
Emmy Noether

Amalie Emmy Noether (ur. 23 marca 1882 w Erlangen, zm. 14 kwietnia 1935 w Bryn Mawr w stanie Pensylwania) – niemiecka matematyczka i fizyczka, znana głównie dzięki osiągnięciom w teorii pierścieni i rozwinięciu nowej gałęzi matematyki – algebry abstrakcyjnej. Bywa opisywana jako „najważniejsza kobieta w historii matematyki”. Albert Einstein pisał o niej:
Życiorys.
Emmy Noether urodziła się w 1882 w Erlangen w tradycyjnej rodzinie żydowskiej. Jej ojciec, Max Noether, był znanym profesorem matematyki. Ponieważ w owym czasie większość ścieżek kariery akademickiej była dla kobiet zamknięta, studiowała oficjalnie pedagogikę. Na wykłady z matematyki mogła uczęszczać jedynie jako wolny słuchacz. Jej talent matematyczny został jednak dostrzeżony przez wykładowców, którzy znali i przyjaźnili się z jej ojcem. Dzięki temu ostatecznie została dopuszczona do egzaminów.
W 1907 roku doktoryzowała się na Uniwersytecie w Erlangen. Ze względu na płeć nie mogła występować w roli profesora. Mogła jedynie podjąć pracę jako pomoc wykładowcy nie otrzymując wynagrodzenia. W roku 1915 skorzystała z zaproszenia od Feliksa Kleina by dołączyć do wydziału matematyki Uniwersytetu w Getyndze, gdzie działali ówcześni najwybitniejsi matematycy niemieccy. Wykładała jako asystentka Dawida Hilberta. Zarówno Hilbert, jak i Klein bezskutecznie zabiegali w Ministerstwie Edukacji o przyznanie jej tytułu profesora wraz z wynagrodzeniem argumentując, że w przeciwnym razie Getynga straci wybitnego matematyka.
W 1918 r. udowodniła fundamentalne twierdzenie, tzw. twierdzenie Noether, które wiąże symetrie (niezmienniczości) praw fizyki z zasadami zachowania pewnych wielkości fizycznych. W latach 1922–1933 była profesorem Uniwersytetu w Getyndze, skąd została usunięta przez nazistów z powodu żydowskiego pochodzenia. Uczyła studentów nielegalnie, potajemnie przyjmując ich we własnym mieszkaniu. W październiku 1933 wyjechała do Stanów Zjednoczonych. Wykładała matematykę w Bryn Mawr College i pracowała naukowo w Institute for Advanced Study w Princeton.
W algebrze od jej nazwiska pochodzi nazwa pojęcia pierścienia noetherowskiego oraz nazwa twierdzenia Laskera–Noether.
Po jej niespodziewanej śmierci (zmarła w trakcie rekonwalescencji po operacji usunięcia nowotworu) Einstein napisał do „New York Timesa”:

</doc>
<doc id="1466" url="https://pl.wikipedia.org/wiki?curid=1466" title="Évariste Galois">
Évariste Galois

Évariste Galois (IPA: , ur. 25 października 1811 w Bourg-la-Reine k. Paryża, zm. 31 maja 1832 w Paryżu) – francuski matematyk i działacz polityczny, student École Normale Supérieure.
Galois zasłynął jako wybitny algebraik. Przez badania wielomianów współtwórzył teorię grup, ciał i łączącą je teorię nazwaną jego nazwiskiem, a przez to szerszy program algebry abstrakcyjnej; dotykał również teorii liczb i analizy. Za życia nie został w pełni doceniony, choć współpracowali z nim niektórzy uczeni jego czasów, a część swoich osiągnięć zdołał opublikować. Kilka dekad później jego prace przeanalizowali i wypromowali inni matematycy francuscy, zwłaszcza Joseph Liouville i Camille Jordan. Tym sposobem teoria grup i teoria Galois stały się żywymi obszarami badań, w XXI wieku dalej rozwijanymi.
Galois był również oponentem monarchii lipcowej i więźniem politycznym. Za krytykę swoich przełożonych został usunięty z uczelni; sprzeciwiał się też publicznie królowi Ludwikowi Filipowi. Był przez to dwukrotnie uwięziony, a działalność opozycyjna mogła się też przyczynić do jego tragicznej śmierci.
Życiorys.
Pochodzenie i lata szkoły.
Évariste Galois był synem Nicolasa-Gabriela Galois (1775–1829) i jego żony Adélaide-Marie z rodziny Demante (1788–1871). Nicolas-Gabriel prowadził miejscową szkołę, a w 1815 roku został merem miejscowości Bourg-la-Reine, w związku ze stoma dniami Napoleona, którego N.G. Galois gorąco popierał. Évariste miał dwójkę rodzeństwa – starszą siostrę Nathalie Théodore oraz brata Alfreda. Cała trójka rodzeństwa była najpierw kształcona domowo przez matkę. U Évariste’a trwało to do 12. roku życia; poznał przy tym łacinę i grekę.
W 1823 roku wstąpił do szkoły Collège Royal de Louis-le-Grand. Prowadził ją wówczas Nicolas Berthot – matematyk zatrudniony wcześniej na École Polytechnique, pedagogicznie surowy i politycznie konserwatywny, nielubiany nawet w kręgu rojalistów. Galois jeszcze jako uczeń zapoznał się z twórczością Lagrange’a i Legendre’a. Dwukrotnie nie zdał egzaminu do École Polytechnique w Paryżu – w 1827 i 1829, a przy drugim podejściu popadł w konflikt z egzaminatorem ustnym. Ten sam rok przyniósł kilka innych znaczących wydarzeń:
Czasy studiów i represji.
W 1830 roku, już jako student ÉNS, napisał dalsze artykuły. W kwietniu opublikował trzy z nich w „Bulletin de Férussac”, we współpracy ze J.Ch.F. Sturmem. Poprawił też zgubę z poprzedniego roku, a nową rozprawę otrzymał i przechowywał J.B.J. Fourier. Niedoszły recenzent zmarł 16 maja, nie zostawiwszy żadnego komentarza, a w jego notatkach nie znaleziono śladu tej pracy Galois. W grudniu młody matematyk skrytykował swojego rektora, M. Guigniaulta, w liście do gazety „Gazette des Écoles”. Évariste został za to wydalony z uczelni; następnie dołączył do organizacji republikańskiej, zdelegalizowanej przez nowego króla Ludwika Filipa pod koniec tamtego roku.
Siméon Denis Poisson zachęcił Évariste’a, żeby po raz trzeci wysłał swoją twórczość do Francuskiej Akademii Nauk, co Galois zrobił w styczniu nowego roku 1831. 4 lutego zmarł Théodore Michel Galois – stryj Évariste’a, bliski swojemu bratankowi, zwłaszcza po śmierci jego ojca. W maju młody opozycjonista uczestniczył w zgromadzeniu, na którym wydawał się grozić królowi. Został za to aresztowany i umieszczony w więzieniu Sainte-Pélagie, ale w czerwcu został uniewinniony. W lipcu Galois aresztowano po raz drugi, w związku z publicznym noszeniem munduru jego zdelegalizowanej gwardii oraz broni (białej i palnej). Za kratami Sainte-Pélagie spędził tym razem pół roku. Otrzymał tam odpowiedź od Poissona; profesor był krytyczny – uznał wywód Galois za niezrozumiały – choć zachęcał początkującego matematyka do dalszych prac na ten temat. Młodzieniec podjął wtedy próbę samobójczą za pomocą ostrza.
W marcu 1832 roku w Paryżu wybuchła epidemia cholery, przez co Galois został przeniesiony z więzienia do szpitala Sieur Faultrier. Poznał tam Stéphanie-Felice du Motel – córkę jednego z lekarzy. Évariste zakochał się w niej; po tym, jak w kwietniu został wypuszczony ze szpitala, wymieniał z nią listy.
Okoliczności śmierci.
30 maja 1832 Galois oraz inny republikanin – być może Perscheux d’Herbinville – stoczyli pojedynek o niejasnych powodach; mogły być związane ze Stéphanie-Felice. Galois został w nim śmiertelnie ranny, porzucony przez świadków i znaleziony potem przez przechodzącego rolnika. Zmarł następnego dnia w szpitalu Cochin, nie ukończywszy 21 lat, i został pochowany dwa dni później (2 czerwca). Jego śmierć wywołała protesty i zamieszki trwające kilka dni. Wysuwano podejrzenia, że Galois został zamordowany za sympatie polityczne, a pojedynek jedynie upozorowano. 
Ostatniej nocy przed pojedynkiem Galois napisał list do przyjaciela. Zawarł w nim swoje najważniejsze idee i osiągnięcia matematyczne, a także wielokrotnie wspominał Stéphanie.
Dorobek badawczy.
Galois zasłużył się głównie algebrze. Badał rozwiązywalność równań wielomianowych przez pierwiastniki, idąc dalej niż poprzedzający go Paolo Ruffini i Niels Henrik Abel. Jego twierdzenie jest rozszerzeniem twierdzenia Abela-Ruffiniego, które wyklucza ogólny wzór pierwiastnikowy na miejsca zerowe wielomianu 5. stopnia. Galois podał wyczerpujące kryterium istnienia takich rozwiązań (warunek równoważny). Tym sposobem odpowiedział na pytanie postawione ponad 200 lat wcześniej. Zrobił to dzięki zbudowaniu podstaw teorii grup oraz teorii nazwanej od jego nazwiska. Jako pierwszy użył słowa „grupa” w tym kontekście i wprowadził szereg kluczowych pojęć jak grupa ilorazowa, warstwa, podgrupa normalna, grupa prosta czy grupa rozwiązalna.
Prace Galois dotyczyły też ciał skończonych – zwanych również ciałami Galois – oraz analizy, zwłaszcza funkcji eliptycznych. Jego wyniki dotyczące ułamków łańcuchowych można zaliczyć do teorii liczb.
Wpływ i upamiętnienie.
Zgodnie z wolą Évariste’a Galois jego brat Alfred oraz jeden z przyjaciół (Auguste Chevalier) wysłali notatki zmarłego matematyka do innych naukowców, w tym C.F. Gaussa i C.G.J. Jacobiego. Nie wywołało to żadnego udokumentowanego odzewu, choć po dekadzie Galois został doceniony przez Josepha Liouville’a. W 1843 roku Liouville ogłosił Francuskiej Akademii Nauk, że Galois poprawnie rozstrzygnął problem rozwiązalności wielomianów przez pierwiastniki. W 1846 roku w swoim czasopiśmie „Journal de Mathématiques Pures et Appliquées” Liouville opublikował notatki Galois wraz z własnym komentarzem. W 1870 roku Camille Jordan wydał monografię "Traité des Substitutions", dzięki której teoria grup, w tym teoria Galois, stała się szeroko znana w środowisku matematyków. W 1897 roku w Paryżu wydano dzieła zebrane Galois z komentarzem C.F. Picarda.
W 1848 roku narodził się syn Alfreda Galois. Został nazwany Évariste, tak jak jego stryj; młodszy Évariste przeżył tylko dwa lata.
W 1958 roku Leopold Infeld opublikował powieść o życiu Évariste’a Galois, zatytułowaną "Wybrańcy bogów".
W 1984 roku ukazał się francuski znaczek pocztowy przedstawiający Galois, „rewolucjonistę i geometrę”.

</doc>
<doc id="1467" url="https://pl.wikipedia.org/wiki?curid=1467" title="Escudo">
Escudo

Escudo (port. i hiszp. "tarcza"), eskudo – nazwa dawnej bądź (w zależności od państwa) współczesnej jednostki monetarnej krajów portugalskojęzycznych, a także Chile (lata 1960-75, zastąpiona przez peso), Hiszpanii (złote escudo od 1566 do 1833 i srebrne escudo w latach 1864-69, zastąpione przez pesetę) oraz Zjednoczonych Prowincji Ameryki Środkowej. Eskudo zazwyczaj dzieli się na 100 "centavos", a jego symbol to najczęściej cifrão ( formula_1 ). Przede wszystkim wyróżnia się:

</doc>
<doc id="1468" url="https://pl.wikipedia.org/wiki?curid=1468" title="Efekt Schwarzschilda">
Efekt Schwarzschilda

Efekt Schwarzschilda - zakłócenie proporcjonalności pomiędzy czasem i skutecznością naświetlania (jedna działka przysłony = 2x mniej (lub więcej) czasu), zbadane w 1899 roku przez pioniera astrofizyki Karla Schwarzschilda. Efekt ten występuje przy bardzo krótkich i bardzo długich czasach naświetlania materiału światłoczułego - potrzeba wtedy dłuższej ekspozycji niż wynikałoby to z obliczeń, lub wskazań światłomierza. W przypadku filmów kolorowych efekt Schwarzschilda dotyczy każdej warstwy światłoczułej indywidualnie. Jeżeli poszczególne warstwy w innym stopniu ulegają efektowi dodatkowo dochodzi do pojawienia się przebarwień. Aby im zapobiec należy stosować odpowiednie filtry. Informacje dotyczące efektu Schwarzschilda (w tym wielkość korekty ekspozycji oraz wymagane filtry) dla konkretnego filmu można znaleźć w materiałach udostępnianych przez producenta: ulotkach, katalogach, stronach WWW; zwykle nie ma tych danych na pudełku filmu.

</doc>
<doc id="1469" url="https://pl.wikipedia.org/wiki?curid=1469" title="Elbląg">
Elbląg

Elbląg (, niem. , prus. "Elbings", ) – miasto na prawach powiatu w województwie warmińsko-mazurskim, siedziba władz powiatu elbląskiego i gminy wiejskiej Elbląg, ale miasto nie wchodzi w ich skład, stanowiąc odrębną jednostkę samorządu terytorialnego. Od 1992 stolica diecezji elbląskiej. Najstarsze miasto w województwie, jedno z najstarszych w Polsce i Niemczech (rok założenia 1237, prawa miejskie 1246). Miasto posiadało prawo do czynnego uczestnictwa w akcie wyboru króla. Obywatelstwo Elbląga dawało przywilej do posiadania ziemi. Elbląg to najniżej położone miasto w Polsce. Leży u ujścia rzeki Elbląg do Zalewu Wiślanego.
Według danych GUS z 30 czerwca 2021 r., Elbląg liczył 117 952 mieszkańców i był pod względem liczby ludności drugim (po Olsztynie) miastem w województwie warmińsko-mazurskim, a także 30. spośród najludniejszych miast w Polsce.
Elbląg to ośrodek przemysłu ciężkiego (Spółka Zamech Marine zajmująca się produkcją śrub napędowych do statków oraz General Electric z Zakładem Metalurgicznym i Zakładem Turbin), przemysłu spożywczego (browar wchodzący w skład Grupy Żywiec), przemysłu meblarskiego, również turystycznego (Kanał Elbląski z pochylniami). Rozwój miasta przypadał m.in. na okres od 1 czerwca 1975 do 31 grudnia 1998, kiedy Elbląg był stolicą województwa. Miał wtedy miejsce znaczny napływ ludności do miasta.
Przez obwodnicę Elbląga przebiegają dwie drogi krajowe: S7 łącząca Elbląg z Gdańskiem, Warszawą i Krakowem oraz S22 będąca najkrótszą drogą łączącą zachód i wschód Europy. Od Elbląga na wschód obie drogi mają status drogi ekspresowej.
Elbląg leży w historycznych Prusach, na ziemi malborskiej, w jej wschodniej części obejmującej północną Pogezanię, a także na Powiślu. Jest jednym ze spenetrowanych archeologicznie polskich miast, dzięki czemu elbląskie muzeum posiada unikatowe eksponaty (np. średniowieczną windę).
Nazwa.
Nazwa Elbląg pochodzi od rzeki Elbląg, zapisanej w 890 roku przez podróżnika Wulfstana jako "Ilfing". Miasto przejęło nazwę (Elbing) od rzeki w 1237 roku.
Istnieje wersja (na przykład według ) o połączeniu nazwy Elbing z plemieniem Helwekonów (należeli do związku Lugiów). Nazwa polska Elbląg jest możliwa w dwóch częściach: "Elb" od Helwekonów i "ląg" od Lugiów.
Spis geograficzno-topograficzny miejscowości leżących w Prusach z 1835 roku, którego autorem jest J.E. Muller notuje nazwy miejscowości we fragmencie: "„Elbing (Elbinga, poln. Elbiag, auch Elblag)”".
Historia.
Średniowiecze.
Niedaleko miejsca, w którym do IX wieku znajdowała się pruska osada handlowa Truso, wiosną 1237 roku Krzyżacy pod dowództwem Hermanna von Balka wybudowali na wyspie u ujścia rzeki Elbląg niewielki drewniano-ziemny gródek. Na dzień 13 stycznia 1238 datowany jest dokument potwierdzający uposażenie klasztoru Dominikanów i przybycie zakonników do Elbląga. Po tym gdy fortyfikacja ta została prawdopodobnie zniszczona przez plemiona pruskich Pogezanów, Krzyżacy po roku 1240 przenieśli się w miejsce dzisiejszego Starego Miasta, w którym w miejscu dzisiejszego Podzamcza istniała już osada zasiedlona od połowy X wieku. Do osady, której nazwa wywodziła się od pruskiej nazwy rzeki Ilfing, Krzyżacy sprowadzili kolonistów niemieckich i stąd prowadzili ekspansję militarną w kierunku ziem zamieszkanych przez Prusów. W tym samym roku uruchomili trzecią w państwie krzyżackim mennicę. W 1242 roku Elbląg był jednym z nielicznych miejsc, które oparły się atakom Prusów w trakcie I powstania pruskiego i najazdowi księcia gdańskiego Świętopełka II, co oznacza, że już wtedy miejsce to musiało być ufortyfikowane. Osada ta już pod panowaniem zakonu krzyżackiego, bardzo szybko, bo już w 1246 roku, uzyskała przywilej miejski na prawie lubeckim, a także znacznie większe przywileje niż sąsiednie miasta lokowane na prawie chełmińskim. Podobnie jak w innych państwach nadbałtyckich ulice wytyczono prostopadle do rzeki, a przecinająca je w poprzek ulica stanowiła rynek. Pod dokumentem z 15 lutego 1242 użyto po raz pierwszy pieczęci miejskiej z wizerunkiem kogi. W latach 1245–1248 rozpoczęto budowanie murowanego zamku, który w 1251 roku został siedzibą mistrza krajowego – dzięki temu przez pierwsze 70 lat Elbląg był najważniejszym ośrodkiem życia miejskiego, jedynym portem morskim oraz podstawową bazą militarną organizującego się państwa krzyżackiego. W 1246 roku po raz pierwszy pojawiła się wzmianka o komturze elbląskim o imieniu Aleksander. 18 stycznia 1255 zatrzymał się w Elblągu wracający z Sambii czeski król Przemysł Ottokar II.
Zbudowany na południe od miasta w okolicach ujścia rzeki Kumieli do rzeki Elbląg zamek krzyżacki był uważany za najpotężniejszy i najpiękniejszy zamek zakonny po zamku w Malborku. Do czasu wybudowania tego ostatniego, pełnił do 1309 roku rolę centrum administracyjnego państwa krzyżackiego i był miejscem spotkań kapituły pruskiej.
Elbląg oparł się plemionom Prusów podczas II powstania pruskiego w latach 1260–1274. Po założeniu i rozbudowie Malborka oraz przeniesieniu z Wenecji na malborski zamek w 1309 r. siedziby wielkiego mistrza zamek elbląski stał się w 1312 roku siedzibą wielkiego szpitalnika zakonu i jednocześnie komtura elbląskiego. Źródłem zamożności miasta było nadanie mu przez Krzyżaków znacznych posiadłości na Żuławach i Wzniesieniu Elbląskim. W 1319 zakończono budowę Bramy Targowej. 
Podstawą rozwoju miasta był handel morski, który powodował napływ nie tylko towarów, ale i osadników z różnych krajów. W Elblągu osiedlali się Meklemburczycy i Lubeczanie, Holendrzy, licznie przybywali Westfalczycy mniej licznie Ostfalczycy (upraszczając Sasi), oprócz tego Anglicy, Francuzi i Szkoci.
Miasto żywo uczestniczyło w życiu Hanzy. Kupcy elbląscy zasiadali w mieście kantorowym Brugii i decydowali o przyjmowaniu do związku kolejnych miast. Także Krzyżacy doceniali wagę Elbląga, wznosząc tu wielki zamek, zniszczony w czasie wojny trzynastoletniej.
Od połowy XIV wieku Elbląg zaczął stopniowo tracić na znaczeniu w stosunku do innych pruskich miast portowych. Wpłynęło na to wiele czynników. Po podboju Prusów Elbląg przestał być wojenną bazą wypadową na wschód, a po zagarnięciu przez Krzyżaków Pomorza Gdańskiego i ustanowieniu stolicy państwa w Malborku, Elbląg stracił charakter głównego ośrodka politycznego i gospodarczego państwa. Skierowanie głównego ujścia Wisły ku Gdańskowi utrudniło żeglugę do Elbląga, a rozwój korzystniej położonego Gdańska zaczął przyciągać do niego większość statków z północy i zachodu Europy. Silnym ciosem dla miasta było założenie przez Krzyżaków konkurencyjnego Nowego Miasta Elbląg w 1337 r. i nadanie mu przywileju lokacyjnego w dziesięć lat później.
Od końca XIV wieku Elbląg stopniowo tracił znaczenie na rzecz Gdańska, który później zmonopolizował handel polskim zbożem. Do osłabienia pozycji miasta przyczyniła się także przyroda. Przejścia morskie przez Mierzeję Wiślaną stopniowo się zamulały, co uniemożliwiało coraz większym statkom wpłynięcie do portu. Jednak regres nie był tak znaczny, jak się dotychczas sądziło. Dowodem są licznie zgromadzone obiekty archeologiczne, świadczące o względnym dobrobycie miasta w epoce nowożytnej.
Po klęsce Krzyżaków z wojskami polsko-litewskimi pod Grunwaldem w 1410, w której Elbląg stał po stronie Krzyżaków, mieszczanie elbląscy zdobyli i wypędzili załogę wraz z wielkim szpitalikiem i tutejszym komturem. Elblążanie 22 lipca 1410 złożyli hołd polskiemu królowi Władysławowi Jagielle. Jednak we wrześniu 1410 zamek ponownie wrócił we władanie Krzyżaków. W 1440 roku to właśnie w Elblągu powstał Związek Pruski. 12 lutego 1454 w wyniku powstania antykrzyżackiego elbląscy mieszczanie zdobyli po 5-dniowym oblężeniu zamek krzyżacki, a następnie, w obawie przed ponownym powrotem Krzyżaków, zniszczyli go. W tym samym roku elblążanie na opanowanym zamku krzyżackim złożyli hołd królowi Kazimierzowi IV Jagiellończykowi. W 1454 Elbląg otrzymał od króla Kazimierza Jagiellończyka wielki przywilej potwierdzający stare prawa, rozszerzający władze samorządu polskiego, zwiększający kompetencje Sądu Miejskiego i powiększający terytorium miasta niemal dwukrotnie. Miasto ponadto przejęło dotychczasowe prawa rybackie komturów, ich młyny i inne dobra. W zamian za to Elbląg czynnie uczestniczył w rozgromieniu floty Zakonu w bitwie na Zalewie Wiślanym. Według postanowień pokoju toruńskiego kończącego wojnę trzynastoletnią w 1466 Elbląg przyznano Polsce, do której należał przez około 300 lat, aż do roku 1772. W 1483 zakończono budowę Kanału Jagiellońskiego, dzięki któremu Elbląg odzyskał połączenie wodne z rzeką Nogat. 7 lutego 1495 w Elblągu gościł król Jan I Olbracht. Od 1503 miasto było strażnikiem pieczęci Prus Królewskich, 18 stycznia 1504 do Elbląga przybył Mikołaj Kopernik. 29 maja 1535 powstało Gimnazjum Elbląskie, pierwsza w kraju szkoła średnia. W 1552 przebywał w Elblągu król Zygmunt II August, dwa lata później Wolfgang Dietmar uruchamia pierwszą w drukarnię w mieście. W 1571 król Zygmunt II August zamówił budowę pierwszego polskiego okrętu wojennego, galeonu "Smok", niestety śmierć króla przerwała prace, do których nigdy nie powrócono. 10 września 1576 w Elblągu przebywał król Stefan Batory, dzięki jego decyzjom od 7 marca 1577 cały morski handel polski miał przechodzić przez Toruń i Elbląg, czyniąc tutejszy port głównym w Rzeczypospolitej.
Złoty wiek Elbląga i jego upadek.
10 września 1577 wybucha wojna Rzeczypospolitej z Gdańskiem, Elblążanie odpierają szturm Gdańszczan. W 1584 Anglicy otwierają w Elblągu filię przedsiębiorstwa Estland Company, która ma się zajmować handlem ze wschodem. W 1592 powstaje Biblioteka Elbląska, a w sierpniu 1601 w Domu Pod Siedmioma Szczytami zaproszona przez rajcę miejskiego Andrzeja Bartowicza angielska trupa teatralna Johna Greena odegrała pierwsze przedstawienie sztuk Szekspira w Polsce. Od 8 do 12 czerwca 1623 w Elblągu przebywał król Zygmunt III Waza.
Na skutek wojny polsko-szwedzkiej o ujście Wisły w latach 1626–1635 w mieście stacjonował szwedzki garnizon, który wycofał się z miasta po podpisaniu rozejmu w Sztumskiej Wsi. 11 lutego 1636 przebywa z wizytą król Władysław IV Waza, w 1651 odwiedził miasto jego brat - król Jan II Kazimierz Waza.
W czasie potopu wojska szwedzkie zajęły miasto 22 grudnia 1655 roku. Zniszczenia spowodowane okupacją szwedzką w latach 1656–1660 zahamowały rozwój, a towarzyszące jej zarazy wyludniły miasto.
W czerwcu 1698 król Polski August II na zjeździe w Jańsborku podpisał z elektorem Fryderykiem I Hohenzollernem tajny układ, na mocy którego ten drugi uzyskał zgodę na zajęcie miasta w zamian za 150 tys. talarów. Oblężenie rozpoczęte w październiku tego samego roku zakończyło się aktem kapitulacji podpisanym przez radę miejską 10 listopada. Wzburzenie polskiej opinii publicznej i zdecydowane, mimo obłudnego postępowania Augusta II, dążenie do odzyskania Elbląga, a nadto próby mediacji ze strony Danii, Szwecji i cesarza skłoniły elektora do przyjęcia propozycji rozmów dyplomatycznych. Zostały one zakończone układem z 17 grudnia 1699, który przyniósł zgodę na zwrot Elbląga Polsce pod warunkiem spłaty należności wobec elektora. Było to tylko chwilowe zwycięstwo, gdyż już w 1703 Fryderyk I Hohenzollern zajął posiadłości ziemskie Elbląga, pozbawiając miasto 50% rocznych dochodów. W 1700 powstał pierwszy majątek mieszczański, znajdował się na terenie Bażantarni, ale ostatecznie został przejęty przez Sebastiana Stolza, który wcześniej posiadał już majątek Stolzenhof. W 1703 w Elblągu przebywał August II Sas, a rok później jego następca Stanisław Leszczyński.
Podczas wielkiej wojny północnej miasto okupowane było kolejno przez oddziały szwedzkie (1703–1710), rosyjskie (1710–1712) i saskie (1712). Kolejne pobyty obcych wojsk uszczuplały kasę miejską. Podupadłe miasto stało się łatwym łupem dla Prus.
Na początku XVIII wieku miejski handel był już w upadku, zamierała praca stoczni. Pod rządami pruskimi największe obok Królewca miasto Prus Wschodnich stało się głównym portem dorzecza Wisły, ale tylko do czasu zajęcia przez Fryderyka II Gdańska. Dwadzieścia lat działania w uprzywilejowanych warunkach pozwoliło Elblągowi wybić się gospodarczo dzięki usprawnieniu pracy portu – pogłębieniu Nogatu, rzeki Elbląg, redy portowej i rozbudowie urządzeń portowych. Ożywienie gospodarcze było jednak krótkotrwałe, a intensywne zamulenie toru wodnego ponownie zahamowało rozwój portu i handlu. W 1737 obchodzono 500-lecie miasta, rok później, 6 września 1738 odbyło się wodowanie statku pełnomorskiego "Stadt Elbing", który powstał na zamówienie tutejszego mieszczanina Henryka Doringa. W 1765 miasto wizytował poseł króla polskiego Stanisława Augusta Poniatowskiego.
Już podczas I rozbioru Polski w 1772 Elbląg znalazł się pod zaborem pruskim. Wojska zaborcy opanowały miasto 12 września, a dzień później miasto opuścił polski garnizon. Elbląg pozbawiony został swoich dotychczasowych przywilejów, odebrano mu prawo lubeckie, a tym samym zniesiono mu samorząd. Władze miasta zostały podporządkowane pruskim urzędnikom państwowym, zaakcentowała to wizyta króla pruskiego Fryderyka II Wielkiego, która miała miejsce w dniach 6-7 czerwca 1773. Z dniem 10 września 1773 władze pruskie zniosły prawo lubeckie i zmieniły ustrój samorządowy, Elbląg stał się prowincjonalnym miastem powiatowym. Dążąc do gospodarczego uzależnienia Polski od Prus zaborca nałożył cło na polskie towary. Odbiło się to ujemnie na, pozostających jeszcze w granicach Polski, Gdańsku i Toruniu, a dodatnio na obrotach handlowych Elbląga, do którego zaczęli się przenosić liczni kupcy. Nie trwało to jednak długo, gdyż już w 1807 rozpoczął się kryzys handlowy. Podczas burzy 26 kwietnia 1777 od pioruna spaliła się wieża kościoła św. Mikołaja i staromiejski ratusz, w 1779 rajcy miejscy podjęli decyzję o budowie nowego ratusza w nowej lokalizacji, na Nowym Rynku, budowę zakończono w 1782. 31 maja 1787 ukazała się drukiem pierwsza elbląska gazeta nosząca tytuł "Elbingsche Anzeigen von handlungs-ökonomischen, historischen und literarischen Sachen", jej redaktorem był Fryderyk Traugott Hartmann. W 1801 Albert Abbeg nabył ziemię na terenie Bażantarni i rozpoczął budowę dworu oraz zabudowań gospodarczych. W dniach 8-9 maja 1807 w Elblągu zatrzymał się cesarz Francji Napoleon Bonaparte. W 1811 w Bażantarni otwarto zajazd Gasthaus .
XIX wiek i rewolucja przemysłowa.
Od 1815 do 1920 Elbląg należał do rejencji gdańskiej w pruskiej prowincji Prusy Zachodnie (przejściowo w latach 1829–1878 Prusy). 1 lipca 1818 utworzono powiat ziemski Elbląg (Elbing), którego pierwszym starosta był Johann Christian Ludwig Bax. W tym czasie do Elbląga przybył z wizytą król Fryderyk Wilhelm III, a miesiąc później jego syn, następca tronu Fryderyk Wilhelm IV. W tym samym roku w Deutsches Haus zostało zainaugurowane działanie pierwszej stałej sceny teatralnej. Na początku XIX wieku miasto zostało zdegradowane do roli portu rzecznego w lokalnym handlu. Elbląg szukał nowych dróg rozwoju gospodarczego i powoli zaczął przeradzać się w ośrodek przemysłowy, administracyjny i wojskowy. 17 lutego 1828 założono Towarzystwo Przemysłowe ("Elbinger Gewerbe-Vereins"), które otworzyło i prowadziło Szkołę Przemysłową, której absolwentem był m.in. Ferdinand Schichau. 24 sierpnia 1828 pierwsi turyści odbyli podróż parowcem Coppernikus do Krynicy Morskiej. 3 kwietnia 1837 powstała Henrich von Plauen Schule, pierwsze gimnazjum męskie będące wyższą szkołą miejską. 
Już w II połowie XIX wieku obok Szczecina, a przed Gdańskiem i Królewcem, był silnym pruskim ośrodkiem przemysłu metalowego. Miejscowi kupcy zaczęli inwestować w manufaktury i fabryki. Rozwinął się przemysł tkacki, farbiarski i szklarski. Powstawały liczne zakłady przetwórstwa rolno-spożywczego, tytoniowe, mydlarnie, olejarnie, krochmalnie. Ważnym działem przemysłu pozostawała nadal budowa statków. Dużą rolę w tej dziedzinie odegrał Ferdinand Schichau, który w 1837 otworzył fabrykę maszyn, a w 1854 swoją stocznię, będącą głównym niemieckim producentem torpedowców. XIX-wieczny Elbląg stawiał przede wszystkim na rozwój nowoczesnego przemysłu. W tym okresie powstały m.in. odlewnia żelaza Tiessena, zakłady metalowe Neufelda, fabryka cygar Losera&amp;Wolffa, fabryka samochodów Komnicka. Rozwój gospodarczy przyniósł ze sobą tworzenie się klasy robotniczej, której trzon stanowili metalowcy. Zatrudnianie dzieci, zbyt długi dzień pracy, niskie zarobki doprowadziły do wzrostu nastrojów rewolucyjnych. W mieście rozpoczęły się strajki rzemieślników. W 1840 powołano spółkę do obsługi linii morskiej łączącej Elbląg z Królewcem. W 1846 zakończono budowę Teatru Miejskiego, którego gmach powstał przy obecnej ul. Rycerskiej. W tym samym roku rozpoczęto budowę linii kolejowej łączącej Braniewo i Malbork przez Elbląg. W 1849 wybudowano pierwszy statek parowy, który powstał od podstaw w Elblągu. 19 października 1852 uruchomiono połączenie kolejowe razem z dworcem kolejowym przy obecnej Alei Grunwaldzkiej. 28 listopada 1859 rozpoczęła pracę gazownia miejska. W 1863 z powiatu ziemskiego elbląskiego wydzielono powiat miejski Elbląg ("Stadtkreis"). W 1865 powstało Muzeum Miejskie, w 1875 zawodowa straż pożarna, a w 1880 uruchomiono browar English Brunnen. W 1882 rada miasta kupiła od spadkobierców Alberta Abbega tereny Bażantarni "(Vogelsang)". W dniach 26-29 marca 1888 miasto doświadczyło jednej z największych w swojej historii powodzi, z brzegów wystąpiły Nogat i Kumiela, w wyniku zalania fabryk pracę straciło ponad tysiąc osób. W grudniu 1892 rozpoczęła pracę rzeźnia miejska, 22 listopada 1895 na miasto wyjechały pierwsze tramwaje, ich trasy liczyły łącznie 3,88 km. 20 maja 1899 uruchomiono pierwszy odcinek Kolei Nadzalewowej łączący Elbląg z Fromborkiem. W 1902 otwarty został pierwszy szpital miejski na 250 łóżek, w 1912 oddano do użytku budynek sądu okręgowego i nową siedzibę Szkoły Henryka von Pluena (obecny budynek urzędu miejskiego). Na terenie miasta swoją działalność rozpoczęły komórki Socjaldemokratycznej Partii Niemiec. Robotnicy zaczęli się formować w organizacje, które prowadziły szeroko zakrojoną akcję propagandową. Do wybuchu I wojny światowej przez miasto przetaczały się kolejne fale wystąpień, niekiedy brutalnie tłumionych przez policję. Nie zahamowało to jednak gospodarczego rozwoju miasta, który trwał do światowego kryzysu ekonomicznego w 1918. 19 maja 1916 podczas wizyty w Elblągu cesarz Wilhelm II na własne życzenie odbył przejażdżkę po mieście tramwajem, wsiadł przy dworcu kolejowym i dojechał do stoczni Schichaua.
Wraz z wybuchem wojny sytuacja miasta znacznie się pogorszyła. Już od 1914 przez Elbląg przewijały się dziesiątki tysięcy uciekinierów. Rezultatem tego był powszechny chaos, brak żywności itp. Dekoniunktura gospodarcza lat wojny spowodowała zamknięcie wielu zakładów przemysłowych, a w rezultacie narastanie bezrobocia. W końcowych latach wojny tysiące mieszkańców cierpiało głód, często brakowało też dachu nad głową. 8 lipca 1917 w wyniku pożaru zostaje zniszczony Most Wysoki (odbudowany w 1926).
Dwudziestolecie międzywojenne.
Po przegranej przez Niemcy wojnie, w wyniku postanowień traktatu wersalskiego z 1919, od 28 listopada 1920 Elbląg, wraz ze wschodnią część Prus Zachodnich, pozostałą w Niemczech po utworzeniu polskiego województwa pomorskiego, czyli tzw. "polskiego korytarza", znalazł się w prowincji Prusy Wschodnie.
Ograniczenia narzucone Niemcom i utrata naturalnych rynków zbytu doprowadziły elbląską gospodarkę do kryzysu. Miasto było zmęczone wojną, panował chaos, bezrobocie i drożyzna. W tej sytuacji dochodziło do strajków i demonstracji, miały one jednak charakter bardziej ekonomiczny niż polityczny. W mieście działała komórka niemieckiej organizacji rewolucyjnej – Związku Spartakusa. 11 listopada 1918 powstała Rada Robotnicza, ale jej działalność ograniczyła się jedynie do zorganizowania kilku wieców. Niemniej jednak utworzona pod koniec tego roku placówka Komunistycznej Partii Niemiec miała pewne wpływy w mieście aż do dojścia Hitlera do władzy. 1 lipca 1922 miasto wykupiło spółkę prowadzącą komunikację tramwajową. W 1926 ustanowiono herb powiatu ziemskiego elbląskiego.
W latach 30. nastąpił ponowny rozkwit przemysłu na terenie miasta. Elbląg stał się jednym z większych miast garnizonowych w III Rzeszy. Przy utworzonym już w czasie wojny lotnisku powołano szkołę lotnictwa wojskowego. W różnych częściach miasta zostały wzniesione budynki koszarowe dla artylerii, kawalerii, wojsk inżynieryjnych i piechoty. Zgromadzenie w mieście dużej liczby wojska nadało mu charakter militarny, a z drugiej strony spowodowało rozwój infrastruktury miejskiej. Pojawiły się liczne nowe osiedla mieszkaniowe, oddano do użytku nowoczesny szpital wojskowy. W mieście rozwijało się szkolnictwo wyższe, powstała szkoła pedagogiczna, inżynierska, rolnicza. W mieście działały dwa muzea, biblioteka, archiwum, a wśród wielu urzędów i instytucji były także konsulaty: szwedzki, szwajcarski i polski. Pomyślny stan elbląskiej gospodarki pozwolił miastu na nowe inwestycje komunalne i przemysłowe. Podjęto modernizację sieci gazowej i kanalizacyjnej, przebudowano linie tramwajowe, unowocześniono port, pogłębiono tor wodny na Zalew Wiślany. 19 grudnia 1933 rozpoczęto budowę autostrady do Królewca. W 1934 przebywał w Elblągu Melchior Wańkowicz zbierając materiały do książki "Na tropach Smętka". W 1935 do Elbląga przybywa Adolf Hitler, aby odebrać nadane mu insygnia Honorowego Obywatela Miasta. W 1937 miasto obchodziło 700-lecie istnienia, uruchomiono wówczas pierwszą stałą linię autobusową. 
II wojna światowa.
Podczas II wojny światowej całe życie gospodarcze i społeczne Elbląga zostało podporządkowane potrzebom wojny. Miasto było przeludnione w wyniku przesiedleń ludności z Meklemburgii, Mazur, a także jeńców różnej narodowości. Od marca 1940 istniał tu jeden, a później dwa podobozy obozu koncentracyjnego Stutthof. Wprowadzono racjonowanie żywności. Od momentu rozpoczęcia wojny Elbląg był niemal nietknięty przez działania zbrojne. Gdy na początku 1945 Armia Czerwona wkroczyła do Prus Wschodnich, do Elbląga zaczęli napływać żołnierze niemieccy.
Pierwsze oddziały wojsk radzieckich zbliżyły się do Elbląga 23 stycznia 1945. W krótkim czasie miasto zostało otoczone z trzech stron. Rozpoczęły się ciężkie walki. Siły hitlerowskie w mieście liczyły ok. 10 tys. żołnierzy Wehrmachtu i ok. 4 tys. członków Volkssturmu. W czasie walk poległo ok. 5 tys. Niemców. Reszta trafiła do niewoli. W walkach poległo też 2731 żołnierzy radzieckich z 2 armii uderzeniowej i 5 armii pancernej gwardii 2 Frontu Białoruskiego pod dowództwem marszałka K. Rokossowskiego. Miasto zostało zajęte przez Armię Czerwoną 10 lutego 1945 roku (ku czci żołnierzy radzieckich po wojnie wzniesiono Pomnik Wdzięczności przy ul. Agrykola). Zaciekłe walki na przełomie stycznia i lutego obróciły miasto w gruzy. Zniszczona została zabytkowa zabudowa Starego Miasta, a także całe Śródmieście. Trudne do określenia były cywilne straty ludzkie. Wielu spośród przedwojennych elblążan utonęło w czasie panicznej ucieczki na Zachód przez Zalew Wiślany.
W pierwszych miesiącach po przejściu frontu Ziemie Północne były zarządzane przez radzieckie władze państwowe. W ciągu kilku miesięcy dokonały one wywozu w głąb ZSRR wszystkich maszyn i wyposażenia elbląskich fabryk. Zaraz po zajęciu miasta NKWD utworzyła w mieście obóz specjalny.
Powrót Elbląga do Polski i lata PRL.
19 maja 1945 r. przed ratuszem odbyła się uroczystość przekazania przez radziecką komendanturę symbolicznych kluczy do miasta władzom polskim, które powołano 3 kwietnia. Po ponad 170 latach Elbląg ponownie znalazł się w granicach państwa polskiego. Priorytetem nowych władz stało się nadanie miastu polskiej tożsamości narodowej i deportacja ludności niemieckiej. Żołnierze Armii Czerwonej zdemontowali i wywieźli do ZSRR wyposażenie fabryk, warsztatów, zakładów pracy. Rabowano dzieła sztuki, instrumenty muzyczne i wyposażenie ocalałych mieszkań. Rozpoczęto reorganizację nadwerężonej gospodarki oraz powolną odbudowę zniszczonego miasta. Już w 1945 wznowiły pracę Zakłady Młynarskie. W 1946 uruchomiono rozbudowaną elektrownię, Elbląskie Zakłady Piwowarsko-Słodownicze oraz Elbląskie Zakłady Naprawy Samochodów. Pod koniec 1948 rozpoczęły pracę Zakłady Mechaniczne im. gen. Karola Świerczewskiego (późniejszy Zamech), które w swych najlepszych latach zatrudniały 5100 pracowników). W 1948 powstały Elbląskie Zakłady Przemysłu Odzieżowego "Truso", a w roku następnym Zakłady Mięsne. Po wojnie rodowici elblążanie stanowili tylko 2% mieszkańców. W 1949 miasto dotknął terror stalinowski w wydarzeniach znanych jako sprawa elbląska. W latach 1951-1953 wybudowano od podstaw Zakłady im. Wielkiego Proletariatu, które produkowały meble przemysłowe. Od 1952 działała Spółdzielnia Pracy im. Feliksa Dzierżyńskiego produkująca kotły centralnego ogrzewania, a od 1963 Zakłady Tworzyw Sztucznych "Styren". W 1954 otworzono muzeum okręgowe. W 1956 oprócz istniejącej komunikacji tramwajowej uruchomiono linie autobusowe. Od roku akademickiego 1961/1962 działał w Elblągu punkt konsultacyjny Wyższej Szkoły Ekonomicznej w Sopocie, prowadziła działalność Wieczorowa Szkoła Inżynierska i Zawodowe Studium Administracyjne. Na początku lat 50. rozebrano ruiny Nowego Miasta i z częściowym zachowaniem dawnej siatki ulic wybudowano pierwsze powojenne osiedle mieszkaniowe. Zniszczone całkowicie Stare Miasto zostało uprzątnięte dopiero w latach 60., wcześniej prowadzono prace rozbiórkowe, a pozyskaną cegłę zgodnie w poleceniem władz centralnych transportowano na odbudowę Warszawy, a następnie Gdańska. Na początku lat 60. powstało przy ulicy Saperów pierwsze osiedle budynków wielorodzinnych położone poza śródmiejską częścią miasta, wytyczono wówczas nową arterię wschód-zachód nadając jej nazwę Alei Tysiąclecia, która prowadziła na nowo wybudowany most nad Elblągiem. Podczas wydarzeń z Grudnia 1970 roku w Elblągu zastrzelony został Tadeusz Marian Sawicz. Na początku lat 70. planowano na Starym Mieście początkowo wybudować blokowisko. Jednakże brak funduszy uniemożliwił realizację tych planów&lt;ref name="NG5/2004"&gt;&lt;/ref&gt;. 
W 1972 rozpoczęto odbudowę Starego Miasta, jako pierwsze odbudowano domy na ulicy Wigilijnej. W 1974 miasto zostało odznaczone Orderem Sztandaru Pracy I klasy.
Od 1 czerwca 1975 r. na mocy ustawy z 28 maja 1975 r. Elbląg stał się stolicą województwa, składającego się ze wschodnich powiatów dawnego województwa gdańskiego (powiaty: elbląski, nowodworski, malborski, sztumski, kwidzyński), oraz z północno-wschodniej części dawnego województwa olsztyńskiego (powiaty: braniewski, pasłęcki, część iławskiego i morąskiego).
W latach 80. przystąpiono do budowy centrum miasta, jednakże zamiast odbudowywać kamienice, by przypominały te przedwojenne, wybudowano nowoczesne budynki o kształcie i wielkości zbliżonej do średniowiecznych. Prace przy fundamentach spowodowały, że odsłonięto fundamenty dawnych budynków, co ukazało wielkość i zamożność dawnego Elbląga.
Przełom XX/XXI wieku.
Funkcję siedziby władz województwa Elbląg pełnił do ostatniej reformy administracyjnej w roku 1999, kiedy to od 1 stycznia 1999 r. na prawach powiatu wszedł w skład województwa warmińsko-mazurskiego wbrew woli większości mieszkańców, którzy opowiedzieli się zdecydowanie za przynależnością miasta do województwa pomorskiego. W tym samym roku jako jedyne polskie miasto dostał nagrodę Unii Europejskiej za dokonania w dziedzinie ekologii oraz prestiżową nagrodę – Flagę Europy.
Zabytki niezachowane.
Znaleziska archeologiczne.
W trakcie odbudowy średniowiecznych zabudowań miasta, trwającej od lat 80. XX wieku do dziś, na każdą parcelę wchodzą wcześniej archeolodzy. Znaleziono dzięki temu ponad 800 tysięcy przedmiotów, w tym kilka unikatów:
W 1982 elbląski archeolog dr Marek Jagodziński odkrył w Janowie Pomorskim 7 km na południe od miasta pozostałości wczesnośredniowiecznej osady. Prowadzone wykopaliska dowiodły, że było to poszukiwane długo, również w Elblągu, Truso wspominane przez średniowieczne kroniki.
W trakcie prowadzonego w roku 2015 nadzoru archeologicznego w Parku Modrzewie odsłonięto fragment fundamentu (szerokość 60 cm, wysokość 45 cm) niezachowanej do dziś willi Carla Zeisego.
Gospodarka.
Elbląg jest ośrodkiem przemysłu ciężkiego (turbiny parowe i gazowe), meblowego i spożywczego. W mieście znajdują się również tereny Warmińsko-Mazurskiej Specjalnej Strefy Ekonomicznej, .
Ochrona przyrody.
W granicach miasta znajduje się część Parku Krajobrazowego Wysoczyzny Elbląskiej oraz Obszaru Chronionego Krajobrazu Wysoczyzny Elbląskiej – Zachód.
Na terenie Elbląga znajduje się 68 pomników przyrody w tym 62 ożywionej i 6 nieożywionej.
Ludność.
Według danych GUS na dzień 30.06.2015 populacja Elbląga wynosiła 121 994 osób. Obecnie liczba mieszkańców systematycznie spada, na co ma wpływ duża emigracja zarobkowa (głównie do Wielkiej Brytanii i Niemiec), niż demograficzny oraz przenoszenie się zamożniejszych mieszkańców do podelbląskich gmin. Najwyższa jak dotąd liczba ludzi mieszkała w Elblągu na przełomie lat 1999/2000 i wynosiła 130 160.
Podział miasta na osiedla i dzielnice.
Elbląg nie jest podzielony na osiedla w sensie administracyjnym (jednostki pomocnicze gminy). Poniższy spis zawiera osiedla wyodrębnione historycznie, których nazwy są powszechnie używane przez mieszkańców, nie stanowią one jednak oficjalnego podziału administracyjnego miasta.
Transport.
Komunikacja miejska.
Komunikacja miejska w Elblągu nadzorowana jest przez Zarząd Komunikacji Miejskiej w Elblągu Sp. z o.o. Transport publiczny obsługiwany jest przez trzy spółki przewozowe: konsorcjum PKS Grodzisk Mazowiecki i PKS Gostynin, Tramwaje Elbląskie Spółka z o.o. oraz PKS Elbląg. ZKM w Elblągu swoim zasięgiem działania obejmuje miasto Elbląg oraz przyległe do niego miejscowości (Nowakowo, Gronowo Górne, Milejewo, Stagniewo).
Kolej.
Elbląg ma bezpośrednie połączenia kolejowe z Gdańskiem, Malborkiem, Tczewem, Słupskiem, Koszalinem, Szczecinem, Olsztynem, Ełkiem i Białymstokiem.
Przez miasto przebiega linia kolejowa z Berlina do Kaliningradu (którą uruchomiono 19 października 1852). W 1897 rozpoczęto pierwsze prace przy budowie Kolei Nadzalewowej, która miała połączyć Elbląg z Królewcem. W maju 1899 oddano do użytku odcinek z Elbląga do Fromborka, zaś we wrześniu tego roku odcinek z Fromborka do Braniewa.
W tym czasie oddano też do użytku dworzec Elbląg Miasto i dworzec Elbląg Angielskie Źródło. Stację Elbląg Miasto łączyła z dworcem wschodnim linia kolejowa biegnąca przez centrum miasta, która to istniała do początku lat 80. XX w.
Zlikwidowano ją po wybudowaniu w latach 1975-1982 obwodnicy łączącej stację Elbląg, czyli dawny Dworzec Wschodni ze stacją Elbląg Zdrój.
Dworce i przystanki kolejowe.
"obecne"
"dawne"
Transport lotniczy.
Elbląg nie posiada własnego portu lotniczego, a najbliższy znajduje się w odległości ok. 70 km od miasta w Gdańsku-Rębiechowie (Port lotniczy Gdańsk im. Lecha Wałęsy).
Lotnisko w Elblągu.
W dzielnicy Nowe Pole istnieje od 1915 trawiaste lotnisko, obecnie aeroklubowe. Rozważany jest jego rozwój i przekształcenie go w komunikacyjny port lotniczy. Alternatywnie, rozważane jest przekształcenie pobliskiej bazy lotniczej lotniska Królewo Malborskie, w port lotniczy Trójmiasto-Elbląg, która w przeciwieństwie do trawiastego lotniska w Elblągu posiada pełnowymiarową betonową drogę startową.
Transport wodny.
W Elblągu mieści się Port morski Elbląg. Nowe nabrzeża zostały wybudowane kosztem 30 mln złotych. Obok części towarowej znajduje się terminal pasażerski-punkt odpraw granicznych o przepustowości jednorazowej 200 osób i 30 samochodów osobowych.
Elbląg jako jeden z dwóch portów na Zalewie Wiślanym (obok Fromborka) ma morskie przejście graniczne, umożliwiające odprawę jachtów udających się do obwodu kaliningradzkiego.
Istnieją tutaj czynne drogowe mosty zwodzone.
Religia.
Na terenie miasta działalność religijna prowadzą następujące kościoły i wspólnoty:
Współpraca międzynarodowa.
Elbląg, dzięki swojemu przygranicznemu położeniu, bierze udział w wielu międzynarodowych projektach, w ostatnich latach szczególnie z obwodem kaliningradzkim. Z jego inicjatywy doszło także do stworzenia Euroregionu Bałtyk, który skupia w swoim zasięgu większość krajów wokół Morza Bałtyckiego (Polska, Rosja, Litwa, Łotwa, Szwecja, Dania). W Elblągu mieści się także jego stały międzynarodowy sekretariat.
Na dowód ścisłej współpracy między Elblągiem a francuskim miastem Compiègne powstało w 2007 roku rondo przy ulicy Nowowiejskiej, które zostało nazwane Rondem Compiègne, a rondo, którym z Compiègne wyjeżdża się w kierunku Paryża, w 2003 roku zostało nazwane Rondem Elblag.
Obecnie Elbląg współpracuje z 13 miastami partnerskimi z 12 krajów:
W listopadzie 2007 miasto Elbląg zainicjowało międzynarodową kampanię „Bałtycka Ukraina”, ukierunkowaną na zbudowanie w mieście ukraińskiego kompleksu portowego. Plany te są ściśle powiązane z decyzją rządu RP o budowie kanału przez Mierzeję Wiślaną. W 2017 roku rondu na skrzyżowaniu ulic Konopnickiej i Niepodległości nadano nazwę ukraińskiego miasta partnerskiego, Tarnopola, w związku z 25-leciem współpracy obu miast.
Honorowi obywatele miasta Elbląga.
Tytuły nadane przed 1945 r.
Tytuły nadane po 1989 r. (data nadania tytułu)
Parlamentarzyści.
Mieszkańcy Elbląga wybierają posłów z okręgu wyborczego Elbląg, senatora z okręgu wyborczego nr 84, a posłów do Parlamentu Europejskiego z okręgu wyborczego nr 3.

</doc>
<doc id="1470" url="https://pl.wikipedia.org/wiki?curid=1470" title="Elbląg (rzeka)">
Elbląg (rzeka)

Elbląg (niem. "Elbing") – rzeka na Żuławach Wiślanych o długości 14,5 km. Wypływa z jeziora Druzno, uchodzi do Zalewu Wiślanego.
Rzeka jest żeglowna na całej długości (od Elbląga traktowana jako morskie wody śródlądowe).
Poprzez kanały posiada połączenie:
Rzeka Elbląg stanowi oś żeglugowego systemu transportowego. Poprzez Kanał Jagielloński i Nogat posiada połączenie z Wisłą, a poprzez Kanał Elbląski łączy się z Iławą i Ostródą. Na rzece znajduje się port morski w mieście Elbląg. W okresach suszy woda z rzeki za pomocą systemu melioracyjnego może być wykorzystywana do nawadniania obszarów rolniczych na Żuławach Elbląskich.
W zależności od wahań poziomu wody w Zalewie Wiślanym spowodowanych "wpychaniem" wód Bałtyku do Cieśniny Piławskiej przez silne wiatry północne i północno-wschodnie, kierunek spływu wód rzeki Elbląg odwraca się i powoduje spiętrzenie wody jeziora Druzno. Znaczna część dorzecza rzeki obejmuje tereny depresyjne, w tym największą depresję w Polsce 1,8 m p.p.m., położoną na południowy wschód od miasta Elbląg pomiędzy wsią Raczki Elbląskie a rzeką Tyną. Prawie na całej długości rzeki Elbląg i jej dopływach w ich dolnym biegu występują obwałowania przeciwpowodziowe.
Według niektórych źródeł do przepływu rzeki zalicza się również przepływ rzeki Dzierzgoń wypływającej w powiecie sztumskim, o długości 45 kilometrów i uchodzącej do jeziora Druzno.
W północno-wschodniej części zlewni występuje Park Krajobrazowy Wysoczyzny Elbląskiej. Poza tym na jej terenie znajdują się:
Do rzeki Elbląg uchodzą cieki:

</doc>
<doc id="1472" url="https://pl.wikipedia.org/wiki?curid=1472" title="Ewolucja człowieka">
Ewolucja człowieka



</doc>
<doc id="1473" url="https://pl.wikipedia.org/wiki?curid=1473" title="Endecja">
Endecja



</doc>
<doc id="1474" url="https://pl.wikipedia.org/wiki?curid=1474" title="Elagabal">
Elagabal



</doc>
<doc id="1475" url="https://pl.wikipedia.org/wiki?curid=1475" title="Elegia">
Elegia

Elegia (gr. ἐλεγεία "elegeia" – pieśń żałobna) – utwór liryczny o treści poważnej, refleksyjny, utrzymany w tonie smutnego rozpamiętywania, rozważania lub skargi, dotyczący spraw osobistych lub problemów egzystencjalnych (przemijanie, śmierć, miłość); wyróżnia się elegie miłosne i patriotyczne. Należy do najbardziej charakterystycznych form liryki bezpośredniej.
W poezji starogreckiej, skąd wywodzi się ten gatunek, obowiązkowa była dla niego forma wierszowa dystychu elegijnego. Mistrzem elegii w poezji starorzymskiej był Owidiusz, po nim gatunek ten uprawiali z powodzeniem John Milton, Johann Wolfgang von Goethe, Aleksander Puszkin, Rainer Maria Rilke.
W Polsce elegie pisali m.in. Jan Kochanowski, Władysław Broniewski, Franciszek Karpiński ("") i Krzysztof Kamil Baczyński ("]"). Tomik zatytułowany "Elegie" jest najbardziej znanym dziełem czeskiego poety Jiříego Ortena.
W historii literatury angielskiej jedną z najważniejszych elegii jest "Beowulf" nieznanego autorstwa – utwór pochodzi z ok. VIII wieku. Następnie: ', ' – oba teksty są elegiami egzystencjalnymi, przedstawiają dwa punkty widzenia mężczyzn udręczonych, skazanych na banicję.

</doc>
<doc id="1476" url="https://pl.wikipedia.org/wiki?curid=1476" title="Epos">
Epos

Epos (gr. έπος, "epos" „słowo”), także: ‘epopeja’, ‘poemat heroiczny’, czasem również ‘poemat epicki’ – jeden z głównych i najstarszych gatunków epiki. Tradycyjne eposy to dłuższe poematy narracyjne o specyficznej konstrukcji elementów świata przedstawionego, podporządkowanej funkcji parenetycznej i afirmującej etos społeczności, w której zostały napisane. Eposy charakteryzują również: typ bohatera (heros, początkowo król bądź rycerz, później także heros duchowy; postać programowo idealizowana), podniosły rejestr stylistyczny, koncepcja podmiotu twórczego (narracja trzecioosobowa) oraz tworzywo poetyckie (początkowo autorzy eposów sięgali najczęściej po zdarzenia mityczne bądź legendarne, do „czasu ojców”, później także po wydarzenia historyczne). Obejmuje utwory najczęściej poetyckie, np. "Iliada", "Odyseja", choć eposami (epopejami) nazywa się także powieści o rozmachu epickim, ukazujące dzieje życia legendarnych, częściowo lub całkiem historycznych bohaterów lub też opowieść o prapoczątkach jakiegoś narodu, a także omawia początki kultów religijnych. Często opowiadają nie tylko o postaciach ludzkich, ale też boskich, magicznych czy demonicznych (np. "Ramajana"). Najwcześniejsze eposy mają charakter zbioru opowieści mitycznych i wywodzą się często z czasów, w których grupa ich autorów nie znała pisma. Epos doby pisma wykształcił odmianę historyczną, ukazującą wydarzenia jeszcze nieprzebrzmiałe (np. poematy Samuela Twardowskiego, "Transakcyja wojny chocimskiej" Wacława Potockiego), a także biblijną, która za pomocą środków biblijnych przedstawia losy bohaterów Pisma św., zwłaszcza Chrystusa, ale też całą historię świętą (odmianę tę reprezentują np. "Chrystiada" Marca Girolama Vidy, "Raj utracony" Johna Miltona).
Prostszą, krótszą odmianą jest poemat epicki (np. "Beowulf"). Często głównym wątkiem eposu jest historia jednego rodu panującego ("Pieśń o Nibelungach") lub dzieje życia jednego niezwykłego heroicznego bohatera ("Odyseja"), ale może on też przedstawiać opowieść o całym ludzie ("Kalevala"), o jego początkach ("Eneida"), także o jego bogach ("Dzieje Dionizosa", "Edda") i przodkach (eposy Hezjoda). Dawne eposy i poematy epickie przeznaczone były do recytowania lub do śpiewania, często zawierały elementy rytuałów i były używane w celach religijnych. Stanowiły też często zapis religii i mitologii danego ludu, jego wartości moralnych, obyczajowości i najstarszego języka.
Za ostatnią „żywą epopeję” uważa się nawiązującą do początków buddyzmu tybetańskiego "Epopeję o Królu Gesarze". Wśród Tybetańczyków i Mongołów znaleźć można około 140 pieśniarzy, którzy zajmują się tą balladą. Eposy powstałe w czasach późniejszych (począwszy od czasów rzymskich, ale i np. "Raj utracony" i "Pan Tadeusz") są dziełami czysto literackimi, które nie wywodzą się bezpośrednio z ustnej tradycji epickiej.
Eposy były pisane przy użyciu różnych form wersyfikacyjnych. Eposy greckie oraz rzymskie były układane iloczasowym heksametrem daktylicznym, bez podziału na strofy i bez rymu. 
W krajach romańskich eposy pisano przeważnie oktawą (abababcc), a niekiedy tercyną (aba bcb cdc...). Oktawa była budulcem wielu renesansowych eposów włoskich, hiszpańskich i portugalskich. Przy jej użyciu tworzyli swoje najsławniejsze dzieła Luigi Pulci, Matteo Maria Boiardo, Ludovico Ariosto, Torquato Tasso, Lucrezia Marinella, Alonso de Ercilla y Zúñiga, Luís de Camões, Vasco Mouzinho de Quevedo, Francisco de Sá de Meneses i Gabriel Pereira de Castro. 
W Anglii w roli tworzywa eposów stosowano albo wiersz biały (blank verse), albo siedmiowersową strofę królewską (ababbcc) i dziewięciowersową strofę spenserowską (ababbcbcc).
W Polsce eposy pisze się przeważnie trzynastozgłoskowcem rymowanym parzyście. W ten sposób pisał między innymi Wacław Potocki. Tradycyjnie rymowanym trzynastozgłoskowcem tłumaczono także starożytny heksametr.
Niekiedy eposy są pisane przy użyciu innych rodzajów wiersza. Serbowie stosują dziesięciozgłoskowiec, zaś fińska Kalevala została skomponowana zasadniczo ośmiozgłoskowcem. W epice staroangielskiej wykorzystywano nie rym, ale aliterację, która budowała wiersz aliteracyjny

</doc>
<doc id="1477" url="https://pl.wikipedia.org/wiki?curid=1477" title="Atena Eudokia">
Atena Eudokia

Atena, Elia Eudokia Augusta, "Aelia Eudocia Augusta" (ur. 401, zm. 20 października 460) – właściwie "Athenais" (Atenaida), Greczynka, córka greckiego filozofa Leoncjusza, została w 421 poślubiona cesarzowi bizantyńskiemu – Teodozjuszowi II. 
Życiorys.
Pod wpływem siostry męża Aelii Pulcherii została ochrzczona i przyjęła imię "Elia Eudokia". Dzięki wsparciu Aelii uzyskała mocną pozycję na dworze uwieńczoną nadaniem tytułu cesarzowej. Oskarżona w 444 roku o romans z urzędnikiem dworskim została zmuszona do opuszczenia dworu i udała się na pielgrzymkę do Jerozolimy, gdzie pozostała do śmierci. 
Matka Arkadiusza, Flacylli oraz Licynii Eudoksji (żony Walentyniana III). Natomiast jej wnuczka, Eudokia, została żoną Huneryka, władcy Wandalów, a także Palladiusza, syna cesarza Petroniusza.

</doc>
<doc id="1478" url="https://pl.wikipedia.org/wiki?curid=1478" title="Eurazja">
Eurazja

Eurazja (nienormatywna forma: Euroazja) – zbiorcze określenie na obszary Europy i Azji, obejmujące powierzchnię około 55 mln km², zamieszkane przez 4,918 mld ludzi, co stanowi 70,65% ludności świata (z czego 60,25% zamieszkuje Azję). Kontynent eurazjatycki dzieli się umownie na Europę i Azję, określane tradycyjnie mianem kontynentów (Europa, tak samo jak Półwysep Indyjski, jest raczej subkontynentem Eurazji). Termin "Eurazja" wprowadzono w XIX wieku w celu podkreślenia ścisłego połączenia Azji z Europą.
Problem podziału na Europę i Azję nie jest łatwy, szczególnie tam, gdzie brakuje wyraźnych granic naturalnych. Współcześnie przyjmuje się najczęściej, że granica ta przebiega wzdłuż wschodnich podnóży gór Uralu, rzeki Emby, północnym wybrzeżem Morza Kaspijskiego, Obniżeniem Kumsko-Manyckim do ujścia Donu do Morza Azowskiego, dalej Cieśniną Kerczeńską, przez Morze Czarne, cieśninę Bosfor, Morze Marmara, cieśninę Dardanele, po wschodnie wybrzeże Morza Egejskiego. Różne źródła podają różny przebieg granicy, a jedynymi powszechnie przyjętymi jej elementami są góry Ural i cieśnina Bosfor.
Eurazja spośród wszystkich lądów Ziemi wyróżnia się dużymi kontrastami ukształtowania. To tu występują najwyższe łańcuchy górskie, wysokie i rozległe wyżyny, wielkie niziny i największe depresje. Również pod względem kulturowo-politycznym Eurazja jest niezwykle zróżnicowana. Leżą na niej najbogatsze (Szwajcaria) i najbiedniejsze (Jemen) państwa świata, najbardziej totalitarne dyktatury (Korea Północna) i gospodarcze potęgi (Niemcy, Japonia), najgęściej (Bangladesz) i najrzadziej (Mongolia) zaludnione państwa.

</doc>
<doc id="1480" url="https://pl.wikipedia.org/wiki?curid=1480" title="Enklawa">
Enklawa

Enklawa – terytorium otoczone ze wszystkich stron terytorium lądowym innego państwa. Jeżeli stanowi ono część jakiegoś państwa, to jest ono jego eksklawą. Na przykład San Marino stanowi enklawę w terytorium Włoch, zaś gmina Campione stanowi enklawę w terytorium Szwajcarii i eksklawę Włoch. Terminów „enklawa/eksklawa” używa się także dla określenia tego typu terytoriów w skali subnarodowej. Terminu „enklawa” używa się także w przypadku, gdy teren jednego rodzaju jest otoczony terenem innego rodzaju, np. las liściasty na terenie porośniętym lasem iglastym.
Wyraz ten pochodzi z języka francuskiego, który stanowił język dyplomacji. Czasownik "enclaver", oznaczające „wbijać się klinem”, pochodzi od łacińskiego "in-clavo", gdzie "clavus" to klin, gwóźdź.
Enklawy są dziedzictwem historycznym i politycznym, choć niekiedy niektóre terytoria stały się faktycznymi enklawami na skutek np. zmiany biegu rzeki.
Życie w enklawach może być niewygodne z racji oderwania od terytorium macierzystego. Stąd też zainteresowane państwa starają się podpisać umowy regulujące takie kwestie jak dostawy energii, przepływ osób, usługi publiczne dla ludności itp. Najlepiej zdefiniowana jest sytuacja enklaw w Europie, zwłaszcza w epoce swobodnego przepływu osób w strefie Schengen. Natomiast w Azji enklawy nadal stanowią nierzadko przedmiot sporów terytorialnych.
Przykłady enklaw.
Państwa-enklawy.
Państwa-enklawy, otoczone ze wszystkich stron przez inne państwo, nie są eksklawami. Istnieją trzy takie państwa:
Terytoria-enklawy.
Cechą charakterystyczną „prawdziwych” enklaw jest to, że by się do nich dostać trzeba poruszać się po terytorium innego państwa. Najbardziej znaną enklawą w Europie był Berlin Zachodni; w historii Polski należący do niej w latach 1412–1769 tzw. zastaw spiski, czyli grupa zamków i 13 miast spiskich będących polskimi eksklawami, stanowiącymi enklawy na terenie Królestwa Węgier. Inne mniej znane przykłady, za to nadal istniejące, to:
Poza Europą enklawy można znaleźć także w Azji:
Szczególne przypadki enklaw stanowią wyspy otoczone wodami terytorialnymi innego państwa. Są to:
Do niektórych terytoriów oderwanych od zasadniczego obszaru kraju można dotrzeć przez wody międzynarodowe. Często w stosunku do nich błędnie używa się określenia enklawa, faktycznie są to jednak tylko eksklawy. Dobrze znanym w Polsce przykładem eksklawy jest obwód kaliningradzki, do którego dotrzeć można albo przez Litwę, albo przez Polskę, albo przez Morze Bałtyckie.
Praktyczne enklawy.
Niektóre terytoria, nieoddzielone fizycznie od danego kraju, są łatwiej dostępne poprzez terytorium sąsiedniego kraju. Dzieje się tak zwłaszcza w obszarach górskich oraz tam gdzie jedyna dostępna droga prowadzi przez obce terytorium. Takie terytoria nie są teoretycznie enklawami, ale w praktyce tak. Kilka przykładów takich obszarów:

</doc>
<doc id="1481" url="https://pl.wikipedia.org/wiki?curid=1481" title="Eksklawa">
Eksklawa

Eksklawa – część terytorium państwa lub innej jednostki administracyjnej, położona w oddzieleniu od głównego jego obszaru, lecz na tym samym obszarze lądowym (przeważnie kontynent). Może być otoczona terytorium innego państwa lub jednostki administracyjnej, stanowi wtedy jednocześnie enklawę.
Eksklawy międzynarodowe.
Istniejące.
Przykłady eksklaw międzynarodowych:
W Polsce.
Województwa.
Obecnie w Polsce nie ma eksklaw na poziomie wojewódzkim. Jednak w jednym miejscu niemal istnieje eksklawa – mający powierzchnię ok. 6,3 ha fragment gminy Jutrosin leżącej w województwie wielkopolskim (powiat rawicki) znajduje się na terenie gminy Cieszków leżącej w województwie dolnośląskim (powiat milicki), a z właściwym terytorium gminy Jutrosin styka się na szerokości drogi polnej łączącej oba obszary.
Do końca 1998 roku istniała eksklawa wojewódzka. W latach 1975–98 wieś Zabieżki była częścią województwa warszawskiego otoczoną ze wszystkich stron obszarem województwa siedleckiego.
Powiaty.
Przykłady eksklaw powiatów w Polsce:
Gminy.
Liczne gminy składają się z kilku fragmentów (część z nich to jednocześnie eksklawy powiatowe) – najczęściej dzieje się tak, gdy miasto, będące siedzibą gminy jest osobną gminą miejską, pozostawiając rozkawałkowaną gminę wiejską, np. gmina Włodawa, gmina Koło, gmina Głogów, gmina Tarnów (składa się z trzech fragmentów), gmina Łowicz. Rzadsze są przypadki, gdy obszar gminy rozdzielony jest terenem innej gminy, np.: gmina Mierzęcice, gmina Brwinów, gmina Stopnica, gmina Lubanie.
Wykaz eksklaw gminnych (bez eksklaw będących jednocześnie na poziomie powiatowym wymienionych w sekcji „Powiaty”)
Miasta.
W tabeli poniżej wymienione są tylko eksklawy w miastach niebędących samodzielnymi gminami (miasta w gminach miejsko-wiejskich). W tych przypadkach są to eksklawy obszaru miasta znajdujące się na obszarze wiejskim danej gminy. Ponadto eksklawy posiadają opisane powyżej: miasto na prawach powiatu Żory oraz miasto Mielec będące gminą miejską.
Ponadto w przypadku trzech miast występuje przypadek istnienia eksklawy obszaru wiejskiego na terenie miasta (enklawa w mieście). Pierwszy to, opisany wyżej, przypadek Zamościa (jest to eksklawa na poziomie powiatowym), drugim opisany wyżej przypadek Tychowa, trzecim jest zaś przypadek miasta Polkowice, znajdującego się w powiecie polkowickim (gmina miejsko-wiejska Polkowice) – eksklawa obszaru wiejskiego znajduje się na terenie miasta, ok. 30 m od właściwego obszaru terenu wiejskiego i ma ok. 0,3 ha powierzchni.

</doc>
<doc id="1482" url="https://pl.wikipedia.org/wiki?curid=1482" title="Wartościowanie leniwe">
Wartościowanie leniwe

Wartościowanie leniwe (ang. "lazy evaluation", ewaluacja leniwa) – strategia wyznaczania wartości argumentów funkcji tylko wtedy, kiedy są potrzebne (na żądanie).
Zaletami tego podejścia są możliwość obliczenia wartości funkcji nawet wtedy, gdy nie jest możliwe wyznaczenie wartości któregoś z jej argumentów, o ile tylko nie jest on używany, wzrost wydajności dzięki uniknięciu wykonywania niepotrzebnych obliczeń oraz możliwość tworzenia nieskończonych struktur danych. Wadą wartościowania leniwego jest to, że mogą nie wystąpić (być może oczekiwane) skutki uboczne procesu wyznaczania wartości argumentów.
Przeciwieństwem wartościowania leniwego jest wartościowanie zachłanne, stosowane w większości popularnych języków programowania.
Przykładem wartościowania leniwego jest obsługa operatorów logicznych w większości języków programowania. Wyrażenie w języku C postaci:
a = funkcja_B(b) || funkcja_C(c) ? b : c;
Przypisanie do zmiennej codice_1 wartości codice_2 nastąpi gdy:
W pierwszym przypadku nie nastąpi wywołanie funkcji codice_4 w związku z czym nie nastąpią jej efekty uboczne. W podobny sposób wykonany zostanie kod:
 a = funkcja_B(b) &amp;&amp; funkcja_C(c) ? b : c;
Przypisanie do zmiennej codice_1 wartości codice_9 nastąpi gdy:
Z tego typu leniwego wartościowania korzysta idiom perla postaci:
 funkcja(X) || die("mamy problem");
który określa, że jeżeli codice_14 nie zwróci "prawdy", to znaczy, że mamy problem i należy zakończyć wykonanie programu.
Jednak oba wzmiankowane powyżej języki dokonują zachłannej ewaluacji wywołań funkcji, ponieważ w obu językach wartości argumentów funkcji są obliczane przed jej wykonaniem.
Przykładami języków stosujących leniwe wartościowanie są Haskell oraz D.

</doc>
<doc id="1483" url="https://pl.wikipedia.org/wiki?curid=1483" title="Ewaluacja zachłanna">
Ewaluacja zachłanna



</doc>
<doc id="1485" url="https://pl.wikipedia.org/wiki?curid=1485" title="Etyka">
Etyka

Etyka (z , "ēthos" – stałe miejsce zamieszkania, obyczaj, zwyczaj) – dział filozofii dotyczący powinności moralnej: moralnego dobra lub zła. Etyka wyznacza jej szczegółową treść, czyli słuszności, a także szuka ostatecznych wyjaśnień norm, genezy zła i sposobów jego przezwyciężenia. 
Etykę odróżnia się od moralności i innych badających ją dyscyplin jak psychologia moralności, socjologia moralności, etologia czy teologia moralna.
Etyka.
Etyka jest analizą dlaczego powinniśmy działać w określony sposób (powinność moralna). 
Terminem „etyka” oznacza się : 
(1) teorię powinności moralnej lub moralnej wartości postępowania. Czasami nazywaną etyką normatywną jako przeciwstawienie etyce opisowej; pierwotne i właściwe znaczenie terminu zapoczątkowane przez Sokratesa - ojca etyki; 
(2) teorię faktycznie uznawanych w określonym środowisku społecznym (etos), a często także praktykowanych w nim, norm moralnych postępowania (moralność); określaną jako etyka opisowa, rzadziej jako "etologia". W tym przypadku etykę utożsamia się z teorią etosu (zwykle z historią moralności, etnologią moralności, psychologią lub socjologią moralności); 
(3) przeświadczenia(oceny) i praktyki moralne danej społeczności lub nawet poszczególnych jej przedstawicieli. To stanowisko utożsamia etykę z samym etosem. "etosem, który jest teorią faktycznie uznawanych i często praktykowanych w danym środowisku norm moralnych" 
(4) metaetykę, zwaną również filozofią moralną (moral philosphy).Takie rozumienie rozpowszechnione jest w obszarze języka angielskiego. Jest to mylne utożsamienie etyki z teorią etyki, która dotyczy podstaw etyki i ma na celu określenie warunków naukowego charakteru etyki jako nauki. 
Moralność to sposób postępowania człowieka zgodny z prawdziwościowo-dobrym stanem rzeczy. 
Celem etyki jest dochodzenie do źródeł powstawania moralności, badanie efektów, jakie moralność lub jej brak wywiera na ludzi oraz szukanie podstawowych przesłanek filozoficznych, na podstawie których dałoby się w racjonalny sposób tworzyć zbiory nakazów moralnych. Poglądy etyczne przybierają zwykle formę teorii, na którą składa się zespół pojęć i wynikających z nich twierdzeń, na podstawie których można formułować zbiory nakazów moralnych. 
Teorie etyczne mogą być zarówno próbą udowadniania słuszności funkcjonujących powszechnie nakazów moralnych, jak i mogą stać w ostrej opozycji do powszechnej moralności, kwestionując zasadność części bądź nawet wszystkich aktualnie obowiązujących w danym społeczeństwie nakazów moralnych.
Podział teorii etycznych.
Istnieje wiele różnych teorii etycznych, które różnią się od siebie tym, jakich działań od nas wymagają i w jaki sposób to argumentują.
Współczesny podział teorii etycznych zaproponowany przez Rudolfa Carnapa:
Do czasów współczesnych opracowano już praktycznie systemy etyczne będące wszystkimi możliwymi kombinacjami tych podziałów. Obecnie praca etyków koncentruje się głównie na analizowaniu i ewentualnym uprecyzyjnianiu istniejących już systemów. Szczególnie duże postępy odnotowuje się w etyce chrześcijańskiej i systemach nominalistycznych.
Niektóre systemy etyczne.
Chrześcijaństwo.
W chrześcijaństwie podstawą etyki jest nauka Jezusa Chrystusa. Etyka chrześcijańska nie jest kazuistyczna (rób to, nie rób tego), ale polega na wzroście w cnotach, nadprzyrodzonych (dzięki łasce, od chwili chrztu) i ludzkich.
Wskazania dla moralności człowieka zawarte są w samej naturze człowieka (tzw. prawo natury), a także – dzięki Objawieniu – w Piśmie Świętym i, według Kościoła Katolickiego, Magisterium Kościoła.
Ze Starego Testamentu ponadczasowy charakter ma dekalog. W Nowym Testamencie etyka skupia się na przykazaniu miłości i nauce Ośmiu błogosławieństw. Przede wszystkim jednak przykładem jest integralne życie Jezusa, doskonałego człowieka.
Islam.
Zachowania moralne opisane są w dwóch źródłach islamu: "Koranie" i tradycji proroka Mahometa ("sunna"). Oprócz tych dwóch źródeł istnieją inne. Islam odnosi się z uznaniem do dekalogu żydowsko-chrześcijańskiego.
Mahajana.
Według buddyzmu mahajany etyka skodyfikowana w postaci nakazów (pięć szkodliwych działań w buddyzmie, dziesięć złych uczynków, ślubowania bodhisattwy) jest swoistą protezą, która jest konieczna ludziom niedostatecznie rozwiniętym duchowo. Jeśli bowiem człowiek posiada rozwinięte współczucie dla innych czujących stworzeń i stara się żyć z nim w zgodzie, to żadne etyczne zasady nie są mu potrzebne – przeciwnie, zasady etyczne w mahajanie są widziane jako uproszczone opisy zachowań rozwiniętego duchowo człowieka.
Echa podobnych koncepcji przewijają się także w myśli filozoficznej Zachodu (nietzscheanizm, humanizm – patrz następny podpunkt), a także w pewnym stopniu w chrześcijaństwie, które uważa, iż człowiek święty jest całkowicie wolny pomimo skrupulatnego przestrzegania zasad moralnych. Nie ma on bowiem pokusy występowania przeciwko tym zasadom, a więc moralność go nie ogranicza, a jest jedynie opisem sposobu jego życia podobnie jak w buddyzmie.
Humanizm.
Zgodnie z deklaracją amsterdamską, przyjętą na Światowym Kongresie Humanistycznym 3-6 lipca 2002 w Noordwijkerhout, Holandia:
"Humanizm uznaje wartość, godność i autonomię każdej jednostki. Humaniści popierają prawo każdego człowieka do jak największego zakresu wolności, pozostającego jednak w zgodzie z prawami innych. Humaniści mają obowiązek dbania o ludzkość, w tym przyszłe pokolenia. Humaniści uważają moralność za wrodzony element ludzkiej natury oparty na zrozumieniu i trosce o innych, niepotrzebujący zewnętrznego sankcjonowania".

</doc>
<doc id="1488" url="https://pl.wikipedia.org/wiki?curid=1488" title="Europejskie Stowarzyszenie Wolnego Handlu">
Europejskie Stowarzyszenie Wolnego Handlu

Europejskie Stowarzyszenie Wolnego Handlu, ESWH, EFTA (od ang. European Free Trade Association) – międzynarodowa organizacja gospodarcza powstała 3 maja 1960 roku na mocy konwencji sztokholmskiej (podpisanej 4 stycznia 1960), mająca na celu utworzenie strefy wolnego handlu artykułami przemysłowymi między państwami członkowskimi drogą redukcji ceł i ograniczeń importowych. Siedziba Sekretariatu mieści się w Genewie.
Utworzenie nowej organizacji postanowiono w Sztokholmie 20 listopada 1959. Pierwotnymi członkami były: Austria, Dania, Norwegia, Portugalia, Szwajcaria, Szwecja i Wielka Brytania. Z czasem większość członków wystąpiła, wybierając członkostwo w konkurencyjnej i o zdecydowanie większej integracji ekonomicznej EWG. W latach 1986–1994 członkiem organizacji była również Finlandia.
Inicjatorami powstania EFTA byli Brytyjczycy, traktujący układ jako przeciwwagę dla stworzonej rok wcześniej Europejskiej Wspólnoty Gospodarczej, redukującej stawki celne między swoimi członkami, co założyciele EFTA uznali za zagrożenie dla własnych interesów gospodarczych. W chwili powstania PKB EFTA wynosiła 84 mld USD wobec 132 mld USD PKB EWG. Konkurencja z EWG okazała się jednak problematyczna: kraje tworzące EFTA nie były wystarczająco powiązane gospodarczo, czemu nie sprzyjało ich rozrzucenie na kontynencie (w przeciwieństwie do tworzącej zwarty blok terytorialny EWG). Układ sztokholmski nie poruszał kwestii przepływu osób i kapitałów oraz unii celnej między członkami, a ze względu na słabość instytucjonalną możliwość skoordynowanego wpływania na państwa trzecie była ograniczona. W tej sytuacji już 1961 Dania i Wielka Brytania wystąpiły o przystąpienie do EWG (co nastąpiło w 1972), dając asumpt do twierdzeń, że traktowały uczestnictwo w EFTA jedynie jako kartę przetargową w negocjacjach z EWG.
Strefa wolnego handlu między państwami ESWH powstała w 1968. W 1977 państwa EWG i ESWH utworzyły wspólną strefę wolnego handlu towarami przemysłowymi. W 1992 EWG i ESWH porozumiały się w sprawie utworzenia wspólnej strefy wolnego handlu na wszystkie towary. Porozumienie w tej sprawie, nieobejmujące jednak Szwajcarii, weszło w życie 1 stycznia 1994, tworząc Europejski Obszar Gospodarczy.
Działalność EFTA jest znacznie słabsza niż działalność byłej EWG. W przeciwieństwie do EWG, układ o EFTA nie zawiera postanowień dotyczących wspólnej polityki gospodarczej, swobodnego przepływu kapitałów, siły roboczej. Całość jego postanowień ogranicza się do handlu zagranicznego. W odróżnieniu od EWG, więzy łączące kraje członkowskie ESWH są bardzo luźne i nie są instytucjonalne.
Struktura organizacyjna.
Rada.
Rada ESWH z siedzibą w Genewie jest naczelnym organem Stowarzyszenia. W jej skład wchodzi po jednym przedstawicielu rządu każdego państwa członkowskiego. Każde państwo dysponuje jednym głosem, co do zasady decyzje zapadają jednomyślnie. Rada zbiera się dwa razy w miesiącu na szczeblu stałych przedstawicieli i dwa razy w roku na szczeblu ministrów (najczęściej handlu). Państwa kolejno, przez półroczne kadencje, przewodniczą Radzie.
Rada ma prawo do wydawania decyzji (nie są bezpośrednio skuteczne) i zaleceń (nie mają wiążącej mocy prawnej). Wykonuje kompetencje nadane jej przez konwencję sztokholmską i nadzoruje jej stosowanie, może też wprowadzać do niej poprawki (w przypadkach określonych w konwencji konieczna jest akceptacja zmian ze strony państw członkowskich). Ponadto Rada:
W zarządzaniu strefą wolnego handlu Radę wspierają powołane przez nią komitety ekspertów, pracujące w trybie stałym. Doraźnie natomiast zwoływane są grupy ekspertów dla rozwiązania określonego problemu. Oba ciała mają wyłącznie uprawnienia doradcze i muszą składać się z przedstawicieli wszystkich państw członkowskich. Funkcjonuje dziesięć stałych komitetów, w tym:
Inne organy.
Bieżącymi sprawami zajmuje się Sekretariat, nadzorowany przez Sekretarza Generalnego. Siedzibą Sekretariatu jest Genewa, natomiast jego część mieści się w Brukseli, gdzie personel zajmuje się sprawami EOG. Sekretariat odpowiedzialny jest za obsługę Rady i komitetów. Oficjalnym językiem urzędowym jest angielski.
W związku z utworzeniem Europejskiego Obszaru Gospodarczego zostały wdrożone pewne zmiany organizacyjne. Powołano Stały Komitet Państw ESWH, który zajmuje się podejmowaniem decyzji związanych z uczestnictwem w EOG (Szwajcaria, w następstwie negatywnego wyniku referendum, nie jest członkiem tego porozumienia, jednak osiągnęła zbliżoną sytuację prawną w drodze umów bilateralnych). W skład Komitetu wchodzą przedstawiciele Islandii, Norwegii i Liechtensteinu, a także, posiadający status obserwatorów, reprezentanci Szwajcarii i Władzy Nadzorczej ESWH (ang. "EFTA Surveillance Authority"). Ta ostatnia czuwa w imieniu ESWH nad przestrzeganiem przyjętych przez EOG reguł funkcjonowania (podobnie jak czyni to Komisja Europejska w przypadku UE). Stały Komitet ma pod sobą 5 podkomitetów (m.in. podkomitety ds. wolnego przepływu towarów, ds. wolnego przepływu kapitału i usług, ds. wolnego przepływu osób, ds. polityk horyzontalnych i towarzyszących).
Stały Komitet jest wspomagany przez Trybunał ESWH. Organ ten składa się z trzech sędziów (po jednym z Norwegii, Islandii i Liechtensteinu) nominowanych na sześcioletnią kadencję. Rozstrzyga sprawy wniesione zazwyczaj przez Władzę Nadzorczą przeciwko państwom ESWH. Ponadto prowadzi mediacje i rozstrzyga spory między państwami ESWH na tle ich udziału w EOG, a także dokonuje interpretacji porozumienia o utworzeniu EOG.

</doc>
<doc id="1489" url="https://pl.wikipedia.org/wiki?curid=1489" title="Europejski Obszar Gospodarczy">
Europejski Obszar Gospodarczy

Europejski Obszar Gospodarczy, EOG (ang. "European Economic Area", "EEA") – strefa wolnego handlu i wspólny rynek, obejmujące państwa Unii Europejskiej i Europejskiego Stowarzyszenia Wolnego Handlu (EFTA), z wyjątkiem Szwajcarii. EOG opiera się na czterech fundamentalnych wolnościach: swobodzie przepływu ludzi, kapitału, towarów i usług.
Porozumienie o utworzeniu EOG podpisano w Porto 2 maja 1992 roku. Wejście w życie układu opóźniało się z powodu odrzucenia go w referendum w Szwajcarii w grudniu 1992 roku. Po wykluczeniu odniesień do Szwajcarii z umowy, ostatecznie weszła ona w życie 1 stycznia 1994 roku.
Na mocy umowy z Porto obywatele wszystkich państw należących do EOG mogą się swobodnie przemieszczać, osiedlać i nabywać nieruchomości na ich terenie. W zamian EFTA łoży na unijny Fundusz Spójności. Kraje EFTA przyjęły do swojego ustawodawstwa dużą część szczegółowych przepisów wspólnotowych (nie dotyczy to jednak m.in. polityki rolnej czy walutowej). Ważną cechą EOG jest to, iż realizuje swoją działalność na poziomie pierwszego i trzeciego etapu integracji, strefy wolnego handlu i Wspólnego Rynku, wyłączając drugi etap, unię celną.
Polska podpisała umowę o członkostwie w EOG 14 października 2003 w Luksemburgu. Została ona ratyfikowana 8 października 2004 na podstawie ustawy z 16 kwietnia 2004 r. Polska weszła do EOG w momencie jej wejścia w życie 6 grudnia 2005.

</doc>
<doc id="1490" url="https://pl.wikipedia.org/wiki?curid=1490" title="Epistemologia">
Epistemologia

Epistemologia (od , episteme – „wiedza; umiejętność, zrozumienie”; λόγος, logos – „nauka; myśl”), teoria poznania lub gnoseologia – dział filozofii, zajmujący się relacjami między poznawaniem, poznaniem a rzeczywistością. Epistemologia rozważa naturę takich pojęć jak: prawda, przekonanie, sąd, spostrzeganie, wiedza czy uzasadnienie. „Co to jest poznanie i czym jest poznawanie? Wyjaśnianie praw dotyczących zarówno poznawania i tego, co jest poznane”.
Etymologia.
Problematyką poznania filozofowie zajmowali się od czasów starożytnych. Dopiero jednak w epoce nowożytnej pojawiła się świadomość, że jest to odrębna dyscyplina filozoficzna. Kantyści nazywali ją "krytyką poznania". W "Metafizyce" Alexandra Baumgartena użył on terminu „gnoseologia” (1799). W 1827 roku Thomas Krug w słowniku filozoficznym użył słowa "Erkenntnislehre". Określenie "Erkenntnistheorie" (teoria poznania) pojawiło się w wydanej w 1862 roku książce Eduarda Zellera "Über Bedeutung und Aufgabe der Erkenntniss-Theorie. Ein akademischer Vortrag" i zyskało znaczną popularność w obszarze języka niemieckiego.
Termin "epistemology" pojawił się po raz pierwszy w książce Jamesa Perriera "Theory of knowing" (1856) i został utworzony od starogreckiego pojęcia "episteme" (), oznaczającego wiedzę pewną, naukową, dotyczącą istoty, i przeciwstawianą opiniom, wiedzy praktycznej czy doświadczeniu.
Główne zagadnienia.
Teoria poznania wiąże się z innymi dyscyplinami filozoficznymi lub im bliskimi, np. ontologią, logiką i psychologią. Ontologia docieka o tym co i jak istnieje. Czy analiza istnienia ma poprzedzać analizę poznania, tj. czy bez istnienia nie byłoby poznania? Logika jest systemem tautologii jako zdań prawdziwych we wszystkich modelach a epistemologia na pewno z tautologii się nie składa. Przedmiot badań psychologii jest częściowo wspólny, jednak każda z tych nauk bada go z innego punktu widzenia. Psychologia zajmuje się przebiegiem procesów poznawczych a epistemologia interesuje się wedle czego poznanie ma być oceniane jako prawda i fałsz...
Teoria wiedzy.
Co to jest wiedza (poznanie), to problem definicji wiedzy, a co jest wiedzą, to problem kryterium wiedzy. Jak dotrzeć do wiedzy, czyli jaka jest właściwa droga poznawania, to problem sposobu poznania. Rozdzielając dwa główne nurty w tej kwestii rozróżnia się: empiryzm metodologiczny (aposterioryzm) mówiący, że źródłem poznania jest doświadczenie i racjonalizm metodologiczny (aprioryzm), który twierdzi, że jest nim rozum. Spór toczy się też o to, czy istnieje wiedza wrodzona (natywizm głosi iż wiedza jest wrodzona), czy nabywamy ją wraz z doświadczeniem, czyli o pochodzenie poznania. I tu mamy też dwa główne stanowiska: empiryzm genetyczny i racjonalizm genetyczny. Naturą poznania naukowego zajmuje się filozofia nauki. Czy wiedza jest w ogóle możliwa i jak bronić się przed zarzutami sceptycznymi to problem sceptycyzmu.
Teoria prawdy.
Poszukiwanie definicji prawdy jest problemem podstawowym dla epistemologii. Tak jak w przypadku teorii wiedzy, teoria prawdy poszukuje jej definicji i kryterium bycia prawdą. Czy prawda istnieje i czy jest poznawalna, to zagadnienia blisko spokrewnione z problemem sceptycyzmu. Wiedza wspiera się na pojęciu prawdy, stąd pojęcie to jest podstawowe dla całej filozofii.
Problem psychofizyczny.
Jaka jest relacja między materią a duchem, rzeczą myślącą a rozciągłą czy używając współczesnej terminologii, umysłem a ciałem czy zjawiskami fizycznymi a psychicznymi to tak zwany problem psychofizyczny. Wyróżnia się tu przede wszystkim monizm (istnieje jedna substancja) i dualizm (istnieją odrębne substancje: ciało i duch).
Problem percepcji.
Problemy pojęciowej analizy percepcji, jej przedmiotu, wiarygodności oraz relacji percepcji i wiedzy to podstawowe problemy filozofii percepcji. Podstawowymi współczesnymi poglądami w filozofii percepcji są: realizm bezpośredni (spostrzeganie jest bezpośrednie, wymaga oddziaływania przyczynowego ze strony przedmiotu, przedmiotami percepcji są przedmioty w przestrzeni fizycznej, percepcja jest zasadniczo wiarygodna), realizm pośredni (przedmiotem percepcji są reprezentacje rzeczy istniejących w przestrzeni fizycznej) oraz fenomenalizm (mamy dostęp jedynie do doznań i nie istnieje możliwość dotarcia do przedmiotów świata zewnętrznego).
Rys historyczny.
Epistemologia jako odrębny dział filozofii została zapoczątkowana przez Kartezjusza, ale zagadnienia epistemologiczne były obecne właściwie już od początku refleksji filozoficznej.
Starożytność.
W filozofii starożytnej problematyka epistemologiczna nie była wyróżniona, lecz rozważano ją przy okazji problematyki logicznej czy metafizycznej.
W użyciu funkcjonowało kilka pojęć, których znaczenie i wzajemne relacje były różnie określane przez poszczególnych filozofów:
Filozofowie przyrody i sofiści.
Już w pierwszym okresie filozofii starożytnej filozofowie przyrody i sofiści głosili pewne tezy na temat poznania, z których wiele antycypowało podstawowe zagadnienia epistemologii. Solon zalecał, żeby na podstawie rzeczy jasnych wnioskować o niejasnych, a Chilon był autorem zwrotu „poznaj samego siebie”. Tezę Heraklita (zm. ok. 480 p.n.e.), że wszystko płynie można rozumieć jako zwrócenie uwagi na problem identyczności przedmiotów w czasie. Dostrzegł, że zmienność podlega konieczności, który określał mianem logosu. Krytykował również bezrefleksyjne opieranie się na poznaniu zmysłowym, o ile nie jest poparte rozumem. Człowiek posiada możliwość poznania samego siebie poprzez myślenie, które umożliwia właśnie rozum. Pitagorejczycy znali dedukcję (dowód istnienia liczb niewymiernych) i przekonani byli o tym, że świat można opisać za pomocą matematyki (Filolaos był zwolennikiem koncepcji poznania jako uchwycenia stosunków ilościowych). Stosując metodę dedukcyjną stali się tym samym, prekursorami racjonalizmu i aproioryzmu w filozofii, obecnego już w poglądach szkoły eleackiej. Parmenides, który ją zapoczątkował, stawiał równość między bytem a poznaniem i twierdził, że wadliwość mniemań jest nieusuwalna. "Episteme", w przeciwieństwie do "doksy", odnosi się do bytu i jest bezbłędna. Melissos krytykował poznanie zmysłowe, a Anaksagoras jako pierwszy pojmował rozum jako autonomiczną zdolność poznawczą. Demokryt był prekursorem Locke'a w twierdzeniu, że jakości są subiektywne i uważał, że rzeczy podobne poznajemy przez podobne (atomy emitują swoje podobieństwa, które docierają do naszych zmysłów). Twierdził, że ilość jest tym aspektem rzeczy, który można poznać w sposób obiektywny. Słynną tezę Protagorasa, że człowiek jest miarą wszechrzeczy można rozumieć jako spostrzeżenie, że każdemu argumentowi da się przedstawić kontrargument, a rzeczywistość widzi się różnie w zależności od punktu obserwacji. Protagoras twierdził też, że nie można poznać bogów. Gorgiasz swojej tezy, że nic nie istnieje, a nawet jeśli istnieje, to nie jest poznawalne, próbował dowodzić nie wprost. Sokrates (zm. 399 p.n.e.) rozumiał cnotę (etykę) jako rodzaj wiedzy i znał metody definiowania i analizy pojęć.
Platon.
Platon (ur. 427, zm. 347 p.n.e.) był twórcą klasycznej teorii wiedzy jako prawdziwego, dobrze uzasadnionego sądu ("Teajtet"). Jego koncepcja poznania była jednak ściśle związana z jego metafizyką. Platon rozróżnił:
Pochodzenie wiedzy tłumaczył koncepcją preegzystencji dusz i metempsychozą, a więc był twórcą natywistycznej koncepcji wiedzy wrodzonej, która jest nabywana poprzez przypominanie – anamnezę. Platon nie utożsamiał wiedzy z prawdziwym sądem, zwrócił bowiem uwagę na fakt, iż można głosić coś prawdziwie wcale o tym nie wiedząc.
Swoją teorię poznania zobrazował w metaforze jaskini. Kolejne stopnie poznania rzeczywistości obrazowane są jako kolejne etapy wyzwalania się duszy. Cienie postrzegane na ścianie jaskini przez uwięzione w niej dusze są pozorem rzeczywistości, a postrzeganie ich związane jest z wyobraźnią ("εἰϰασία"). Dusza, która odwraca się od pozorów, by postrzec przedmioty, które rzucają cienie, opiera się na wierze ("πίστιϛ"). Przejście na wyższy poziom poznania, od oglądu przedmiotów zmysłowych ku przedmiotom prawdziwym dokonuje się za pomocą odbić obrazów: nauk matematycznych (poznanie pośredniczące, "διάνοια"). Poznanie rzeczywistości poza jaskinią następuje dzięki noezie (, "noesis"), intuicyjnemu wglądowi w istotę rzeczywistości.
Arystoteles.
Jego uczeń, Arystoteles (ur. 384, zm. 322 p.n.e.) był autorem koncepcji wiedzy jako systemu dedukcyjnego, w którym wychodzimy od przesłanek (aksjomatów) i wyprowadzamy kolejne twierdzenia za pomocą dedukcji. Wiedza ("episteme") według jego modelu miała być ogólna i pewna.Stagiryta łączył z przekonaniem, że poznanie pochodzi z doświadczenia, choć rozum jest czymś w rodzaju jego moderatora, łączącego w sobie czynności zmysłów i myślenie. Przedmiotem poznania dla Arystotelesa jest substancja, połączenie materii i formy. Wprowadza również pojęcie przyczyny teleologicznej – celowej, jako racji powstawania rzeczy.
Jako pierwszy (przed Johnem Lockiem) użył pojęcia "tabula rasa" ("de Anima" 430A) na określenie umysłu, który wszelką wiedzę może zdobyć jedynie poprzez doświadczenie. Stanął w ten sposób w opozycji do platońskiej koncepcji wiedzy wrodzonej (natywizm). Podzielił nauki na teoretyczne ("theoria"), praktyczne ("phronesis") i poetyczne ("poetike"). Stworzył podstawowe narzędzia logiczne dla nauki, teorię retoryki i ugruntował pojęcia takie jak spostrzeżenie, pamięć, doświadczenie i poznanie naukowe.
Sceptycyzm starożytny.
Choć filozofia sceptyków powstała na gruncie etyki, jako że zalecali oni powstrzymanie się od wszelkich sądów ze względu na dążenie do ideału moralno-życiowego, to ich osiągnięcia są doniosłe z punktu widzenia epistemologii. Skupili się na argumentowaniu za tym, że nic nie można poznać z pewnością, czyli że ideał "episteme" jest nierealizowalny. Pirron (zm. ok. 286 p.n.e.) twierdził, że dla każdych dwóch zdań A i nie-A trzeba przyjąć ich równe prawdopodobieństwo i mówić tylko „nie wiem, że A i nie wiem, że nie-A”. Ainezydem stworzył listę argumentów przeciw poznaniu bezpośredniemu:
Niepewność była dla sceptyków nieusuwalnym atrybutem wszystkich sądów, choć nie podważali racji mówienia o tym, co się wydaje (dostępu do doświadczenia wewnętrznego). Sekstus Empiryk rozróżnił: dogmatyzm, którego zwolennicy głoszą, że prawda jest poznawalna i że oni są jej właścicielami, akademizm mówiący o prawdzie, że jest niepoznawalna, bo jej nie ma i wreszcie sceptycyzm. Ideałem dla starożytnych sceptyków był mędrzec, który uznaje, że nie wie, czy jest prawda, czy jej nie ma, ale nie przeszkadza mu to w jej poszukiwaniu.
Filozofia średniowieczna.
Augustyn z Hippony (ur. 354, zm. 430) twierdził, że człowiek może poznać Boga, ale potrzebny jest do tego akt oświecenia ("illuminatio") i uważał, że choć Objawienie ma status wiedzy pewnej, to warto jest dążyć do zrozumienia prawd wiary. W średniowieczu kluczowym problemem stał się stosunek wiary do rozumu.
Wiara a rozum.
Greckie "pistis" (zaufanie), Rzymianie tłumaczyli jako "fides" a "doksa" jako "opinio". Nie było słowa na oznaczenie wiary. Łacińskim "fides" zaczęli nazywać ją dopiero pierwsi chrześcijanie. Orygenes wierzył w zgodność Objawienia z filozofią grecką. Tertulian uważał, że rozumowe poznanie jest bezwartościowe, bo prawdy wiary są pewne, lecz dla rozumu niepojęte. Był autorem formuły "credo quia absurdum" („wierzę, choćby to było absurdalne”). Pseudo-Dionizy twierdził, że możliwości poznawcze człowieka są ograniczone a o Bogu możemy tylko mówić, czym nie jest. Anzelm z Canterbury, autor formuły "fides quarens intellectum" („wiara domaga się rozumu”), wierzył w spójność prawd wiary i rozumu. Jednak Piotr Abelard, twórca tekstu zawierającego zdania sprzeczne w Biblii, "Sic et non", kwestionował jego optymizm. Awerroes głosił wyższość filozofii nad teologią. Siger z Brabancji zasłynął teorią dwóch prawd: rozumowej i religijnej. Tomasz z Akwinu (ur. 1225, zm. 1274) zajmując kompromisowe stanowisko, wyróżniał prawdy wiary niedostępne dla rozumu, prawdy rozumu obojętne dla wiary i prawdy wiary udowadnialne rozumowo. Jego formuła "nihil est in intellectu quod non prius fuerit in sensu" (zapożyczona od Arystotelesa) to zasada empiryzmu genetycznego.
William Ockham redukował poznanie do wiedzy o indywiduach, więc nie dopuszczał do niej konieczności ani pewności i stworzył metodologiczną zasadę niemnożenia bytów ponad potrzebę (brzytwa Ockhama).
Buddyzm.
Głównymi twórcami buddyjskiej epistemologii (język angielski: valid cognition; transliteracja Wyliego: "tshad ma") byli komentatorzy dzieł doktryn jogaczary i madhjamaki: Dignaga (ok. 480-540) i Dharmakirti (ok. 530-600) z Indii. Dignaga napisał m.in. "Kompendium Prawomocnego Poznawania" (Sanskryt: "Pramāṇasamuccaya"; transliteracja Wyliego: "tshad ma kun las btus pa") a najważniejszym dziełem Dharmakirtiego jest Pramanavartika (s. "Pramāṇavārtika", tyb. "tshad ma rnam 'grel") stanowiąca komentarz do tekstu Dignagi i będąca jednym z siedmiu traktatów Dharmakirtiego – "Siedem Traktatów Prawomocnego Poznawania" (transliteracja Wyliego: "tshad ma sde bdun"). Dzieła te były kontynuacją wielowiekowego rozwoju myśli buddyjskiej począwszy od starożytnych wczesnych szkół buddyjskich. Dla Dignagi istnieją dwa ważne sposoby poznania ("pramāna"), postrzeżenie ("pratyakṣa") i wnioskowanie ("anumāna"). Pomimo to, nie odrzuca innych źródeł poznania, takich jak porównanie czy świadectwo autorytetu, uznając je jednak za formy postrzeżenia i wnioskowania. W ten sposób, jego myśl niejako zawiera w sobie wcześniejsze ustalenia buddyjskiej epistemologii.
Filozofia nowożytna.
Francis Bacon wystąpił przeciwko logicznym naukom Arystotelesa i stworzył metodę naukową, Galileusz rozwinął ją jako metodę hipotetyczno-dedukcyjną.
Kartezjusz.
Kartezjusz (ur. 1596, zm. 1650) dokonał epistemologicznej reorientacji filozofii. Stworzył geometrię analityczną, co skłoniło go do stwierdzenia, że filozofia powinna stosować podobną metodę. Wiedza powinna więc być systemem prawd pewnych, koniecznych, powiązanych logicznie i niezależnych od doświadczenia. Stwierdzenie "cogito ergo sum" („myślę, więc jestem”) oznacza spostrzeżenie, że nie może istnieć akt myślenia bez podmiotu myślenia. Uznał to za przezwyciężenie sceptycyzmu, który wątpi we wszystko i stworzenie aksjomatycznej podstawy dla absolutnego systemu. Wychodząc od tej podstawy, w umyśle odnalazł Kartezjusz idee wrodzone Boga, duszy ("res cogitans") i ciała ("res extentia") i rozłożył akcenty tak, że położył nacisk na problem psychofizyczny.
W "Rozprawie o metodzie" zawarł podstawowe zasady metodologiczne:
Kartezjusza krytykował Thomas Hobbes, który dowodził, że z "cogito" nie wynika w żaden sposób twierdzenie o niezależności umysłu.
Locke.
John Locke (ur. 1632, zm. 1703) przeciwstawił Kartezjuszowi i jego koncepcji wiedzy wrodzonej wizję umysłu jako niezapisanej tablicy ("tabula rasa"), która zapisuje się dopiero w wyniku doświadczenia. Tak jak Kartezjusz nie kwestionował doświadczenia wewnętrznego i twierdził, że w bezpośrednim doświadczeniu mamy dostęp do wiedzy pewnej. To on właśnie rozróżnił doświadczenie na zewnętrzne (wrażenia zmysłowe) i wewnętrzne (refleksja). Intuicja działająca w obrębie pamięci daje według niego pewność poznawczą i jest podstawą logiki oraz matematyki. Idee dostarczane przez nią są proste. Idee złożone powstają w wyniku abstrakcji z prostych. Niepewność tkwi w poznaniu zewnętrznym.
Głosił przyczynową teorię percepcji mówiąc, że idee mają przyczyny w poznaniu zewnętrznym. Argumentował za tą teorią, że zmysły nie wytworzyłyby wrażeń bez przyczyny i dowodził jej słuszności z różnicy między ideami powstającymi w umyśle i ideami-wrażeniami, na które nie mamy wpływu.
Zastanawiał się też nad tym, czy wszystkie nasze idee mają odpowiedniki w rzeczywistości. To doprowadziło go do rozróżnienia między jakościami pierwotnymi (kształt, objętość, ruch), które są obiektywne i odpowiadają własnościom ilościowym a wtórnymi (na przykład barwy, zapachy, dźwięki), które są subiektywne i są tożsame z własnościami jakościowymi.
Berkeley.
Naiwny realizm twierdzi, że wszystkie jakości są pierwotne. George Berkeley stwierdził, że wszystkie są wtórne. Odrzucił pojęcie substancji jako zbędną metafizykę, ograniczył poznanie wyłącznie do poznania zmysłowego (nawet matematyka była nauką o zmysłowo ujmowalnych przedmiotach) i podważył istnienie rzeczywistości fizycznej tworząc formułę "esse est percipi" („być znaczy być postrzeganym”). Na zarzuty formułowane przeciwko jego teorii, iż rzeczy istnieją, pomimo tego że nie są postrzegane, odpowiadał, że dzieje się tak, ponieważ są one postrzegane przez Boga.
Hume i nowożytny sceptycyzm.
Uważany za najwybitniejszego przedstawiciela nowożytnego sceptycyzmu, David Hume (ur. 1711, zm. 1776) uznawał, że problem obiektywności poznania jest bezprzedmiotowy i stwierdził, że dowiedzenie istnienia świata zewnętrznego jest niemożliwe. Jednak dodawał, że wiara w istnienie rzeczywistości jest przydatna ze względu na jej walory praktyczne.
Rozróżnił impresje (lockowskie wrażenia), których doznajemy, gdy coś widzimy lub słyszymy i idee, które tworzymy, gdy coś wyobrażamy sobie lub myślimy. Proces poznania przebiega według niego od impresji do złożonych idei, które powstają za sprawą operacji umysłowych, polegających na kojarzeniu idei przez relację podobieństwa, styczność w czasie i przestrzeni oraz ideę przyczynowości.
Wyróżnił fakty i relacje między ideami, którymi zajmuje się matematyka – wiedza pewna, lecz nie mówiąca nic o faktach. Wiedza o faktach natomiast byłaby pewna, gdyby opierała się wyłącznie na impresjach, ale opiera się też na ideach leżących u podstaw doświadczenia: idei substancji i przyczynowości.
Te dwa pojęcia Hume krytykował. Tak naprawdę doświadczamy tylko ciągów zdarzeń, mówił, a wiązanie ich w związki przyczynowe, to już działanie umysłu wynikające z wyrobionego nawyku. Tymczasem związek przyczynowy nie jest konieczny, bo przyszłość może przynieść inne dane.
Leibniz.
Gottfried Wilhelm Leibniz przeformułował Tomaszową zasadę empiryzmu genetycznego dodając do niej "nisi intellectus ipsae " (z wyjątkiem samego umysłu). Planował stworzenie specjalnego języka ("lingua universalis"), w którym można by jednoznacznie przedstawić a następnie rozwiązać każdy problem.
Wydzielił dwa rodzaje prawd odpowiadających dwóm zasadom myślenia: prawdy dotyczące faktów (zasada racji dostatecznej) i prawdy rozumowe (zasada sprzeczności). Każda prawda jest a priori (predykat zawarty w podmiocie) i konieczna (prawdziwa we wszystkich możliwych światach), ale tylko z punktu widzenia Boga. My musimy uznać prawdy dotyczące faktów za przypadkowe, a pełne ich dowody są nieskończone (właściwie to samo mówił Hume w krytyce przyczynowości).
Thomas Reid bronił zaufania do własnego zdrowego rozsądku, czym stał się prekursorem Moore'a.
Kant.
Immanuel Kant (ur. 1724, zm. 1804) dokonał rozróżnienia na sądy analityczne, czyli takie, które wynikają z definicji (por. klasyczny rachunek zdań), zaś wiedza, którą zdobywamy na ich mocy nie może wykraczać poza granicę już znanych pojęć i syntetyczne (por. indukcja niezupełna), które pozwalają na zdobywanie wiedzy istotnie nowej. Z drugiej strony podzielił sądy na aprioryczne (poza doświadczeniem, wynikające z wrodzonych własności umysłu i zmysłów) i aposterioryczne (na podstawie zdobywanego doświadczenia).
Stosując te kategorie do wcześniejszych dziejów filozofii można powiedzieć, że platońska "episteme" miała zawierać tylko sądy "a priori". Leibniza prawdy rozumu są "a priori", prawdy faktów syntetyczne "a posteriori", choć dla Boga wszystkie są analityczne "a priori". Hume akceptował tylko istnienie analitycznych "a priori" (relacje między ideami) i syntetycznych "a posteriori" (fakty).
Natomiast Kant argumentował za możliwością istnienia sądów syntetycznych "a priori", które jego zdaniem są obecne w matematyce i przyrodoznawstwie i byłyby składnikami przyszłej metafizyki. Argumentował za pomocą dedukcji transcendentalnej, czyli poszukiwania argumentów na konieczność warunków możliwości tego, co jawi się nam jako wymagające uzasadnienia.
Początkiem poznania jest doświadczenie, którego składnikami apriorycznymi są czas i przestrzeń, jakie określa transcendentalnymi warunkami poznania, oraz kategorie intelektu, z których za najważniejsze uznawał kategorię przyczynowości i substancji. Doświadczamy więc z pomocą wrodzonych narzędzi.
Możemy, zdaniem Kanta, rozróżnić zjawiska naszej świadomości (fenomeny) i rzeczy same w sobie (noumeny). Te ostatnie nie są poznawalne, ale musimy przyjąć ich istnienie, by wytłumaczyć istnienie zjawisk.
Kant rozróżnił też pytania "de iure" (dotyczące kwestii dopominających się wyjaśnienia i wymagających transcendentalnej dedukcji) i "quid facti" dotyczące tego, co jest. Teoria poznania zajmuje się według niego pytaniami "de iure".
Wiek XIX.
Hegel odrzucał epistemologię (teorię poznania), gdyż nie miał zaufania do bezkrytycznie stosowanego pojęcia poznania w epistemologii. Poznanie w ujęciu epistemologicznym nie jest absolutne a poza Absolutem nie można znaleźć ani prawdy, ani wiedzy i dlatego należy odrzucić epistemologię. Dlatego w przypadku filozofii Hegla nie można mówić o epistemologii, ponieważ utożsamiał wiedzę z bytem – racjonalnym, logicznym i koniecznym.
Bradley, heglista, głosił holistyczną koncepcję, wedle której wszelkie wyodrębnianie faktów z całości wiedzy prowadzi do uproszczeń.
Schleiermacher stworzył hermeneutykę, czyli teorię rozumienia tekstu. Stwierdził, że w przypadku każdego tekstu mamy do czynienia z sytuacją koła hermeneutycznego: żeby zrozumieć tekst, trzeba zrozumieć inny, być może całą kulturę i nie ma wyjścia poza ten krąg.
Neokantyzm.
Szkoła marburska.
Szkoła marburska rozwija epistemologię jako krytykę poznania, której założenia można ująć następująco:
Szkoła badeńska.
Szkoła badeńska rozwija epistemologię opartą na filozofii wartości. Heinrich Rickert uznaje, że przedmiotem poznania są wartości poznawcze.
Wilhelm Windelband rozróżnił nauki idiograficzne skupione na opisie (np. historia) i nomotetyczne formułujące prawa przyrody (np. fizyka).
Nowa Szkoła Friesa.
Nowa Szkoła Friesa rozwija krytyczną filozofię Immanuela Kanta w duchu Jakoba Friedricha Friesa. Jej reprezentant Leonard Nelson w rozprawie "O niemożliwości teorii poznania" (1911) dowodzi, że naukowa teoria poznania nie jest możliwa, ponieważ nie da się uzasadnić naukowo obiektywnej ważności poznania bez założenia tego rodzaju ważności.
Pozytywizm.
August Comte twierdził, że wiedza winna służyć celom praktycznym, by można było realizować cel ulepszania życia społecznego. Rozróżnił nauki na abstrakcyjne (matematyka, astronomia, fizyka, chemia, biologia i socjologia, przy czym każda z nich jest coraz mniej ogólna i podporządkowana poprzedniej), a filozofię sprowadzał do teorii wiedzy. John Stuart Mill twierdził, że twierdzenia matematyki są uogólnieniami indukcyjnymi i opracował kanony indukcji eliminacyjnej chcąc odeprzeć zarzuty Hume'a.
Empiriokrytycyzm (Ernst Mach, Richard Avenarius) stworzył zasadę ekonomii myśli mówiącą, że aparat pojęciowy musi być jak najprostszy. William James opracował zasadę pragmatyczną, która mówi, że należy rozpatrywać praktyczne konsekwencje działań oparte na określonych przekonaniach.
W nurtach okresu pozytywistycznego leżących na peryferiach tej myśli należy wymienić Fryderyka Nietzsche, który był relatywistą i uważał poznanie za akt biologiczny, warunkowany celami praktycznymi i twierdził, że rzeczywistość ujmujemy zawsze fałszywie z uwagi na różne ograniczenia. Argumentował też za tym, że każde uogólnienie jest nieadekwatne, a poznając rzeczywistość odwołujemy się do własnych konstrukcji poznawczych. Hans Vaihinger twierdził, że nasze poznanie operuje użytecznymi fikcjami. Henri Bergson był zwolennikiem intuicji, która w jego koncepcji dostarcza poznania bezpośredniego i jest jak uchwycenie budowli jednym rzutem oka. Theodor Ziehen, przedstawiciel psychologizmu chciał redukować epistemologię do psychologii.
Wiek XX.
Antypsychologiści.
Edmund Husserl (ur. 1859, zm. 1938) uważał filozofię za pierwszą i fundamentalną naukę w gmachu wiedzy, która w odróżnieniu od nauk szczegółowych ma dostarczyć "episteme". Zjawiska należy według niego opisywać takimi, jakimi są, w myśl hasła powrotu do rzeczy samych. Fenomenologia (epistemologia) jako podstawa ma być bezzałożeniowa. Jego zasada wszystkich zasad zalecała opisywanie wszystkiego tak, jak jawi się w świadomości. Wymaga to, zdaniem Husserla, redukcji ejdetycznej, czyli oczyszczenia świadomości z balansu poznawczego, wzięcia w nawias wiedzy naukowej i prowadzi do uchwycenia istoty fenomenów. Uczeń Husserla, Roman Ingarden, stworzył projekt czystej epistemologii, niezależnej od nauki i innych dyscyplin filozoficznych.
Bernard Bolzano wyróżnił kategorię sądów i przedstawień samych w sobie (niezależne od umysłu i konstytuujące realność badaną przez logikę) oraz sądów i przedstawień w sensie subiektywnym. Zdefiniował pojęcie analityczności, prawdy logicznej, wynikania logicznego i prawdopodobieństwa.
Gottlob Frege sformułował sławne rozróżnienie sensu i znaczenia. Zdefiniował zdania aprioryczne jako te, które są sprowadzalne do logiki.
Konwencjonalizm.
Konwencjonalistów łączył pogląd, że uznanie stwierdzenia zależy od przyjęcia umów terminologicznych (konwencji). Antycypacji tego poglądu można dopatrywać się u sofistów. Do konwencjonalistów należeli: Henri Poincaré, Pierre Duhem i Kazimierz Ajdukiewicz. Konwencje nie oznaczają tu jednak całkowitej arbitralności. Chodziło im o to, że nie są jednoznacznie wyznaczane przez fakty. Stanisław Leśniewski krytykował konwencjonalizm mówiąc, że możemy umówić się do wszystkiego, ale to nie wpłynie na to, jakim jest świat.
Filozofia analityczna.
Brytyjczycy.
Bertrand Russell stworzył projekt naukowej epistemologii: kombinacji logiki i metody przyrodniczej. Sądził, że struktura logiki jest izomorficzna ze strukturą świata. Stałym logicznym odpowiadają indywidua, zdaniom atomicznym – fakty, itd. Indywidua, proste istności stanowiące fundament dla własności są według niego poznawalne bezpośrednio a nie przez opis.
George Edward Moore bronił zdrowego rozsądku jako źródła trafnych przekonań. Wprowadził termin „dana zmysłowa” i badał relacje między danymi a własnościami rzeczy. W filozofii percepcji wyróżnił:
Moore sformułował też paradoks analizy. Mówi on, że jeśli analizowane pojęcie jest tożsame z objaśniającym je kontekstem, to analiza jest trywialna. Jeśli nie, to jest niepoprawna, gdyż zmienia sens analizowanego pojęcia.
Ludwig Wittgenstein w słynnej tezie stwierdził, że granice języka są granicami naszego świata, a więc poznanie jest ściśle związane z językiem. Choć z początku postulował konieczność języka doskonałego w rodzaju leibnizowskiej "lingua universalis", później przekonał się do pracy nad językiem potocznym. Filozofia miałaby ograniczyć się do istotnych dla niej kontekstów takich jak „wiedzieć”, „wierzyć”, „widzieć”. Kontynuował tę myśl John Austin, który sformułował fenomenologię lingwistyczną opartą na zaufaniu właśnie do języka potocznego.
Gilbert Ryle zdefiniował pojęcia „wiedza jak” – praktyczna i „wiedza, że” – teoretyczna.
Neopozytywizm.
Filozofowie z kręgu Koła Wiedeńskiego (Moritz Schlick, Otto Neurath czy Rudolf Carnap) chcieli stworzyć "Encyklopedię jedności nauki", gdzie całość wiedzy miała być ujęta w jednolitym języku fizykalistycznym. Filozofię sprowadzali do logiki.
Bezsensowność metafizyki próbowali wykazać w drodze analizy logicznej. Za kryterium sensowności zdania przyjęli jego weryfikowalność i uznali równozakresowość nazw „sprawdzalny”, „sensowny”, „naukowy” z jednej strony i „niesprawdzalny”, „bezsensowny” i „metafizyczny” z drugiej. Jak zauważył Roman Ingarden sama zasada sensowności jest nieweryfikowalna, czyli bezsensowna według ich własnego kryterium.
Popper.
Krytykował zasadę sensowności a zamiast weryfikacji zaproponował falsyfikowalność jako kryterium wiedzy naukowej. Uważał, że celem nauki jest zbliżanie się do prawdy poprzez zwiększanie prawdopodobieństwa i wyznawał ewolucjonizm traktując wiedzę jako narzędzie w procesie ewolucji.
Lingwistyka kartezjańska.
Noam Chomsky stworzył gramatykę generatywną, system reguł wywodzących strukturę zdania z prostych elementów o charakterze syntaktycznym. W jego koncepcji uczenia się języka przez dzieci odrodziła się koncepcja wiedzy wrodzonej.

</doc>
<doc id="1491" url="https://pl.wikipedia.org/wiki?curid=1491" title="Efekt Elizy">
Efekt Elizy

Efekt Elizy – zjawisko przypisywania przez ludzi znaczenia i sensu znakom, słowom i zdaniom, które takiego sensu same z siebie nie mają. Przykładem takiego efektu jest np. interpretowanie przypadkowych wzorów pojawiających się w fusach po kawie na dnie szklanki, czy tworzonych przez chmury na niebie jako obrazy, które przedstawiają jakieś konkretne kształty. 
Nazwa tego efektu pochodzi od pierwszego, naśladującego zwykłą konwersację programu o nazwie ELIZA. Program ten wybierał pewne kluczowe słowa z wypowiedzi ludzi, a następnie tworzył "odpowiedź" łącząc słowo kluczowe ze zwrotami z wcześniej wprowadzonej bazy danych "otwartych zwrotów", takich jak "co to dla ciebie znaczy", "zawsze ma sens", "Nie znam" itp, co dawało czasami efekt "głębokiego znaczenia" odpowiedzi, a czasami chęć jej kontynuowania. 
Np. program Eliza na pytanie: "Czy jesteś człowiekiem" mógł dać odpowiedź: "Być człowiekiem... czy to coś znaczy..." albo "Co myślisz o mojej matce?" dawał odpowiedź: "Nie znam twojej matki, opowiedz mi o niej coś więcej".
Efekt Elizy powoduje, że ludzi jest stosunkowo łatwo przekonać, że dana maszyna naprawdę myśli i daje sensowne odpowiedzi, nawet jeśli te odpowiedzi są po prostu losowane z wcześniej przygotowanego zbioru, pod warunkiem że dana osoba nie wie, że rozmawia z maszyną. W przypadku gdy dana osoba wie, że może rozmawiać z maszyną, efekt Elizy traci na znaczeniu.

</doc>
<doc id="1492" url="https://pl.wikipedia.org/wiki?curid=1492" title="Eratostenes">
Eratostenes

Eratostenes (gr. "Eratosthenes"; ur. 276 p.n.e. w Cyrenie, zm. 194 p.n.e.) – grecki matematyk, astronom, filozof, geograf i poeta. W 255 p.n.e. przeniósł się do Aleksandrii.
Wyznaczył obwód Ziemi (zob. opis eksperymentu) oraz oszacował odległość od Słońca i Księżyca do Ziemi. Twierdził, że, płynąc na zachód od Gibraltaru, można dotrzeć do Indii. Jako pierwszy zaproponował wprowadzenie roku przestępnego, czyli wydłużonego o jeden dodatkowy dzień w kalendarzu (por. Dekret z Kanopos). Opracował katalog 675 gwiazd. Skonstruował mezolabium.
Najważniejsze dzieła Eratostenesa to:
Był również badaczem twórczości Homera – ustalił datę zdobycia Troi na rok 1184 p.n.e., czyli nieodbiegającą od współczesnych szacunków. Podał sposób znajdowania liczb pierwszych – sito Eratostenesa. W 236 p.n.e. przejął po Apolloniosie z Rodos zarządzanie Biblioteką Aleksandryjską.
W wieku 82 lat, nie mogąc pogodzić się z utratą wzroku, zagłodził się na śmierć.
Prace matematyczne Eratostenesa są znane głównie z pism Pappusa z Aleksandrii, zaś jego opisy geograficzne z ksiąg Strabona.

</doc>
<doc id="1494" url="https://pl.wikipedia.org/wiki?curid=1494" title="Errno">
Errno

errno – uniksowy mechanizm zgłaszania błędów przez funkcje libc, a w szczególności jądra. Jeśli funkcja zakończy się błędem, sygnalizuje to zwracając zwykle codice_1 lub, w przypadku funkcji zwracających wskaźnik, codice_2. Program powinien wtedy zajrzeć do zmiennej globalnej codice_3, żeby dowiedzieć się, jaki dokładnie błąd wystąpił. Jeśli funkcja zakończy się pomyślnie, zawartość codice_3 nie jest zdefiniowana – w szczególności może tam znajdować się kod błędu niezwiązany z ostatnim wywołaniem danej funkcji. Ma to miejsce np. w przypadku, gdy funkcja codice_5 wywoła funkcję codice_6, przy czym ta druga zwróci błąd, co jednak nie przeszkodzi pierwszej w pomyślnym wykonaniu. Może się też zdarzyć, że pomyślne wywołanie funkcji potencjalnie modyfikującej codice_3 nie zmieni poprzedniej wartości tej zmiennej.
Zmienna codice_3 jest zdefiniowana w nagłówku codice_9.
Popularne kody błędów to:
Opis błędu w odpowiednim dla danego locale języku można otrzymać za pomocą funkcji:
char *strerror(int errnum);
Niestety nie jest ona bezpieczna w programach wielowątkowych, gdzie należy korzystać z trudniejszej w użyciu:
int strerror_r(int errnum, char *buf, size_t n);
Przykład.
int main(int argc, char **argv)
 int files = argc - 1;
 FILE *file;
 char *fname;
 int i;
 if (!files) {
 fprintf(stderr, "Usage: %s &lt;files&gt;\n", argv[0]);
 return 1;
 for (i = 1; i &lt;= files; ++i) {
 fname = argv[i];
 file = fopen(fname, "r");
 if (file == NULL) {
 fprintf(stderr, "Error while trying to open '%s': %s\n", fname, strerror(errno));
 } else {
 fprintf (stderr, "'%s' opened successfully\n", fname);
 fclose (file);
 return 0;

</doc>
<doc id="1496" url="https://pl.wikipedia.org/wiki?curid=1496" title="Empatia">
Empatia

Empatia (gr. "empátheia" „cierpienie”) – zdolność odczuwania stanów psychicznych innych osób (empatia emocjonalna), umiejętność przyjęcia ich sposobu myślenia, spojrzenia z ich perspektywy na rzeczywistość (empatia poznawcza).
Sam poznawczy komponent empatii, czyli zdolność do wyobrażenia sobie perspektywy myślowej należącej do innej osoby (innymi słowy, rozeznania sposobów rozumowania innej osoby), określany jest mianem decentracji (czemu może, ale nie musi towarzyszyć komponent emocjonalny empatii, czyli zdolność do jednoczesnego współodczuwania stanów emocjonalnych owej innej osoby).
Osoba nieposiadająca tej umiejętności jest „ślepa” emocjonalnie i nie potrafi ocenić ani dostrzec stanów emocjonalnych innych ludzi. Silna empatia objawia się między innymi uczuciem bólu wtedy, gdy przyglądamy się cierpieniu innej osoby, zdolnością współodczuwania i wczuwania się w perspektywę widzenia świata innych osobników.
Psychologowie zastanawiali się, skąd bierze się uczucie empatii i dlaczego niektórzy ludzie są go pozbawieni. Empatia jest jednym z najsilniejszych hamulców zachowań agresywnych, więc jest to także pytanie istotne z punktu widzenia inżynierii społecznej.
Źródła empatii.
Teoria Jeana Piageta.
Empatia jest pochodną rozwoju poznawczego. Myślenie dzieci w wieku przedszkolnym i niekiedy we wczesnym wieku szkolnym związane jest z tak zwanym egocentryzmem myślenia, nieuświadamianiem sobie, że perspektywa widzenia innych ludzi może być inna od naszej. Dziecko nie potrafi wczuć się w położenie innej osoby, bo wymaga to złożonego procesu poznawczego, dlatego pozbawione jest empatii.
Teoria Margaret Mahler.
Wczesna relacja między matką i dzieckiem polega między innymi na nieświadomym procesie przepływu emocji i istnieniu „emocjonalnej pępowiny” między matką i dzieckiem. W pierwszych miesiącach życia emocje matki przechodzą na dziecko, natomiast nastroje dziecka udzielają się matce. Jeśli w tym okresie życia następują pewne traumy (na przykład matka przeżywa bardzo negatywne uczucia, jest w depresji) lub matka jest nieobecna (śmierć, choroba, zła opieka), to dziecko buduje „mur ochronny”, którego zadaniem jest niewczuwanie się w emocje matki. W wyniku tego zostaje pozbawione zdolności empatycznego wczucia się w położenie innej osoby.
Empatia, obok asertywności jest jedną z dwóch podstawowych umiejętności wchodzących w skład tzw. inteligencji emocjonalnej. Duże zdolności empatyczne posiadają osoby charyzmatyczne.
Empatia to umiejętność zrozumienia innych ludzi oraz zdolność współodczuwania z nimi ich uczuć i emocji. Jest to dar wczuwania się w sytuację innych ludzi i zrozumienia motywów nimi kierujących, jako źródeł ich decyzji i postaw. Empatia jest podstawową cechą umożliwiającą prawidłowy dialog zarówno na płaszczyźnie interpersonalnej, jak i ogólnospołecznej. Brak empatii u spierających się oponentów powoduje polaryzację stanowisk oraz ich zaostrzenie do stopnia uniemożliwiającego rozwiązanie konfliktu w sposób inny niż siłowy. Zdolność odczuwania empatycznego, jako podstawa zrozumienia oponentów jest wstępem do wybaczenia. Osoby empatyczne dzięki znakomitemu wczuciu się w sytuację i psychikę innych stron mają zdolność rozwiązywania konfliktów, w związku z czym często pełnią rolę mediatorów.
Osoby pozbawione zdolności do empatii są bardzo agresywne, o silnej osobowości, narzucające swą wolę i wizję świata, nieznoszące sprzeciwu, nieuznające argumentów innych stron, niedopuszczające do swojej świadomości możliwości własnej pomyłki lub błędu, wysoce konfliktowe, bezkompromisowe.
Badania Barona-Cohena.
Empatia jest zdolnością człowieka do rozpoznawania myśli lub uczuć innej osoby oraz reagowania na jej myśli i uczucia odpowiednią emocją. Zdolność ta jest zależna od działania obwodu empatii znajdującego się w mózgu człowieka. Baron-Cohen określił biorące w obwodzie empatii obszary za pomocą czynnościowego rezonansu magnetycznego (fMRI), według Barona-Cohena jest to około 10 wzajemnie połączonych obszarów mózgu.
Uważa się, że bierze udział w przetwarzaniu informacji społecznych i porównywaniu własnego punktu widzenia z perspektywy widzenia innej osoby.
Grzbietowa część (dMPFC) jest odpowiedzialna za metareprezentację, tzn. aktywizuje się kiedy myślimy o uczuciach i myślach innych ludzi.
Brzuszna część (vMPFC) według Mike’a Lombardo może odgrywać decydującą rolę w procesie samoświadomości, ponieważ aktywizuje się w kiedy skupiamy się na samym sobie. Według innej teorii António Damásio z Uniwersytetu Stanu Iowa vMPFC przypisuje wartość emocjonalną dla określonych działań. Jeśli czynność była nagrodzona otrzymuje wartość dodatnią, jeśli negatywne skutki wartość karzącą i ma znak ujemny. Damasio nazywa te wartości markerami somatycznymi.
Kora oczodołowo-czołowa (OFC)
Badania Barona-Cohena, Howard Ring i Valerie Stone wykazały, że obszar aktywizuje się w okoliczności myślenia o czynnościach oraz jego uszkodzenie może spowodować brak zdolności oceny społecznej czynu (foux pax). Inne badania wykazały wzmożoną aktywność w obszarze OFC w okolicznościach bólu. Doświadczenie dotyczyło patrzenie jak igła wbija się w nieznieczuloną dłoń.
Wieczko czołowe (FO)
Obszar odpowiedzialny za ekspresję języka, uszkodzenie powoduje afazję Broki. W empatii ten obszar jest odpowiedzialny za identyfikowanie intencji i celów innych osobników u małp. Obszar wykazuje wzmożoną aktywność gdy małpa widzi jak inna małpa sięga po przedmiot.
Zakręt czołowy-dolny (IFG)
Badania Bhismadev Chakrabarti wykazały, że obszar IFG jest odpowiedzialny za rozpoznawanie mimiki twarzy, obszar wykazuje tym wyższą aktywność im łatwiej jest określić emocje jakie wyraża mimika twarzy.
Część ogonowa przedniego zakrętu obręczy i przednia część wyspy
Przednia część zakrętu obręczy (aACC) lub też inaczej środkowy zakręt obręczy (MCC) oraz Przednia część wyspy (AI) uczestniczą w procesie empatii jako macierz bólu. Badania Tanii Singer w Zurychu wykazały wzmożoną aktywność w momentach odczuwania bólu i obserwowania bólu, obszary te aktywizują się również w momencie gdy czujemy obrzydliwy smak oraz wyraz takiej emocji. Obszary wykazuje aktywność również gdy oceniamy postępowanie drugiej osoby, u mężczyzn i kobiet obszary były bardziej aktywne gdy osoba obserwowała cierpienie osoby, którą uznała za sympatyczną i mniej aktywne gdy obserwowała cierpienie osoby uznanej za niesympatyczną.
Uszkodzenie tego obszaru jest związane z upośledzeniem zdolności rozpoznawania emocji takich jak radość, wstręt i ból.
Skrzyżowanie skroniowo-ciemieniowe (TPJ)
Badania wykazały aktywność gdy oceniamy intencje i przekonania innej osoby, działanie tego obszaru jest odpowiedzialne za tzw. teorię umysłu. Uszkodzenie tego obszaru może skutkować trudnościami w ocenie cudzych intencji oraz doświadczenie eksterioryzacji. Stymulowanie tego obszaru natomiast może wywołać wrażenie obecności innej osoby, kiedy jest ona sama.
Bruzda skroniowa górna (pSTS)
Jest aktywna kiedy zwierzęta obserwują kierunek patrzenia innego osobnika. Uszkodzenie obszaru może powodować niezdolność określenia na co patrzą inni osobnicy. Patrzenie w oczy ma też ważną funkcję w określeniu emocji innego osobnika.
Kora czuciowo-somatyczna (SMC)
Bierze udział w odbieraniu wrażenia dotyku osobistego i obserwowaniu jak inny osobnik jest dotykany. Może być źródłem emocji jakie powstają kiedy widzimy jak inna osoba doznaje bólu wynikającego z dotyku (np.: wbijanie igły w czyjś palec), na zasadzie utożsamienia się obszar tworzy obraz jakby ból był w ciele osoby obserwującej. Trwałe lub czasowe zaburzenie funkcjonowania obszaru może spowodować poważne problemy w identyfikowaniu emocji innych osób. Badania Yawei Cheng wykazały, że osoby wykonujące na kimś akupunkturę mają mniej aktywny ten obszar.
Płacik ciemieniowy dolny (IPL) i bruzda ciemieniowa dolna
FO, IFG i IPL wchodzą w skład systemu neuronów lustrzanych, w procesie empatii obszary są istotne nie ze względu na powierzchowną mimikrę, ale zaraźliwość emocjonalną. Jedno płaczące dziecko w szpitalu potrafi „zarazić płaczem” inne znajdujące się w sali. W działaniu tego obszaru nie jest wymagane świadome myślenie o emocjach innych osób co w pozostałych obszarach jest konieczne.
Ciało migdałowate (Amyg)
Badania Josepha LeDoux uważa, że jest to centrum mózgu emocjonalnego, w nim następuje uczenie się strachu.
"Empathy Quotient" (EQ).
S. Baron-Cohen i wsp. opracowali metody określania poziomu empatii („iloraz empatii”, "Empathy Quotient", EQ), wykorzystywane w czasie badań diagnostycznych dotyczących takich zaburzeń rozwoju, jak autyzm wysokofunkcjonujący i zespół Aspergera (zob. spektrum autystyczne).

</doc>
<doc id="1497" url="https://pl.wikipedia.org/wiki?curid=1497" title="Eksperyment">
Eksperyment

Eksperyment (łac. "experimentum" – doświadczenie, badanie) – w naukach przyrodniczych i społecznych zbiór działań wzbudzających w obiektach materialnych określone reakcje i zjawiska w warunkach pozwalających kontrolować wszelkie istotne czynniki, które poddaje się dokładnej obserwacji.
Cele i efekty eksperymentów.
Eksperymenty wykonuje się w celu potwierdzenia lub sfalsyfikowania określonej hipotezy. Hipoteza z jednej strony określa ściśle warunki eksperymentu a z drugiej nadaje sens poczynionej w wyniku eksperymentu obserwacji i w ogóle decyduje, co w danym eksperymencie jest właściwą obserwacją, a co tylko nieistotnym jego zakłóceniem. Jak twierdził Max Planck, eksperyment jest swojego rodzaju pytaniem jakie teoria zadaje naturze.
Nauki przyrodnicze rozwijają się głównie poprzez świadome eksperymenty, spora część wiedzy tych nauk pochodzi też z bezpośrednich obserwacji zjawisk zachodzących naturalnie. Dotyczy to zwłaszcza nauk biologicznych i medycznych i w mniejszym stopniu fizyki i chemii.
W informatyce i po części w matematyce stosuje się metody eksperymentalne dopiero wtedy, gdy złożoność problemu uniemożliwia rozwiązanie go metodami dedukcyjnymi.
Techniki przeprowadzania eksperymentów.
W naukach przyrodniczych uważa się, że dobry eksperyment to taki, w którym eksperymentator bądź kontroluje cały jego przebieg, bądź dokładnie zna obszary które nie podlegają jego ścisłej kontroli.
Planowanie eksperymentów jest trudną sztuką. Dobry eksperyment musi być jak najprostszy w wykonaniu i jednocześnie dawać jak najbardziej jednoznaczną odpowiedź potwierdzającą lub falsyfikującą daną teorię. Często w praktyce zdarza się, że pozornie nieudane eksperymenty, które wymknęły się spod kontroli i nie dały jednoznacznej odpowiedzi na założone wcześniej pytanie, stały się początkiem rozwoju nowych dziedzin. W ten sposób odkryto promieniowanie Roentgena (zwane też promieniami X), penicylinę i in.
Większość eksperymentów przeprowadza się w specjalnie do tego przystosowanych pomieszczeniach zwanych laboratoriami, aczkolwiek czasami przeprowadza się też tzw. eksperymenty plenerowe, na przykład, aby zbadać jakieś zjawisko w miejscu jego występowania.
Eksperyment a obserwacja.
Klasyczny eksperyment świadomie ingeruje w naturę i polega na analizie skutków tej ingerencji. Ściśle zdefiniowanemu obiektowi dostarcza się bodźców, a następnie obserwuje się reakcję obiektu. Dokładne obserwacje natury, bez ingerowania w nią nazywa się często eksperymentami naturalnymi.
Analiza obserwacji i eksperymentu jest w gruncie rzeczy bardzo zbliżona. W obu przypadkach chodzi o to aby ustalić związek między bodźcem (przyczyną) i zachowaniem obiektu (skutkiem). Różnica polega tylko na tym, że w eksperymencie naturalnym bodźców dostarcza sama natura. Przykładem eksperymentu klasycznego jest obserwacja efektów zderzenia cząstek akceleratorze, a eksperymentu naturalnego obserwacja zaćmienia Słońca.
Eksperymenty psychologiczne.
Eksperymenty psychologiczne, prowadzone na ludziach, wywołują wiele kontrowersji. Najbardziej znane to:

</doc>
<doc id="1498" url="https://pl.wikipedia.org/wiki?curid=1498" title="Elektrownia">
Elektrownia

Elektrownia – zakład przemysłowy wytwarzający energię elektryczną przez przetwarzanie innych form energii pierwotnej.
Elektrownia jest to obiekt techniczny składający się z jednego lub kilku zespołów urządzeń służących do wytwarzania energii elektrycznej.
Typy elektrowni.
Podział ze względu na wykorzystanie ciepła odpadowego:
Podział ze względu na źródło energii pierwotnej:
Ponadto wyróżnia się typy:

</doc>
<doc id="1499" url="https://pl.wikipedia.org/wiki?curid=1499" title="Eos (mitologia)">
Eos (mitologia)

Eos (także Świt, Jutrzenka; gr. Ēṓs ‘jutrzenka’, łac. Aurora ‘jutrzenka’, ‘zorza’) – w mitologii greckiej bogini i uosobienie zorzy porannej, brzasku i świtu; jedna z tytanid; utożsamiana z Hemerą i rzymską Aurorą.
Należała do drugiego pokolenia tytanów. Była bóstwem związanym z kultem jutrzenki. Według wierzeń starożytnych Greków każdego świtu przemierzała niebo na lekkim rydwanie, zaprzężonym w parę białych lub różowych koni – Lamposa (Świecącego, Jasnego) i Faetona (Promiennego, Błyszczącego). Swą wędrówkę rozpoczynała (wynurzała się z fal Okeanosa) na Wschodzie (kraina północno-wschodnia Grecji) i kończyła (zanurzała się w falach Okeanosa) na Zachodzie, po drugiej stronie widnokręgu. Otwierała „wrota dnia” (bramę nieba) przed wozem Heliosa. Rozpraszała mroki nocy, zwiastowała światło dnia ludziom i bogom. Jej nadejście zapowiadał Fosforos (Gwiazda Poranna) – „jaśniejący syn Jutrzenki”.
Uchodziła za córkę tytana Hyperiona i tytanidy Tei (albo tytana Pallasa) oraz za siostrę Heliosa i Selene, przypuszczalnie także Tytana.
Była bardzo kochliwa. Uprowadziła i uwiodła kilku mężczyzn, m.in.: olbrzyma Oriona, Kefalosa, Klejtosa. Ze swoim pierwszym mężem, tytanem Astrajosem, miała liczne potomstwo, m.in. gwiazdy, Fosforosa (Hesperosa; bóstwo uosabiające planetę Wenus), Boreasza, Zefira, Notosa, Eurosa, przypuszczalnie także Apeliotesa, Kajkiasa, Lipsa, Skirona (bóstwa uosabiające wiatry).
Ze związku z Titonosem (był jej drugim mężem) urodzili się synowie Emation i król Etiopii, Memnon, który zginął pod Troją. Długo opłakiwała swego syna Memnona, zabitego przez Achillesa (łzy jej opadły na ziemię w postaci kropel porannej rosy). Dla Titonosa uprosiła u boga Zeusa dar nieśmiertelności, ale zapomniała uprosić dar wiecznej młodości. Gdy Titonos stał się zupełnie niedołężny, Zeus zamienił go w świerszcza lub cykadę.
W panteonie greckim odgrywała ona drugorzędną rolę, a jej kult należał do rzadkości. W mitach pojawiała się sporadycznie.
W sztuce przedstawiana jest zwykle jako urodziwa kobieta z wielkimi skrzydłami u ramion, w peplosie szafranowego (intensywnie żółty) koloru, powożąca rydwanem lub w locie przed wozem Heliosa, z kwiatami i pochodnią w ręce. Starożytni Grecy nazywali ją „różanopalcą” (gr. "rododáktylos" ‘mająca palce w kolorze przypominającym płatki róż’), „szafrannoszatą” (gr. "krokópeplos" ‘w peplosie szafranowego koloru’).
Wyobrażenie o bogini przejawia się w sztukach plastycznych, między innymi w greckim malarstwie wazowym (wazy z V wieku p.n.e. ukazujące skrzydlatą Eos goniącą Kefalosa oraz opłakującą Memnona), malarstwie olejnym i rzeźbie, oraz w muzyce i literaturze ("Iliada" Homera).
Imieniem bogini została nazwana jedna z planetoid – (221) Eos.

</doc>
<doc id="1500" url="https://pl.wikipedia.org/wiki?curid=1500" title="Eutektyka">
Eutektyka

Eutektyka (eutektyk, mieszanina eutektyczna) – mieszanina dwóch lub więcej faz o określonym składzie, która wydziela się z roztworów ciekłych w pewnej temperaturze, zwanej temperaturą eutektyczną. Mieszanina ta jest produktem przemiany eutektycznej. Nazwa wywodzi się z greckiego słowa "eutektos", co znaczy „łatwo topliwy”.
Ogólne informacje.
W większości przypadków temperatura eutektyczna jest niższa od temperatury krzepnięcia czystych składników. Wykorzystuje się to przy topieniu i przerabianiu materiałów trudno topliwych, na przykład mechaniczne złączenie materiałów doprowadza do dyfuzyjnego utworzenia eutektyki. Kryształy eutektyki są czystymi kryształami składników lub roztworami stałymi o różnych składach. Eutektyki są stałymi odpowiednikami mieszanin azeotropowych.
Dużo uwagi poświęcono badaniom stopów eutektycznych – liczne po zakrzepnięciu w odpowiednich warunkach tworzą regularną drobnodyspersyjną strukturę, w której jedna z faz występuje w postaci płytek lub włókien. Eutektykę można uznać za kompozyt "in situ", jeżeli wystąpi szczególne przestrzenne rozmieszczenie faz o różnych właściwościach. Stopy eutektyczne charakteryzują się niskimi temperaturami topnienia i dobrymi własnościami lejnymi.
Na rysunku 1 przedstawiono wykres fazowy z modelowym przykładem eutektyki stopów Fe–C, zwanej ledeburytem.
Rodzaje eutektyk.
W zależności od kształtu i rozmieszczenia dwu faz stałych tworzących eutektykę rozróżnia się mieszaniny:
Na rysunku 2 przedstawione są graficznie różne typy eutektyk.
Ocena morfologii eutektyk jest sprawą umowną. W zależności od wybranej techniki obserwacyjnej dla tego samego zgładu można określić różne typy eutektyki. Nie do końca również poznane są przyczyny powstawania konkretnego typu mieszaniny.
Z punktu widzenia osiągnięcia minimum swobodnej energii powierzchniowej najkorzystniejszą jest eutektyka globularna. W rzeczywistości liczne, rozproszone cząstki fazy globularnej muszą wielokrotnie zarodkować, co wymaga znacznego przechłodzenia.
Eutektyki prętowe i płytkowe mogą wzrastać przy niskich wartościach przechłodzenia. Ich powierzchnia międzyfazowa jest dużo większa od eutektyk globularnych i iglastych, toteż pojawia się większy czynnik energetyczny hamujący krystalizację.
Jeżeli jedna z faz w eutektyce charakteryzuje się znaczną anizotropią prędkości wzrostu, to rośnie w określonym kierunku krystalograficznym znacznie szybciej od osnowy i zarodkuje wielokrotnie. Taki mechanizm wzrostu promuje powstanie eutektyki iglastej.
Ogólnie na morfologię eutektyk mają wpływ poniższe czynniki:
Krzepnięcie eutektyk.
Wzrost kryształów w eutektykach jest zasadniczo podobny do wzrostu w stopach jednofazowych. Sterowanie gradientem temperatury i prędkością wzrostu pozwala zachować płaski front krystalizacji, a fluktuacje prędkości wzrostu prowadzą do zmian składu i struktury na kierunku tegoż wzrostu. Podczas takiego krzepnięcia domieszka jest wypychana do cieczy i dyfunduje zarówno w kierunku wzrostu, jak i w kierunku do niego poprzecznym.
Krzepnięcie eutektyk płytkowych i prętowych.
Krzepnięcie cieczy o składzie eutektycznym rozpoczyna się heterogenicznym zarodkowaniem jednej z faz, na przykład formula_1 (bogata w składnik A). Powoduje to jednoczesne wzbogacanie się w składnik B cieczy przed frontem przemiany. W przechłodzonych obszarach cieczy na fazie formula_1 zarodkuje faza formula_3 Tworzenie się fazy formula_4 powoduje z kolei wzbogacanie cieczy w składnik A, co sprzyja dalszemu wzrostowi kolejnych kryształów fazy formula_1 wzdłuż wydzieleń fazy formula_3 Taka mieszanina eutektyczna składa się z ułożonych na przemian płytek lub prętów faz formula_1 i formula_3 Gdy udział objętościowy fazy formula_4 jest większy od formula_10 eutektyka ma zwykle budowę płytkową. W przeciwnym przypadku tworzą się prętowe wydzielenia fazy formula_4 w osnowie fazy formula_12
Krzepnięcie cieczy o składzie eutektycznym może również polegać na ciągłym heterogenicznym zarodkowaniu i wzroście obu faz. Jedna faza (np. formula_1) zarodkuje, powodując wzbogacenie cieczy w składnik B i sprzyjając zarodkowaniu fazy formula_4 w sąsiedztwie. Faza formula_4 zarodkuje zupełnie niezależnie, nie wykazując uprzywilejowanej orientacji krystalograficznej w stosunku do istniejącej w innym miejscu fazy formula_1.
Krzepnięcie eutektyk globularnych i iglastych.
Krystalizacja eutektyk może być zapoczątkowana stałym heterogenicznym zarodkowaniem jednej z faz w cieczy przed frontem krystalizacji. Osnowa krystalizuje niezależnie z cieczy, w której istnieją już wydzielenia drugiej fazy. Mają one w stosunku do osnowy przypadkową orientację krystalograficzną. Eutektykę globularną uzyskuje się w wyniku wzrostu izotropowego, a eutektykę iglastą w wyniku anizotropowego wzrostu wydzieleń.
Prędkość wzrostu eutektyk płytkowych.
Problematyka.
Problematyka wzrostu eutektyk płytkowych wymaga rozwiązania dwu zagadnień:
gdzie:
Rozwiązanie.
Uzyskanie eutektyki o skończonych odległościach międzypłytkowych wymaga obniżenia temperatury poniżej temperatury eutektycznej. Krzepnięcie nie może się odbywać przy temperaturze eutektycznej, ponieważ powodowałoby to powstawanie eutektyki o nieskończenie dużych odległości międzypłytkowych.
Clarence Zener rozwiązał problem wpływu parametrów procesu na wielkość odległości międzypłytkowych przez założenie, że realizowane w danych warunkach odległości międzypłytkowe to takie, które maksymalizują prędkość wzrostu. Opisuje to poniższa relacja:
gdzie:
Krytyczna odległość międzypłytkowa dana jest wzorem:
Czynniki wpływające na prędkość wzrostu płytek.
Jeżeli prędkość wzrostu nagle zwiększy się, odległości międzypłytkowe będą zbyt duże w porównaniu ze średnimi drogami dyfuzji domieszki. Nadmiar substancji gromadzi się przed frontem krystalizacji szerszej płytki lub obiema, obniżając temperaturę procesu.
Prędkość wzrostu płytek silnie zależy od wielkości przechłodzenia formula_38:

</doc>
<doc id="1501" url="https://pl.wikipedia.org/wiki?curid=1501" title="Epikur">
Epikur

Epikur (gr. "Epikuros") (341 – 270 p.n.e. urodził się na wyspie Samos) – grecki filozof, twórca epikureizmu. Epikur był jednym z najważniejszych filozofów tzw. drugiej fazy greckiej filozofii klasycznej, w której dominowały zagadnienia filozofii życia – czyli rozważania na temat jak osiągnąć pełne szczęście (także stan ataraksji).
Epikur dzielił filozofię na trzy części:
W czasie swego życia, jak również po śmierci miał wielu uczniów i kontynuatorów m.in.:
Był także jednym z czterech wielkich „mistrzów” Karola Marksa, który poświęcił mu swoją pracę doktorską pt. „"Różnica między demokrytejską a epikurejską filozofią przyrody"”, 1841 r.
Życiorys.
Rodzice Epikura, Neocles i Chaerestrate, oboje ateńskiego pochodzenia, wyemigrowali do ateńskiej kolonii na wyspie Samos, około dziesięciu lat przed urodzeniem przyszłego filozofa. Jako chłopiec, Epikur przez okres czterech lat studiował filozofię pod kierunkiem platonika Pamfilusa. W wieku 18 lat udał się do Aten podejmując dwuletnią służbę wojskową.
Po śmierci Aleksandra Wielkiego, Perdikkas wydalił ateńskich osadników z Samos do Kolofonu, na wybrzeżu Azji Mniejszej. Po zakończeniu służby wojskowej, Epikur dołączył do swojej rodziny. Studiował u Nauzyfanesa, który był zwolennikiem nauk Demokryta. W 311/310 p.n.e. Epikur sam nauczał w Mytilenie, ale jego wystąpienia spowodowały zamieszki i został zmuszony do opuszczenia miasta. Następnie założył szkołę w Lampsakos, gdzie przez jakiś czas nauczał. Powrócił do Aten w 306 p.n.e. zakładając kolejną szkołę, nazwaną Ogrodem. Nazwę zawdzięczała ona ogrodowi usytuowanemu w posiadłości w połowie drogi między Stoą i Akademią, który służył jako miejsce spotkań z uczniami.
Mimo że wiele z jego nauk było pod silnym wpływem wcześniejszych Demokryta i innych myślicieli, różnił się w kwestii demokrytejskiego determinizmu. Epikur, często zaprzeczał istnieniu takiego czynnika, potępiał innych filozofów uznając ich poglądy za pogmatwane. Sam o sobie mówił, że jest filozoficznym „samoukiem”.
Nigdy się nie ożenił i nie miał dzieci. Cierpiał na kamicę nerkową, która ostatecznie stała się powodem jego zgonu. Umarł w 270 p.n.e. w wieku 72 lat Pomimo długotrwałego bólu tak pisał do Idomeneusa:
"Piszę ten list do Ciebie w szczęśliwym dla mnie dniu, który jest również ostatnim dniem mojego życia. Zostałem zaatakowany przez bolesną niemożność oddania moczu, a także biegunkę, tak gwałtowną, że nic nie może wyrazić mojego cierpienia. Ale radość mojego umysłu, która pochodzi z wspomnień wszystkich moich kontemplacji filozoficznych, równoważy wszystkie te dolegliwości. I błagam Cię, dbaj o dzieci Metrodorusa, w sposób godny pobożności ukazanej przez tego młodego człowieka dla mnie i dla filozofii".
Szkoła.
Szkoła Epikura liczyła niewielu, ale za to oddanych uczniów Głównymi jej członkami byli: Hermarchus, finansista Idomeneus, Leonteus i jego żona Themista, satyryk Colotes, matematyk Polyaenus z Lampsakos i Metrodorus z Lampsakos, najbardziej znany popularyzator epikureizmu. Jego szkoła była pierwszą z greckich szkół filozoficznych, która uznawała przyjmowanie kobiet bardziej za regułę niż za wyjątek. Pierwotnie szkoła miała siedzibę w domu i ogrodzie Epikura. Napis widniejący na bramie do Ogrodu został przywołany przez Senekę w XXI liście „Epistulae morales ad Lucilium”. Głosił on:
"Gościu, tutaj będzie ci dobrze, tutaj dobrem najwyższym jest rozkosz".
Epikur podkreślał rolę przyjaźni jako ważnego składnika szczęścia, a szkoła pod wieloma względami przypominała wspólnotę przyjaciół mieszkających razem pod jednym dachem. Jednakże nie przeszkadzało mu to w ustanowieniu hierarchicznego systemu wśród swoich zwolenników, a także nakłaniania ich do złożenia przysięgi wierności swoim podstawowym założeniom.
Poglądy.
Zapowiedź nauki i etyki.
Epikur jest kluczową postacią w rozwoju nauki i metody naukowej. Kluczowe jest tutaj jego twierdzenie, że nie należy niczego brać z góry na wiarę, ale uznawać tylko to, co zostało zbadane w drodze bezpośrednich obserwacji i dedukcji logicznej. Wiele z jego pomysłów na temat natury i fizyki traktować można jako zapowiedź ważnych pojęć naukowych naszych czasów.
Epikur był znaczącą postacią ery osiowej, okresu od 800 p.n.e. do 200 p.n.e. podczas którego podobnie rewolucyjne myśli pojawiły się w Chinach, Indiach, Iranie, na Bliskim Wschodzie i w starożytnej Grecji. Jako pierwszy w starożytnej Grecji sformułował postulat wzajemności jako fundament etyki (złota reguła etyczna). Różnił się on od utylitaryzmu sformułowanego później przez Johna Stuarta Milla, podkreślając istotną rolę minimalizacji szkód własnych i innych jako sposób na zwiększenie szczęścia.
Nauki Epikura stanowiły odejście od głównych idei współczesnych mu Greków jak i wcześniejszych myślicieli, jednakże wciąż opierały się na wielu wcześniejszych zasadach i terminach, ustalonych głównie przez Demokryta. Za Demokrytem Epikur był atomistą, wierzył, że podstawowymi składnikami świata są niepodzielne małe kawałki materii (y, gr. "atomos", „niepodzielne”) pływające w pustej przestrzeni (gr. "kenos"). Wszystko, co nam się jawi jest rezultatem ruchu atomów ich wzajemnego zderzania się ze sobą i oddziaływaniom zachodzącym bez celu lub planu. (Warto porównać to z nowoczesnymi badaniami w fizyce kwantowej.) Jego teoria różni się od wcześniejszego atomizmu Demokryta, gdyż zakłada, że atomy nie zawsze poruszają się po liniach prostych, a kierunek ich ruchu może niekiedy wykazywać „zboczenie” (gr. "clinamen"). To pozwoliło mu uniknąć determinizmu domyślnego we wcześniejszych atomizmach i utrzymać pojęcie wolnej woli.
Regularnie przyjmował kobiety i niewolników w swojej szkole, wprowadzając koncepcje podstawowych praw człowieka i egalitaryzmu w myśli greckiej. Był jednym z pierwszych Greków, którzy zrywali z bogobojną tradycją popularną w tamtym czasie. Z drugiej strony podkreślał użyteczność praktyk religijnych jako sposobu kontemplacji i myślenia o bogach i używania ich jako przykładu przyjemnego życia. Epikur uczestniczył w działalności tradycyjnej greckiej religii, ale nauczał, że należy unikać posiadania fałszywych opinii na temat bogów. Bogowie są nieśmiertelni, błogosławieni i bezbożnością byłoby przypisywanie im innych cech odmiennych od nieśmiertelności i błogosławieństwa.
Jego zdaniem bogowie nie karzą złych, ani nie nagradzają dobrych jak myśli większość. W opinii tłumu, twierdził Epikur, „bogowie wysyłają wielkie zło dla niegodziwych i wielkie błogosławieństwo dla tych sprawiedliwych, którzy się na nich wzorują”, podczas gdy w rzeczywistości bogowie w ogóle nie zajmują się ludźmi.
"Nie ten jest bezbożny kto zaprzecza bogom czczonym przez tłum, lecz ten kto podziela pogląd ludu na ich temat."
Przyjemność jako brak cierpienia.
Filozofia Epikura opierała się na teorii, że całe dobro i zło wynika z odczuwania przyjemności i bólu. Dobre jest to, co przyjemne, złe jest to, co bolesne.
Dla Epikura przyjemność i ból były ostateczną podstawą moralnego rozróżnienia między dobrem a złem. Jeśli ból zostaje świadomie wybrany zamiast przyjemności, to tylko dlatego, że w niektórych przypadkach prowadzi do większej przyjemności.
Jako wskazówki etyczne Epikur podkreślał istotę zminimalizowania bólu i maksymalizacji szczęścia swojego jak i innych:
"Nie da się wieść przyjemnego życia, jeśli nie żyje się mądrze, dobrze i sprawiedliwie („ani szkodzić, ani doznawać szkody” i nie da się żyć mądrze i dobrze i sprawiedliwie, nie żyjąc przyjemnie".
Dzieła.
Zachowały się nieliczne fragmenty dzieł filozofa, m.in. „Zasady” i odnalezione w Herkulanum części 37 ksiąg "O naturze" (patrz Papirusy z Herkulanum). Trzy listy Epikura znajdują się w "X księdze Żywotów Diogenesa Laertiosa", reszta zaginęła. Liczbę wszystkich prac Diogenes Laertios szacował na ok. 300, wymieniając z tytułu 40. Inne fragmenty i relacje można znaleźć w całej starożytnej literaturze greckiej i rzymskiej. Głównym źródłem wiedzy o Epikurze i jego systemie są prace rzymskiego filozofa i poety Lukrecjusza.

</doc>
<doc id="1502" url="https://pl.wikipedia.org/wiki?curid=1502" title="Etery koronowe">
Etery koronowe

Etery koronowe, korony – makrocykliczne związki organiczne pochodzenia syntetycznego. Są to cykliczne polietery o regularnej budowie wykazujące właściwości jonoforetyczne. Etery koronowe są zdolne do selektywnego tworzenia trwałych kompleksów (koronatów) z kationami, np. metali alkalicznych. Pierwszy eter koronowy, 18-korona-6, został otrzymany przez Charlesa Pedersena w 1967 roku. Badania nad eterami koronowymi o trójwymiarowymi strukturach (kryptandami, sferandami, karcerandami) prowadzili Donald Cram i Jean-Marie Lehn, którzy wraz z Pedersenem otrzymali Nagrodę Nobla w dziedzinie chemii w roku 1987 za „wkład w badania cząsteczek, które dzięki swej strukturze oddziałują w sposób wysoce selektywny z innymi cząsteczkami”.
Własności.
Etery koronowe są szczególnie interesującą klasą związków chemicznych z czterech powodów:
Własności te są szeroko wykorzystywane. Stosuje się je do:
Otrzymywanie.
Etery koronowe otrzymuje się zwykle poprzez typowe reakcje eteryfikacji, wychodząc z odpowiednich chlorków i alkoholi. Ich otrzymywanie wymaga prowadzenia reakcji w dużych rozcieńczeniach, gdyż zbyt duże stężenia substratów prowadzą do otrzymywania zwykłych, liniowych polieterów.
Nazewnictwo eterów koronowych.
Związki te, zależnie od ich budowy dzieli się na:
Etery ściśle cykliczne.
Nazwy tworzy się przez podanie liczebnika określającego wszystkie atomy wchodzące w skład makrocyklicznego pierścienia, następnie oddzielona myślnikami nazwa „korona” (ang. „crown”), a następnie liczbę atomów tlenu w pierścieniu makrocyklicznym. Jeśli w cząsteczce eteru koronowego fragmenty etylenowe są fragmentami pierścienia benzenowego lub cykloheksanowego podaje się odpowiednio w przed nazwą przedrostek „benzo” lub „cykloheksylo”, jeśli jest więcej takich pierścieni dodaje się odpowiedni liczebnik, np. dibenzo-18-korona-6, tricykloheksylo-21-korona-7.
Etery mieszane.
Do eterów koronowych zalicza się też czasami układy otwarte i półcykliczne, gdyż zachowują się one często podobnie do typowych eterów koronowych. Są to:

</doc>
<doc id="1505" url="https://pl.wikipedia.org/wiki?curid=1505" title="Eubakterie">
Eubakterie



</doc>
<doc id="1507" url="https://pl.wikipedia.org/wiki?curid=1507" title="Elektroniczny instrument płatniczy">
Elektroniczny instrument płatniczy

Elektroniczny instrument płatniczy – termin prawniczy, którego definicja przewidziana była w uchylonej ustawie z dnia 12 września 2002 r. o "elektronicznych instrumentach płatniczych". Oznaczała każdy instrument płatniczy, w tym z dostępem do środków pieniężnych na odległość, umożliwiający posiadaczowi dokonywanie operacji na pieniądzu elektronicznym przy użyciu elektronicznych nośników informacji lub elektroniczną identyfikację posiadacza niezbędną do dokonania operacji, w szczególności kartę płatniczą lub instrument pieniądza elektronicznego.

</doc>
<doc id="1508" url="https://pl.wikipedia.org/wiki?curid=1508" title="Ftyzjatria">
Ftyzjatria



</doc>
<doc id="1509" url="https://pl.wikipedia.org/wiki?curid=1509" title="Film">
Film

Film – seria następujących po sobie obrazów z dźwiękiem lub bez dźwięku, wyrażających określone treści, utrwalonych na nośniku wywołującym wrażenie ruchu. Utwór artystyczny wykorzystujący tę technikę, składający się ze scen, złożonych z jednego lub więcej ujęć. Pierwotnie filmy wyświetlano w kinach, po II wojnie światowej również w telewizji, w latach 80. pojawiły się wypożyczalnie wideo, w XXI wieku wraz z rozwojem szerokopasmowego dostępu do internetu oraz telefonii trzeciej generacji nastąpił rozkwit telewizji internetowej i usług typu VOD.
Technika filmowa.
Nośniki.
Podstawowym nośnikiem, na którym był utrwalany i z którego wyświetlano film, była światłoczuła błona fotograficzna 35 mm. Na taśmie filmowej obraz jest zapisany w postaci pojedynczych kadrów, przesuwających się w projektorze kinowym z prędkością 24 klatek na sekundę (w epoce kina niemego było to 16 klatek na sekundę).
Rzadziej stosowane były inne szerokości taśmy: 70, 16, 8 mm i super 16 i super 8. Dla potrzeb telewizji filmy rejestruje się także na nośnikach magnetycznych (Beta SP) i cyfrowych (Beta cyfrowa, DVCAM, High Definition). W wieku XXI powszechna stała się cyfrowa rejestracja obrazu filmowego na twardych dyskach. Tradycyjne kina przekształciły się w kina cyfrowe, gdzie filmy wyświetla się z dysków komputerowych lub za pośrednictwem internetu, z serwerów dystrybutorów.
Proporcje ekranu.
Początkowo filmy wyświetlano na ekranach w formacie obrazu 4:3, czyli w proporcji 1:1,33 (podstawa 4, wysokość 3) – taki format przyjęto też dla ekranu telewizyjnego w pierwszych latach rozwoju tego medium. Po wprowadzeniu na taśmę filmową ścieżki dźwiękowej format standardowy to 1:1,37. W walce z rosnącą konkurencją telewizji w latach 50. XX wieku wynaleziono szeroki panoramiczny ekran (cinemascope). W kamerach i projektorach zastosowano obiektywy anamorfotyczne; ekran wydłużył się do proporcji 1:2,55. W latach 60. prowadzono tzw. cineramę z wykorzystaniem taśmy 70 mm, z ekranem w proporcjach 1:2,75. Taśma 70 mm znalazła zastosowanie przy wyświetlaniu obrazów trójwymiarowych w systemie IMAX. Ekran ma tu proporcje 1:1,43, a kadr jest ustawiony na taśmie poziomo, a nie pionowo jak na taśmie 35 mm.
Obecnie w kinach najczęściej stosuje się format pośredni tzw. kaszetę (1:1,66 lub 1:1,85),
Rozwój technologiczny.
Większość współczesnych filmów jest udźwiękowiona – to znaczy, że posiada dźwięk ściśle zsynchronizowany z obrazem. Dawniej, do początków lat 30 XX wieku, popularne były filmy nieme, na których wszystkie teksty pojawiały się w formie napisów.
W historii filmu istniało kilka momentów przełomowych. Pierwszym z ważniejszych było wprowadzenie dźwięku, później koloru, różnych formatów ekranów (np. ekran panoramiczny – cinemascope), filmów trójwymiarowych (zob. IMAX), dźwięku stereofonicznego (Dolby Stereo). Jednym z ostatnich przełomów było pojawienie się technologii cyfrowej zarówno przy efektach inscenizacyjnych, jak i przy rejestracji obrazu (High Definition). Dotyczy to szczególnie filmu animowanego, gdzie coraz powszechniejsza jest animacja 3D imitująca przestrzeń trójwymiarową (zobacz np. Final Fantasy – realistyczną animację 3D, imitującą film aktorski). Wiek XXI to nowa epoka kina – film i kina cyfrowe.
Typy filmów.
Film to bardzo obszerna dziedzina i można ją podzielić na wiele sposobów. Podstawowy i najbardziej popularny z nich to podział na rodzaje.
Trzy podstawowe rodzaje filmu to:
Inne, często wyodrębniane rodzaje filmów:
Pojęcie "filmu" w społecznej świadomości najczęściej kojarzy się jednak z rozrywkowymi aktorskimi filmami fikcji nazywanymi też fabularnymi.
W zależności od długości filmy dzielą się na:
Dla telewizji powstają seriale telewizyjne złożone z wielu odcinków. Najczęściej stosowane długości odcinków to 13, 15, 25, 30, 50 minut.

</doc>
<doc id="1511" url="https://pl.wikipedia.org/wiki?curid=1511" title="Font">
Font

Font (ang., z „źródło”) – komputerowy nośnik pisma, znaki zaprojektowane w formie wektorów lub bitmapy, zestaw czcionek o określonych wspólnych cechach zapisany w postaci elektronicznej, zazwyczaj w jednym pliku.
Początkowo, gdy pojedyncze czcionki były obrazkami pojedynczych znaków (glifów) o określonym rozmiarze, na jeden zestaw składały się czcionki tylko jednego rozmiaru. Wraz z rozpowszechnieniem się fontów wektorowych, które z samej swojej natury są skalowalne, rozmiar stracił na ważności. Obecnie pojedynczy font to najczęściej zestaw czcionek danego kroju (np. "Arial") i odmiany (np. "Pogrubiony"). Czyli „Arial Pogrubiony” i „Arial Kursywa”, to dwa różne fonty jednego kroju Arial.
Istnieją jednak również fonty, w których możliwe jest tworzenie wielu odmian z pojedynczego zestawu czcionek (czyli z pojedynczego fonta). Takimi fontami były np. Multiple Master Fonts przedsiębiorstwa Adobe Systems.
Należy przy tym zaznaczyć, że font, w porównaniu do zestawu czcionek w danym kroju i odmianie, zawiera więcej informacji niż tylko same kształty znaków, są to np. pary kerningowe, możliwość stosowania znaków alternatywnych itd.
Formaty fontów.
Te same fonty (o tym samym kroju) mogą występować od strony czysto technicznej w wielu różnych formatach. Trzy najpopularniejsze z nich to: Type 1 (w skrócie T1), TrueType (w skrócie TTF) oraz OpenType (w skrócie OTF).
Type 1.
Type 1 to fonty postscriptowe. Format powstał w przedsiębiorstwie Adobe w 1985 r. równolegle z samym językiem PostScript. Pierwotnie fonty te zostały zastosowane w komputerach przedsiębiorstwa Apple, które w tym samym czasie jako pierwsze weszły na rynek z graficznym środowiskiem pracy użytkownika (a nie jak do tej pory – znakowym) oraz z przystosowaną do druku w tym trybie drukarką laserową. Umożliwiało to całkowitą zmianę w podejściu do tekstu drukowanego – można było drukować dowolne kształty liter i w płynnej skali wielkości.
Znaki w T1 opisane są za pomocą krzywych Béziera trzeciego stopnia tworzących obwiednie (kontury) kształtów znaków w dwuwymiarowym układzie współrzędnych. Krzywe te są definiowane poprzez ciągi punktów kontrolnych (węzłów). Fonty T1 pozwalają przekształcać litery jako obiekty graficzne zależnie od możliwości używanego oprogramowania (zmiana stopnia pisma, transformacja kształtu, niezależna zmiana konturu i wypełnienia itd.), oraz przede wszystkim w zależności od możliwości urządzeń drukujących (w różnych technikach), naświetlających czy plotujących.
W przeciwieństwie do TrueType, fonty Type 1 korzystają z metryk w osobnym pliku, zwykle w tekstowym formacie Adobe Font Metrics lub binarnym Printer Font Metric.
TrueType.
Format danych stworzony przez przedsiębiorstwo Apple jako antidotum na PostScript (za który trzeba było płacić), stosowany na komputerach Macintosh od 1991 r. – obecnie rozpowszechniony na wszystkich platformach komputerowych na równi z fontami Type 1. Znaki w TrueType opisane są za pomocą krzywych Béziera tylko drugiego stopnia, jednak jest to format znacznie bardziej skomplikowany technicznie. Daje za to większe możliwości – szczególnie pod względem jakości wyświetlania na ekranie monitora.
Czcionka TrueType czcionka, która może być skalowana i niekiedy jest generowana jako czcionka bitmapowa, zależnie od możliwości drukarki. Czcionki TrueType są niezależne od urządzenia i przechowywane w formie zbioru konturów znaków. Rozmiary czcionek tego typu można zamieniać odpowiednio z dowolną wysokością i drukować precyzyjnie w taki sposób, w jaki pojawiają się na ekranie.
Od 1992 r. stosowany również w Microsoft Windows 3.1, aczkolwiek dopracowany dopiero w Microsoft Windows 95. Słaba premiera w starszych wersjach systemu Windowsa spowodowała odwrócenie się na pewien czas producentów od profesjonalnego (czyli dla DTP) wykorzystania formatu TT, tym bardziej że Adobe odtajniła częściowo swój konkurencyjny standard (czyli T1) oraz udoskonaliła i rozpowszechniła oprogramowanie rasteryzujące (ATM), co spowodowało dominację na wiele lat standardu T1 w zastosowaniach profesjonalnych (poligrafia). Obecnie fonty w obu standardach są tak samo dobre, a poważne niegdyś problemy z TrueType w DTP należą już do przeszłości, i to do tego stopnia, że teraz u producentów oprogramowania to standard T1 cieszy się mniejszą popularnością w porównaniu z TrueType, a nawet przez samą Adobe uznany został za nierozwojowy, a w konsekwencji całkowicie poniechany w produkcji nowych fontów przez to przedsiębiorstwo.
OpenType.
Najnowszym formatem fontów jest OpenType, który ma również zakończyć istnienie wielu różnych formatów i pozostawienie tylko jednego – do używania bezpośrednio na wszystkich platformach komputerowych. Jest wspólnym dziełem przedsiębiorstw Adobe i Microsoft i choć prace nad nim zaczęły się już w 1996 r., to pierwszą aplikacją DTP obsługującą ten format był dopiero Adobe InDesign. 
Fonty w formacie OpenType mają znaki kodowane w Unicode, a ponadto zawierają szereg nowych funkcji niedostępnych w starszych formatach, jak np. znaki alternatywne i inne tzw. funkcje zecerskie, czy osadzanie krojów na stronach WWW (umożliwiające wyświetlanie ich w przeglądarce na komputerze, w którym tych znaków nie ma zainstalowanych). OpenType może zawierać obrysy PS lub TTF z pełną implementacją dziesiątków tysięcy znaków Unicode.
Pozostałe formaty.
Inne formaty to np. "TeX font metric" (TFM) oraz "generic font" (GF) – fonty programu METAFONT. Obecnie za pomocą programu MetaPost można utworzyć również postscriptowy font Type 3 ze źródła przeznaczonego dla METAFONT.
Niegdyś używane były również fonty bitmapowe (), w których kształty poszczególnych znaków zdefiniowane były jako obrazki bitmapowe na sztywno dla wybranych stopni pisma i rozdzielczości monitorów czy drukarek, co bardzo utrudniało ich stosowanie w innych wielkościach nie mówiąc już o poważniejszych transformacjach. Przykładem formatu fontów bitmapowych jest GF, częściej konwertowany na skompresowany PK – "packed font". GF jest generowana dla danego urządzenia i rozdzielczości. Dziś stosowane one są rzadko – praktycznie tylko w urządzeniach z wyświetlaczami pracującymi w określonej rozdzielczości oraz prostych szybkich urządzeniach drukujących.
Ciekawą odmianą są fonty używane przez program Calamus – fonty typu CFN (Calamus FoNt). Jest to niejako połączenie czcionek typu TT i PS – opis znaków na wektorowych wzorach 3 stopnia na tak zwanych b-spline’ach, podobnie jak w fontach postscriptowych, ale z zachowaniem układu jak w zestawach typu True Type.
Kodowanie znaków.
Strony kodowe.
Przez wiele lat w pojedynczym foncie można było zapisać tylko 256 znaków (fonty jednobajtowe). Była to liczba niewystarczająca nawet do umieszczenia w jednym zestawie wszystkich znaków diakrytycznych wszystkich języków posługujących się alfabetem łacińskim. Powodowało to konieczność tworzenia odmian regionalnych dla fontów, np. wersji dla środkowej Europy, państw bałtyckich itd. Fonty greckie, cyrylica, znaki z innych języków, znaki fonetyczne, piktogramy, symbole nut itd. z założenia zawierały się od razu w odrębnych zestawach 256 znakowych.
Wszystkie te fonty były podzielone na 2 części: pierwsze 128 znaków było najczęściej takie samo, a druga połowa stanowiła dla każdej z wersji językowych oddzielny repertuar znaków charakterystyczny dla danej grupy języków z towarzyszeniem pewnej liczby innych uniwersalnych symboli. Dokładniej – w pierwszej połówce znaki od 0 do 31 oraz znak 127 zawierały tzw. kody sterujące jak znak tabulacji poziomej (9), powrotu karetki (13) czy escape (27), oraz znaki od 32 do 126 zawierające znaki drukowalne, czyli wszelkie wielkie i małe litery, cyfry, spację i inne symbole jak przecinek (44), kropka (46) czy średnik (59). Jeśli zaś były to fonty nie z literami tylko z innymi znakami czy symbolami, to nawet pierwsza połówka mogła być nimi zapełniona. Podział na 2 części wynikał z tego, że pierwotnie kodowanie to było zaledwie 7-bitowe (oferowało więc znaki jedynie z zakresu 0-127), a nie jak obecnie 8-bitowe (0-255).
Unicode.
Obecnie upowszechniły się fonty (TT i OTF) w wersji dwubajtowej, czyli w standardzie Unicode, co umożliwia zapisanie w jednym pliku (i jednym foncie) do 65536 znaków. Stało się przez to możliwe wygodne stosowanie naraz nie tylko alfabetów wszystkich języków indoeuropejskich, ale nawet najważniejszych znaków języków Dalekiego Wschodu. Jednocześnie jest miejsce na wszelkie ligatury, kapitaliki, indeksy, znaki specjalne, piktogramy etc., ale najważniejszą rzeczą jest to, że w Unicode jest tylko jeden standard kodowania znaków – obowiązujący na wszystkich platformach (np. polskie „ą” ma odtąd ten sam numer kodu na wszystkich komputerach świata). Odpadły więc wszelkie problemy z konwertowaniem tekstów między komputerami opartymi na standardzie Apple Macintosh a tymi w standardzie IBM PC, czy też między różnymi standardami kodowania tych samych znaków w obrębie PC.
Należy jednak zwrócić uwagę, że Unicode to nie jest nowy format fontu, tylko zestaw znaków, tak jakby kolejna strona kodowa – tyle że uniwersalna, a przez to ostatnia. Więcej już ich nie będzie, ponieważ Unicode jest zestawem, w którym mieszczą się naraz znaki ze wszystkich dotychczasowych stron kodowych.

</doc>
<doc id="1512" url="https://pl.wikipedia.org/wiki?curid=1512" title="Figura wypukła">
Figura wypukła



</doc>
<doc id="1513" url="https://pl.wikipedia.org/wiki?curid=1513" title="Fermat">
Fermat



</doc>
<doc id="1515" url="https://pl.wikipedia.org/wiki?curid=1515" title="Funkcja zdaniowa">
Funkcja zdaniowa

Funkcja zdaniowa (inaczej predykat lub formuła zdaniowa, także forma zdaniowa) to wyrażenie językowe zawierające zmienne wolne, które w wyniku związania tych zmiennych kwantyfikatorami lub podstawienia za nie odpowiednich wartości staje się zdaniem. W ujęciu formalnym jest to funkcja, której wartościami są zdania - choć to ujęcie nie eksponuje możliwości otrzymania z funkcji zdaniowej zdania przez skwantyfikowanie jej argumentów; jeżeli w funkcji zdaniowej o wielu argumentach skwantyfikujemy część argumentów, a za część pozostałych podstawimy elementy stosownych zbiorów, to otrzymamy nową funkcję zdaniową zależną od tych argumentów, których ani nie skwantyfikowano ani nie podstawiono.
Dla funkcji (formuły) zdaniowej "F(x)" o jednej zmiennej wolnej x, rozważanej w zbiorze X, wprowadza się pojęcie dziedziny "DX(F)" lub "D(F,X)" funkcji zdaniowej, obejmując tą nazwą podzbiór elementów zbioru X o tej własności, że po podstawieniu w funkcji zdaniowej "F(x)" w miejsce zmiennej "x" tych elementów otrzymuje się zdanie prawdziwe lub fałszywe.
Każde równanie liczbowe i każda taka nierówność z jedną niewiadomą jest funkcją (formuła) zdaniową, której dziedziną jest pewien zbiór liczb. Każde równanie z dwiema lub więcej niewiadomymi jest funkcją zdaniową, której dziedziną jest zbiór par lub trójek lub odpowiednio większej liczby argumentów. Jeżeli zdanie "F(a)" jest prawdziwe, to mówi się, że element "a" spełnia funkcję zdaniową "F(x)". Zbiór elementów zbioru X spełniających daną funkcję zdaniową nazywa się ekstensją funkcji zdaniowej lub wykresem formuły zdaniowej w X.
Przykład.
Funkcja zdaniowa "x&gt;2" zamienia się w zdanie dla tych "x", dla których ten zapis ma sens. Wszelkie liczby rzeczywiste należą więc do jej dziedziny, podczas gdy na przykład nazwa LEW lub liczba zespolona "1+2i" już nie. Ekstensją natomiast jest podzbiór liczb rzeczywistych, które są większe od 2.

</doc>
<doc id="1516" url="https://pl.wikipedia.org/wiki?curid=1516" title="Forma zdaniowa">
Forma zdaniowa



</doc>
<doc id="1517" url="https://pl.wikipedia.org/wiki?curid=1517" title="Fał">
Fał

Fał – element olinowania ruchomego. Służy do podnoszenia ruchomych elementów ożaglowania, omasztowania i innych. Nazwę tę noszą nie tylko wszystkie liny służące do stawiania żagli, ale także niektóre z lin podnoszących ruchome części omasztowania oraz liny do podnoszenia płetwy sterowej oraz miecza. Jeśli poluzowanie fału nie powoduje opuszczenia danego elementu pod wpływem siły ciężkości to stosuje się linę o działaniu przeciwnym - kontrafał. Każdy fał służy konkretnemu celowi, a jego pełna nazwa powstaje poprzez połączenie z nazwą elementu, który jest podnoszony, np. fał foka (inaczej fokfał), fał grota (grotfał), fał miecza, fał piku (pikfał) itp.
W przypadku ożaglowania bermudzkiego fał mocowany jest przy pomocy szekli do ucha fałowego, następnie przechodzi przez blok na topie masztu i biegnie wolnym końcem w kierunku pokładu, gdzie wiązany jest na knadze. Tam gdzie ciężar podnoszonego żagla jest znaczny stosuje się miękką, stalową linę mocowaną do ucha fałowego przeprowadzoną przez blok i zakończoną kolejnym blokiem, przez który przebiega miękka lina wybierana z pokładu. Daje to dodatkowe przełożenie redukujące potrzebną siłę załogi. Dodatkowe ułatwienie stanowi koncepcja wciągarki zwanej windą fałową. Może być ona obsługiwana ręcznie poprzez korbę lub elektryczna.
Przy ożaglowaniu gaflowym do postawienia żagla potrzebne są dwa fały: pikfał oraz gardafał. Pierwszy mocowany jest do piku gafla, drugi zaś do okucia gardy. W pierwszej kolejności wybiera się pikfał, następnie gardafał. 

</doc>
<doc id="1518" url="https://pl.wikipedia.org/wiki?curid=1518" title="Tor wodny">
Tor wodny

Tor wodny (z niderl. "farwater") – droga dla statków, bezpieczna o odpowiedniej głębokości, wyznaczona na akwenie trudnym lub niebezpiecznym, oznaczona stałymi (np. brama torowa) lub pływającymi znakami nawigacyjnymi, na której może odbywać się bezpiecznie i swobodnie ruch żeglugowy. Kierunek (prawa i lewa strona) toru wodnego, prowadzi zawsze z morza do portu, ujścia rzeki lub do innej drogi wodnej.
Na wodach śródlądowych kierunek toru jest zgodny z kierunkiem prądu rzeki, a na wodach stojących jest wyznaczany przez władze lokalne. Ważną kwestią jest utrzymanie szlaku żeglownego.
Na przykład farwater szczeciński (tor wodny Świnoujście–Szczecin) to droga wodna począwszy od pławy podejściowej w Zatoce Pomorskiej przez Świnoujście, Zalew Szczeciński i Odrę do portu morskiego w Szczecinie.

</doc>
<doc id="1519" url="https://pl.wikipedia.org/wiki?curid=1519" title="Filozofia">
Filozofia

Filozofia ( od: – „miły, ukochany” i – „mądrość”, tłumaczone jako „umiłowanie mądrości”) – różnie definiowany element kultury umysłowej. Obejmuje te krytyczne rozważania, które nie są oparte na wierze i nie należą do żadnej z nauk formalnych ani empirycznych. Bywa zaliczana do nauk i kontrastowana z naukami szczegółowymi. Czasem definiuje się ją:
Zakres znaczeniowy filozofii ewoluował i w latach 20. XXI wieku pozostaje on wieloznaczny:
Filozofia wyłoniła się najpóźniej w starożytności, niezależnie na co najmniej trzech obszarach Eurazji: w basenie Morza Śródziemnego, na subkontnencie indyjskim oraz w Chinach, przez co wyróżnia się trzy tradycje filozoficzne: zachodnią i dwie wschodnie: indyjską oraz chińską. Ta pierwsza występowała nie tylko w Europie, ale również na innych obszarach kultury greckiej, hellenistycznej, rzymsko-bizantyjskiej oraz islamskiej. Nazwy pozostałych dwóch tradycji także opisują obszar tylko w przybliżeniu, ponieważ myśl indyjską – związaną z hinduizmem – rozwijano także w innych miejscach Azji, a idee chińskie dotarły też do innych obszarów Azji Wschodniej. Geograficzne granice tych filozofii zatarły się w epoce globalizacji.
Z filozofii wyłoniły się pewne koncepcje, które stały się później hipotezami ściśle naukowymi jak atomizm, który dodatkowo został potwierdzony, tj. stał się paradygmatem badawczym. W XIX wieku jedna z dyscyplin filozofii – logika – stała się dziedziną matematyki. Filozofia zajmuje się też samą definicją naukowości – zwaną problemem demarkacji – i posłużyła krytyce koncepcji, które postulowano jako naukowe, np. marksizmu i psychoanalizy. Filozofia wytworzyła też i rozwinęła szereg doktryn politycznych i ideologii, wpłynęła na ewolucję religii i sztuki. Wpływowym filozofom czasem przyznaje się nagrody jak jedna z kategorii Nagrody Rolfa Schocka, Nagroda Klugego czy Nagroda Templetona. Piśmiennictwo filozoficzne nagradzano też literacką Nagrodą Nobla.
Główne działy.
W tradycji zachodniej wyróżnia się kilka centralnych obszarów filozofii:
Poszczególne dyscypliny filozoficzne, często zawierają elementy kilku dyscyplin centralnych, np. filozofia społeczna obejmuje kwestie etyczne, ontologiczne i epistemologiczne. Inne dziedziny to:
Różnorodność filozofii.
Pojęcia "filozofii" i "filozofa" pojawiły się w starożytnej Grecji ok. VI/V w. p.n.e. Po raz pierwszy poświadczone są u Herodota. Początkowo oznaczały szeroko rozumianą ciekawość intelektualną i poszukiwanie wiedzy. Ściślejszy sens terminom nadał Sokrates, Platon, a za nim Arystoteles. Celem filozofii miała być teoria (, "theorίa)", czyli poznanie prawdziwej rzeczywistości, w odróżnieniu od wiedzy pozornej czy przemijającej (, "doksa"), a także od wiedzy praktycznej (, "praksis"). W dalszym swoim rozwoju, znaczenie terminu "filozofia" ulegało znaczącym zmianom i nie można go definiować w oderwaniu od jego historii. 
Filozofia jako forma poznania i oświecenia.
Ze względu na cele jakie sobie stawia, filozofię można ujmować albo jako przedsięwzięcie poznawcze na podobieństwo nauki albo też jako formę oświecenia, samowiedzy czy przemiany tożsamości. Oba znaczenia terminu filozofia często się przenikają w dziełach i życiu poszczególnych filozofów.
Filozofia rozumiana jako nauka, stawia sobie za zadanie prawdziwe poznanie danego przedmiotu. Z tak rozumianej filozofii wywodzą się współczesne szczegółowe dyscypliny naukowe. Leszek Kołakowski zaproponował podział typów filozofii, ze względu na cele poznawcze jakie sobie stawia:
Z kolei „Filozofia jako oświecenie” nastawiona jest na przemianę filozofującego podmiotu, który może być rozumiany indywidualnie lub zbiorowo (np. społeczeństwo). Filozofia ma być formą osiągania samowiedzy, środkiem przemiany podmiotu zgodnie z określonym systemem wartości, lub też formą terapii.
Paradygmaty filozofii.
Zasadnicze zmiany w sposobie filozofowania i zainteresowaniach filozofii określić można mianem zmiany paradygmatów. Wyróżnić można trzy, następujące po sobie paradygmaty: ontologiczny, mentalistyczny i lingwistyczny. Różnice między nimi można przedstawić w tabeli:
Trzeba tu jednak wyraźnie zaznaczyć, iż obecnie paradygmaty te się przenikają. I tak np. problem świadomości nie sprowadza się już jedynie do wiedzy, ale także do zrozumienia (co i jak mogę zrozumieć?). Język nie wiąże się już tylko z przedmiotem, ale także podmiotem jako tym, który partycypuje w języku.
Paradygmat ontologiczny.
Paradygmat ontologiczny był pierwszym modelem uprawiania filozofii. Do dziś, dla wielu osób, pozostaje wzorcem filozofii jako takiej. Stąd filozofia uprawiana w paradygmacie ontologicznym określana jest mianem "filozofii klasycznej". Centralną dyscypliną takiej filozofii jest metafizyka. Paradygmat ontologiczny dominował w starożytności i średniowieczu. Swoją rozwiniętą postać uzyskał w twórczości Platona i Arystotelesa, a współcześnie obecny jest m.in. w neotomizmie.
W paradygmacie ontologicznym, filozofia jest nauką o bycie i jego formach. Filozofowanie wychodzi od przedmiotu, zajmuje się zasadniczymi formami jego istnienia. Zasadniczym jej pytaniem jest „co istnieje?” Arystoteles definiował taką filozofię, jako „wiedzę rozważającą byt jako byt”.
Paradygmat mentalistyczny.
Zasadniczy zwrot filozoficzny dokonał się we wczesnej nowożytności i łączony jest przede wszystkim z filozofią Kartezjusza. Charakterystyczny dla nich typ filozofowania jest jednak starszy i jego początki można odnaleźć już u starożytnych sceptyków.
O ile filozofia nastawiona ontologicznie zajmowała się przedmiotem i jego istnieniem, to filozofia mentalistyczna postawiła pod znakiem zapytania możliwość prawdziwego poznania. Filozofia taka wskazywała, że nim możemy określić jaki jest byt, należy najpierw określić czy prawdziwe poznanie jest możliwe i jakie są jego warunki. Zasadniczym jej pytaniem jest „co można poznać?”, „co można wiedzieć?”. Stąd też, zamiast ontologii, w paradygmacie mentalistycznym zaczęto uprzywilejowywać epistemologię.
Również w tym okresie, tworzono systemy filozoficzne z rozbudowaną teorią bytu. Punktem wyjścia było jednak wątpienie. Dopiero rozstrzygając zasadnicze kwestie epistemologiczne (jakiego rodzaju wiedza jest wiedzą pewną), filozofowie przystępowali do stawiania tez ontologicznych. Przykładem rozwiniętej tradycji mentalistycznej jest klasyczna filozofia niemiecka, obejmująca twórczość takich filozofów jak Immanuel Kant, Johann Gottlieb Fichte, Friedrich Wilhelm Joseph von Schelling i Georg Wilhelm Friedrich Hegel.
W opinii niektórych nurtów filozoficznych, zmiana paradygmatu na mentalistyczny oznaczała upadek filozofii. Tomiści krytykowali filozofię nowożytną, za rezygnację z zajmowania się bytem, i skupienie się na świecie subiektywnych idei. Miało to oznaczać rezygnację z autonomii dyscypliny, i odejście od jej klasycznego programu wyznaczonego przez Arystotelesa. Dla marksistów mentalistyczny zwrot w filozofii był oznaką "burżuazyjnej ideologii", ukrywającej prawdziwą postać świata.
Paradygmat lingwistyczny.
Pogłębiona refleksja nad źródłami poznania, jaką rozwijano w nowożytności, doprowadziła do rosnącego przekonania o dominującej w tym procesie roli języka. Stąd, na początku XX wieku nastąpił tzw. „zwrot lingwistyczny” w filozofii, który doprowadził do ukształtowania się paradygmatu lingwistycznego. Centralną postacią w tym procesie był Ludwig Wittgenstein. Filozofowie zwrócili uwagę, że język jest zasadniczym medium zapośredniczającym wyjaśnianie świata i w konsekwencji filozofia powinna się skupić na jego analizie.
Zadanie to podjęła szczególnie filozofia analityczna. Lingwistyczny charakter miały też takie nurty jak pozytywizm logiczny czy postmodernizm (szczególnie dekonstrukcjonizm). Ukształtowanie się paradygmatu lingwistycznego wiązało się też ze znacznym rozwojem filozofii języka. Zdaniem filozofów paradygmatu lingwistycznego, wiele problemów, którymi dotychczas zajmowała się filozofia było pseudoproblemami, wynikającymi z nieprecyzyjnego zdefiniowania używanych pojęć. Prawdziwym zadaniem filozofii jest natomiast oddzielanie pseudoproblemów od problemów prawdziwych, czyli określenie co może zostać poznane i wyjaśnione, i jak można o tym sensownie mówić. Filozofia ma więc pełnić funkcję porządkującą i przygotowującą grunt pod badania naukowe nauk szczegółowych. Ma zajmować się analizą ich języka (języka nauki, języka etyki) i usuwać problemy językowe.
Paradygmat lingwistyczny, podobnie jak mentalistyczny, jest krytykowany przez tomistów za zerwanie z klasyczną ideą filozofii, jeszcze dalsze (w stosunku do filozofii mentalistycznej) odejście w subiektywizm, popadanie w irracjonalizm i jest określany jako upadek filozofii.
Filozofia a inne dziedziny.
Filozofia jest często ujmowana jako metadyscyplina – najbardziej ogólna z dziedzin wiedzy, wyznaczająca ich podstawy metodologiczne i warunki funkcjonowania. Z drugiej strony jest ona dyscypliną autonomiczną – mającą własną problematykę i metody badawcze. Już u swoich korzeni w starożytnej Grecji zasadniczym problemem były kwestie odróżniania filozofii od innych dziedzin działalności ludzkiej. Filozofia powstała poprzez odróżnienie od mitu. Podobnie jak mit, zajmowała się wyjaśnianiem podstawowych kwestii dotyczących rzeczywistości. Mit czynił to jednak metodami poetyckimi, opierał się na wyobraźni i wierze, natomiast filozofia starała się to robić poprzez racjonalne rozumowanie. Zgodnie z tym wyjaśnienie początków świata w "Theogonii" Hezjoda ma charakter mityczny, a za pierwszego filozofa, starającego się rozumowo wyjaśnić przyczyny wszechrzeczy, uznaje się Talesa z Miletu.
Współcześnie szczególne problemy nastręcza odróżnienie filozofii od nauki i teologii. Bertrand Russell wskazywał, że filozofia jest dyscypliną pośrednią między nimi, atakowaną przez każdą ze stron. Podobnie jak teologia, zajmuje się pytaniami, na które nauka nie potrafi udzielić odpowiedzi, a jej rozważania mają charakter spekulatywny. W przeciwieństwie jednak do teologii, nie przyjmuje dogmatycznych założeń, a przyjmuje bliską nauce postawę krytyczną i dążącą do wyjaśnienia.
Filozofia a nauka.
W starożytności nie było podziału między filozofią a nauką. Narodziły się one razem w VI w. p.n.e. Arystoteles nie widział istotnej różnicy między rozważaniami na temat państwa a botaniką. Być może około II wieku p.n.e. z filozofii wyodrębniły się prawo i medycyna, a na pewno stało się to w średniowieczu. Wtedy jednak też nie było ostrego rozróżnienia, choć czasem mówiono o filozofii jako wiedzy niepochodzącej z objawienia wyższego stopnia i nauce jako wiedzy niepochodzącej z objawienia niższego stopnia. Jeszcze Kopernik oddawał swoje dzieło pod osąd filozofów, a intencje jego pracy były filozoficzne (por. filozofia polska).
Dopiero później, po opracowaniu metody naukowej, coraz ostrzej zarysowywał się podział. Do dzisiaj istnieją różne koncepcje stosunku filozofii do nauki, które można rozmieścić na skali, gdzie jednym końcem będzie teza, że filozofii w ogóle nie ma (neopozytywizm), a drugim klasyfikacje, które do filozofii zaliczają całą naukę (neotomizm).
Mówi się, że nauka ma duży wpływ na tendencje w filozofii. Platon tworzył pod wrażeniem geometrii, Arystotelesa inspirowały astronomia i biologia, a Leibniza – matematyka. Paradoksalnie najdłużej trwają jednak te koncepcje filozoficzne, które nie bazują na rozwiązaniach naukowych, a jak pokazuje XX-wieczna filozofia nauki, koncepcja nauki jako dziedziny jasnych i ostatecznych rozwiązań, co do których istnieje powszechna zgoda badaczy, jest też modelem dalekim od realnej praktyki naukowej.
Z drugiej strony można zauważyć, że gdy filozofia opanuje jakiś obszar do punktu, w którym zaczyna spełniać standardy nauki, obszar ten wydziela się z niej i staje się autonomiczną dziedziną naukową. Tak się stało na przykład z psychologią i socjologią, wyodrębnionymi z filozofii na przełomie wieku XIX i XX.
Filozofia a religia.
Religie, podobnie jak doktryny filozoficzne, odpowiadają na najbardziej podstawowe pytania o naturę i sens świata czy życia. Robią to inaczej niż filozofia, która jest oparta na rozumie, natomiast religia opiera się na wierze w objawienie. Wierzenia religijne mają nadto charakter partykularny i przyjmują postać konkretnej doktryny religijnej. Z kolei filozofia (w szczególności filozofia religii) zajmuje się takimi pojęciami jak Bóg, absolut, religia, sacrum w najbardziej ogólnym sensie. Rozumową analizą treści wiary religijnej zajmuje się teologia. Stosunki między tymi dwoma zjawiskami kształtowały się różnie – zdarzały się zarówno wzajemne wpływy jak i opozycja:
Dzieje filozofii.
Periodyzacja filozofii zachodniej.
Najpopularniejszy sposób periodyzacji historii filozofii zachodniej opiera się na ogólnym podziale zachodniej historii na starożytność, średniowiecze i nowożytność. W historii filozofii zastosował go Wilhelm Gottlieb Tennemann, w oparciu o wcześniejszą periodyzację Johanna Jakoba Bruckera. Zgodnie z nim dzieje filozofii dzielą się na trzy etapy:
Jest on luźno stosowany w niektórych pracach, jednak był krytykowany jako zbyt schematyczny i ogólny. Przez to część historyków nie wyróżnia tak szerokich epok, dzieląc dzieje filozofii na mniejsze jednostki chronologiczne i geograficzne. Okres patrystyczny i renesans są często uważane za okresy przejściowe.
Niektórzy polscy historycy filozofii przyjmują inną konwencję. Etap nowożytny uznają za zamknięty w I połowie XIX wieku; datą graniczną jest często śmierć Hegla w 1831 roku. Poźniejszą filozofię nazywają współczesną.

</doc>
<doc id="1520" url="https://pl.wikipedia.org/wiki?curid=1520" title="Fryderyk Chopin">
Fryderyk Chopin

Fryderyk Franciszek Chopin (), forma spolszczona: Szopen (ur. 22 lutego lub 1 marca 1810↓ w Żelazowej Woli, zm. 17 października 1849 w Paryżu) – polski kompozytor i pianista. Od października 1831 roku mieszkał we Francji.
Jest uważany za jednego z najwybitniejszych kompozytorów romantycznych, a także za jednego z najważniejszych polskich kompozytorów w historii. Był jednym z najsłynniejszych pianistów swoich czasów, często nazywany "poetą fortepianu". Elementem charakterystycznym dla utworów Chopina jest pogłębiona ekspresja oraz czerpanie z wzorców stylistycznych polskiej muzyki ludowej.
Z okazji 150-lecia urodzin Fryderyka Chopina uchwałą Rady Państwa z 8 maja 1958 rok 1960 został ogłoszony Rokiem Chopinowskim. Z okazji jubileuszu 200-lecia urodzin Chopina zarówno uchwałą Sejmu RP z 9 maja 2008, jak i uchwałą Senatu RP z 7 października 2009, rok 2010 został ustanowiony Rokiem Fryderyka Chopina.
Życiorys.
Dzieciństwo.
Według legendy Chopin urodził się w czasie, gdy w pobliżu jego ojciec Mikołaj grał na skrzypcach w jednej z dworskich oficyn Kaspra Skarbka, w której mieszkała rodzina Mikołaja i Tekli Justyny z Krzyżanowskich. Na chrzcie nadano mu imiona Fryderyk Franciszek (na cześć ojca chrzestnego i zapewne dziadka – François). W księdze metrykalnej z kościoła w Brochowie jako chrzestni widnieją Franciszek Grembecki ze wsi Ciepliny wraz z panną Anną Skarbkówną, hrabianką z Żelazowej Woli. Sami Chopinowie jako chrzestnego zwyczajowo traktowali młodego hrabiego Fryderyka Floriana Skarbka oraz jego rok młodszą siostrę Annę Emilię.
Jesienią 1810 Mikołaj i Justyna wraz z dziećmi przenieśli się do Warszawy. W październiku zamieszkali w nieistniejącej obecnie kamienicy należącej do Jana Böhma przy Krakowskim Przedmieściu nr hipoteczny 411 (obecnie w tym miejscu znajduje się kamienica Józefa Grodzickiego pod nr 7). Tam w lipcu 1811 przyszła na świat ich druga córka, Izabella. Wkrótce potem Chopinowie przeprowadzili się do służbowego mieszkania w pałacu Saskim, w którym mieściło się Liceum Warszawskie, gdzie Mikołaj miał uczyć języka francuskiego.
Przeprowadzka do Warszawy wynikała prawdopodobnie z pogarszającej się sytuacji finansowej Skarbków. Kasper Skarbek prowadził hulaszczy tryb życia i popadał w długi, a po rozwodzie z Ludwiką (1807) uciekł z Księstwa Warszawskiego do Wielkiego Księstwa Poznańskiego. Również dorastające dzieci Skarbków nie wymagały już opieki guwernera. Mikołaj prawdopodobnie myślał o przeprowadzce do dawnej polskiej stolicy jeszcze przed urodzeniem się syna. Po wyjeździe na stałe do Warszawy Chopinowie utrzymywali bliskie kontakty z rodziną Skarbków – Fryderyk jeździł tam na wakacje, a młody Fryderyk Florian Skarbek wydał pierwsze polonezy Chopina.
Na przełomie czwartego i piątego roku życia Chopin rozpoczął naukę gry na fortepianie, początkowo u swej matki. W 1816 zaczął brać lekcje u Wojciecha Żywnego. Szybko się uczył. 27 listopada 1831 Mikołaj pisał do Fryderyka:
Żywny sam nie był wybitnym muzykiem, a zdolnego i pojętnego ucznia uczył techniki palcowania i tradycyjnego ułożenia ręki. Podczas lekcji koncentrował się głównie na zaznajamianiu ucznia z dziełami muzyki barokowej i klasycznej oraz objaśnianiu budowy utworów fortepianowych Johanna Sebastiana Bacha, Josepha Haydna, Wolfganga Amadeusza Mozarta oraz (w mniejszym stopniu) Johanna Nepomuka Hummla. Pozostałością po tej niekonwencjonalnej edukacji było zamiłowanie Fryderyka do dawnych kompozytorów. Następnym nauczycielem Fryderyka był, pochodzący – tak samo jak Żywny – z Czech, Wacław Wilhelm Würfel.
Przed ukończeniem siódmego roku życia był już autorem kilku drobnych kompozycji (były to polonezy – owa forma muzyczna była w polskiej muzyce fortepianowej tą, która wraz z wychowaniem muzycznym Żywnego oraz modną w owym czasie operą w stylu włoskim składała się na atmosferę muzyczną, w jakiej dorastał Fryderyk), które pomagali zapisywać mu Żywny oraz ojciec.
W tym okresie liceum przeniesiono z dotychczasowej siedziby w Pałacu Saskim do pałacu Kazimierzowskiego przy Krakowskim Przedmieściu, a Chopinowie zamieszkali w 1817 w prawej oficynie pałacu (tzw. gmachu porektorskim), w jego środkowej części na drugim piętrze, za sąsiadów mając m.in. Lindego, Brodzińskiego i Kolberga z synami.
Hrabia Skarbek w okresie swojego pobytu w Polsce stał się jednym z najaktywniejszych protektorów Chopina, ale to za przyczyną Żywnego, który spisał – według wskazówek Fryderyka – kilka arkuszy z kompozycjami wariacji i tańców, po czym pokazywał je w innych domach, Chopin stał się znany w Warszawie. Fryderyk wystąpił w pałacu Brühla przed księciem Konstantym i zaprezentował mu nieznany marsz, który książę kazał sobie zagrać po raz drugi.
W 1817 w parafialnym zakładzie typograficznym kościoła Nawiedzenia Najświętszej Marii Panny na Nowym Mieście w Warszawie ukazał się pierwszy wydany drukiem utwór Chopina – polonez w tonacji g-moll wydany pod tytułem "Polonoise pour le Piano-Forte Dédiée à Son Excellence Mademoiselle la Comtesse Victoire Skarbek faite par Frédéric Chopin Musicien âgé de huit ans". Z tego samego roku pochodzi wydany pośmiertnie polonez B-dur oraz Marsz wojskowy, którego pierwodruk zaginął. O dedykacji pierwszego wydanego utworu Fryderyka siostrze młodego hrabiego Skarbka może świadczyć to, iż sam Skarbek prawdopodobnie pokrył koszty druku kompozycji (niewiele wcześniej powrócił on ze studiów za granicą oraz objął stanowisko profesora ekonomii politycznej na Uniwersytecie Warszawskim). W styczniu 1822 w tomie X „Pamiętnika Warszawskiego” ukazała się pierwsza dłuższa wzmianka na temat Chopina, opisująca go m.in. jako „prawdziwego geniusza muzycznego”:
Do 1818 Chopin znany był tylko w kręgach akademickich, w których obracała się jego rodzina. Pierwsza recenzja, która ukazała się drukiem, wzbudziła duże zainteresowanie jego osobą. Marsz wojskowy, który tak spodobał się księciu Konstantemu, ukazał się drukiem, choć bezimiennie. Marsz grywany był w czasie ulubionych przez księcia parad wojskowych i kompozycja wykonywana była przez orkiestry wojskowe.
24 lutego 1818 w pałacu Radziwiłłów odbył się pierwszy koncert publiczny Chopina, zorganizowany na rzecz Towarzystwa Dobroczynności przygotowany przez ordynatową Zamoyską. Julian Ursyn Niemcewicz fakt wykorzystania talentu 8-letniego dziecka dla potrzeb filantropii skomentował w komedii, gdzie wyśmiał pogoń za sensacją i gorliwość filantropek, które na fikcyjnym zebraniu licytują się w odejmowaniu lat „Szopenkowi”, a w końcu sztuki zapada uchwała, że cudowne dziecko na scenę wniesie niańka. Sam koncert zgromadził licznych słuchaczy. Chopin zagrał koncert fortepianowy wiedeńskiego kompozytora Adalberta Gyrowetza, przy czym wykazał się dużą wprawą techniczną. W pamiętniku Aleksandry Tarczewskiej, siostry Klementyny z Tańskich Hoffmanowej, widnieje wzmianka na temat jednego z pierwszych występów Chopina poza domem, który miał miejsce podczas wieczoru w salonie Olimpii Grabowskiej w pałacu Radziwiłłów:
Pierwsze sukcesy.
20 września 1818 odwiedziła Warszawę carowa Maria Fiodorowna, której Chopin ofiarował dwa swoje tańce polskie. Wójcicki wspomina, że gdy mały Chopin grywał u księcia Konstantego, namiestnika warszawskiego, podczas gry Marsza wojskowego wznosił oczy w górę, że książę go pytał: „Co tam patrzysz w górę mały? Czytasz nuty na suficie?”.
W 1825 w kościele ewangelickim św. Trójcy odbył się koncert Chopina dla cara Aleksandra I, który przyjechał do Warszawy na obrady Sejmu. Chopin zagrał na niedawno wynalezionym instrumencie – eolomelodikonie. Zachwycony monarcha podarował młodemu muzykowi drogi pierścień z brylantem. Później w Paryżu Chopin otrzymał propozycję zostania nadwornym kompozytorem carskim, jednak odrzucił tę propozycję, a pierścień sprzedał. W Warszawie kilkakrotnie grał w Belwederze, będącym od 1822 siedzibą wielkiego księcia Konstantego.
W latach 1823–1826 Chopin uczył się w Liceum Warszawskim, gdzie pracował jego ojciec. W tych latach zwiedził znaczną część Polski. Z pobytów w Szafarni (lata 1824–1825) wysłał swoje słynne listy – „Kuriery Szafarskie” do rodziców, będące parodią „Kuriera Warszawskiego”, które rozsławiły sielankowe wakacje w majątku Juliusza Dziewanowskiego. Słynna jest również anegdota mówiąca, że został niegdyś w szkole przyłapany na rysowaniu nauczyciela w czasie lekcji. Obrazek tak zadziwił rysowanego, że ten go pochwalił. Maurycy Karasowski wspomina również anegdotę z tradycji rodzinnej, w której Chopin pomógł guwernerowi uspokoić hałaśliwych wychowanków. Zaimprowizował im opowieść, a potem uśpił wszystkich łącznie z guwernerem kołysanką. Gdy pokazał uroczy widok siostrom i matce, obudził wszystkich przeraźliwym akordem. Wraz z siostrą Emilią Fryderyk pisał także dla zabawy wiersze i komedie. Balzac wspominał, że Chopin miał zastraszająco prawdziwy dar naśladowania każdego, kogo tylko zechciał. Wielu biografów (np. Jeżewska, Iwaszkiewicz, Willemetz) przekonuje, że Chopin był geniuszem uniwersalnym, ponieważ posiadał również niezwykły talent literacki, czego dowodem są jego słynne listy, a także talent malarski i aktorski.
W latach 1826–1829 był studentem warszawskiej Szkoły Głównej Muzyki, będącej częścią Konserwatorium, która związana była z Uniwersytetem Warszawskim, gdzie podjął naukę harmonii i kontrapunktu u Józefa Elsnera. Został zwolniony z przedmiotu instrumentu, ponieważ zauważono nieprzeciętny sposób i charakter gry Chopina. Ten okres w jego twórczości charakteryzuje fascynacja muzyką ludową. Powstały wówczas "Sonata c-moll" op. 4, "Wariacje B-dur na temat "Là ci darem la mano" z "Don Juana" W.A. Mozarta" op. 2 na fortepian i orkiestrę, "Trio g-moll" op. 8 i pierwsze "Mazurki" (op. 6, 7) oraz oparte na motywach ludowych "Rondo c-moll" op. 1 i "Rondo à la Krakowiak" F-dur op. 14. W raporcie po trzecim roku nauki Józef Elsner zapisał: „Trzecioletni Szopen Fryderyk – szczególna zdatność, geniusz muzyczny”.
W 1826 odbywał swoją pierwszą zagraniczną podróż do Berlina, a także spędził wakacje w Bad Reinertz. W latach 1827 i 1829 spędzał wakacje na dworze księcia Antoniego Radziwiłła w Antoninie i bywał u przyjaciół mieszkających w odległych miejscach kraju.
Rozpoczęcie niezależnej kariery kompozytorskiej.
Lata 1829–1831 były dla Chopina okresem pierwszej miłości (do śpiewaczki Konstancji Gładkowskiej) i pierwszych ogromnych sukcesów kompozytorskich. W liście do swojego przyjaciela, Tytusa Woyciechowskiego, Chopin nazywał Konstancję „ideałem”. Powstały wówczas "Koncerty fortepianowe f-moll" op. 21 i "e-moll" op. 11.
W lipcu 1829, niezwłocznie po ukończeniu studiów, Chopin wraz z przyjaciółmi wyjechał na wycieczkę do Wiednia. Dzięki Würflowi wszedł w środowisko muzyków. W Theater am Kärntnertor wystąpił dwukrotnie; grał "Wariacje B-dur" za pierwszym, a oprócz "Wariacji" także "Rondo à la Krakowiak" za drugim razem. Odniosły one fenomenalny sukces wśród publiczności. Nawet krytyka mimo zastrzeżeń dotyczących jego gry (zbyt mała siła dźwięku) uznała kompozycje za nowatorskie. Dobre przyjęcie kompozycji na koncertach ułatwiło kontakt z wydawcami: w kwietniu 1830, po raz pierwszy za granicą, wydano drukiem w Austrii, w oficynie Tobiasa Haslingera grane tu już "Wariacje" op. 2.
W lipcu 1830 spędził tydzień w Poturzynie na ziemi chełmskiej u Tytusa Woyciechowskiego. Możliwe, że przywiózł do Poturzyna pierwodruk dedykowanych przyjacielowi Wariacji B-dur, napisanych na mozartowski temat z Don Giovanniego „La ci darem la mano”. Dwukrotnie przyjechał też do pobliskiej Starej Wsi, gdzie grał na fortepianie firmy Pleyel.
2 listopada 1830 wyjechał z Warszawy do Kalisza, żegnany przy rogatkach Wolskich przez przyjaciół i kolegów z klasy kompozycji warszawskiego konserwatorium, którzy odśpiewali mu z akompaniamentem gitary kantatę "Zrodzony w polskiej krainie" ułożoną przez Ludwika Adama Dmuszewskiego do melodii Józefa Elsnera. Przed wyjazdem pożegnał się z Konstancją Gładkowską i wręczył jej pierścień przechowywany przez nią aż do śmierci. W Kaliszu, dokąd dotarł 3 listopada i gdzie dołączył do niego Tytus Woyciechowski, Chopin spędził ostatnie trzy dni w ojczyźnie.
5 listopada 1830 Chopin na zawsze opuścił Polskę: wyjechał z Kalisza i przez Wrocław udał się do Drezna. W Wiedniu usłyszał śpiewaną niegdyś przez Konstancję "cavatinę" Rossiniego i . Pojechał do Monachium i w końcu udał się do Paryża. W czasie drogi Chopin napisał dziennik (zwany „Dziennikiem stuttgarckim”), przedstawiający stan jego ducha podczas pobytu w Stuttgarcie, gdzie ogarnęła go rozpacz z powodu upadku powstania listopadowego. Wedle tradycji, powstały wtedy pierwsze szkice do "Etiudy „Rewolucyjnej”" Utwory tego okresu wypełnione są dramatyzmem, który z wolna zaczyna dominować w twórczości kompozytora.
Lata dojrzałości.
W Paryżu od 5 października 1831 Chopin zamieszkał początkowo w małym mieszkaniu przy Boulevard Poissonnière. 25 lutego 1832 w salonie Pleyel przy 9 rue Cadet dał pierwszy z dziewiętnastu publicznych koncertów w Paryżu (podczas 18 lat pobytu w tym mieście). Organizował go Friedrich Kalkbrenner, pianista. Chopin zagrał "Koncert e-moll" i "Wariacje B-dur". Koncert oszołomił publiczność, w tym obecnego na nim Franciszka Liszta. Krytyk François Fétis zapowiadał, że Chopin odrodzi muzykę fortepianową. Następnego dnia wydawcy przysyłali Chopinowi propozycje kupna utworów. Chopin zaczął prowadzić żywot wirtuoza, komponując utwory, które szybko stawały się modne na salonach. Przyjaźnił się z wieloma wybitnymi muzykami (Liszt, Vincenzo Bellini, Hector Berlioz), był zapraszany na prywatne występy nie jako muzyk, ale jako gość, nawet na sam dwór. Szybko więc przeprowadził się do Chaussée d’Antin, modnej dzielnicy Paryża. Przyjaciele nazywali jego mieszkanie Olimpem ze względu na dającą się stamtąd słyszeć boską muzykę. Jednak z powodu trudnej sytuacji finansowej Chopin zaczął dawać coraz więcej lekcji gry na fortepianie. Oszałamiała ogromna liczba propozycji. Chopin uczył między innymi księżniczkę de Noailles, księżnę de Chimay i de Beauvau, baronową Rothschild, hrabinę Peruzzi i Potocką. Wśród uczniów także wielkie talenty – Karolina Hartmann, Karol Filtsch, a także wierny przyjaciel Chopina Adolf Gutmann. Chopin jako nauczyciel znany był z niezwykłych wymagań i nerwowości.
W latach 1835–1846 porzucił karierę wirtuoza na rzecz komponowania. Zaczął żyć życiem polskiej emigracji, utrzymując ścisłe kontakty z głównymi intelektualistami polskimi (Adam Mickiewicz, Julian Ursyn Niemcewicz, Cyprian Kamil Norwid). Józef Bem poprosił go o zaliczkę dla powstającego Towarzystwa Politechnicznego Polskiego w Paryżu, które kompozytor wsparł finansowo. Chopin gościł też u siebie najbliższego przyjaciela z lat dziecięcych, Jana Matuszyńskiego, który zamieszkując u Fryderyka studiował medycynę w Paryżu (zmarł na gruźlicę w 1842). U Chopina bywał też jego słynny przyjaciel, wielki pianista i kompozytor – Julian Fontana.
W 1836 zaręczył się z Marią Wodzińską. Była ona uzdolniona muzycznie i malarsko. Hrabina Teresa Wodzińska, matka Marii, zaczęła nazywać wtedy Chopina swoim dzieckiem. W podzięce za darowane Chopinowi kompozycje Marii przysyła on Walca As-dur i niektóre ze słynnych Pieśni. Ostatecznie jednak rodzina sprzeciwiła się związkowi, uważając, że Chopin jest zbyt chorowitym kandydatem na męża i zaręczyny zostały zerwane. W 1836 zaczął poważnie chorować na gruźlicę, jednak w późniejszych latach spekulowano, że cierpiał na chorobę genetyczną o zewnętrznych objawach bardzo podobnych do gruźlicy – mukowiscydozę; zobacz hipotezę mukowiscydozy).
W 1837 poznał starszą od siebie o sześć lat i dominującą nad nim powieściopisarkę George Sand, z którą tworzył nerwowy i chaotyczny związek. W dziennikach Sand opowiadała, że przez dziewięć lat "przyjaźni" z Chopinem żyła jak w klasztornym celibacie, a w liście do Justyny Chopinowej nazwała się „drugą matką Chopina”. Uciekając przed zazdrością byłego kochanka George, udali się na Majorkę do Valldemossy, która powitała ich okropną pogodą. Jednocześnie w Hiszpanii trwała wojna, a choroba Chopina wzmagała się. Sand zabrała swoje dzieci, które miały się tam leczyć (syn Maurycy chorował na reumatyzm) i uczyć. Mieszkali tam w byłym klasztorze. Chopin skomponował wtedy "Preludia" op. 28, "Preludium Des-dur", które Sand uznała za straszne. Wspominała ona pamiętny wieczór, gdy podczas spaceru rozszalała się burza, a po powrocie do domu Chopin zaczął pluć krwią. Gdy stan zdrowia Chopina poprawił się, Chopin i Sand wrócili do Francji. Sprowadzono wtedy z Neapolu ciało samobójcy, wielkiego śpiewaka, Adolfa Nourrita. Chopin zagrał w Marsylii podczas pogrzebu na organach. Pomimo iż instrument był rozstrojony, Chopin oszołomił publiczność graną na organach pieśnią Schuberta.
Wiosną lub latem 1838 E. Delacroix zaczyna malować portret podwójny (niedokończony) F. Chopina i G. Sand. W pracowni Delacroix troje przyjaciół spędza wiele czasu na rozmowach; Chopin sprowadza specjalnie wypożyczony fortepian, aby móc grać w pracowni malarza.
W 1839 wrócili do Francji. Chopin był wówczas bardzo chory, kaszląc, wypluwał duże ilości krwi. Mieszkał w Nohant, w posiadłości George Sand. Chwilowa poprawa zdrowia wniosła okres spokoju do życia przyjaciół, wkrótce jednak nastąpiło ponowne pogorszenie. Sand napisała kontrowersyjną książkę "Lukrecja Floriani", w której – według Liszta – ośmieszyła Fryderyka. Chopin przemilczał tę obrazę, jednak jego rodzina w Polsce była oburzona. Sand tłumaczyła, że nie opisała w niej Fryderyka, jednak biografowie Chopina podważają te tłumaczenia. Pojawił się wtedy znany rzeźbiarz Auguste Clésinger, który poślubił Solange – córkę Sand – wbrew woli matki. Gdy w wynikłym konflikcie Fryderyk stanął po stronie Solange, pisarka wpadła w szał i zerwała z kompozytorem. Okres przed rozstaniem z Sand, a także po rozpadzie związku oraz pogłębiająca się choroba, odcisnęły głębokie piętno na twórczości i życiu towarzyskim Chopina. Powstały wówczas najpiękniejsze z jego nokturnów i mazurków.
Ostatni okres życia.
Po rozstaniu z George Sand Chopin popadł w głębokie przygnębienie, które z pewnością przyspieszyło jego śmierć. Po opuszczeniu Nohant nie skomponował już żadnego znaczącego utworu, jedynie kilka miniatur. Po wybuchu rewolucji w Paryżu w 1848 r. Chopin wyjechał do Anglii i Szkocji na bardzo wyczerpującą jego siły podróż. 16 listopada 1848 w sali Guildhall w Londynie odbył się jego ostatni publiczny koncert. Organizatorką i sponsorką pobytu była jego uczennica, Szkotka Jane Stirling, zwana „wdową po Chopinie”. Kobieta obdarzyła kompozytora miłością, zaproponowała mu nawet małżeństwo. Ten jednak czuł się zbyt chory, ponadto nie odwzajemniał jej uczucia. Z listów Chopina wiemy, że czułość i opiekuńczość Jane działały mu na nerwy.
Śmierć i pogrzeb.
Po powrocie ze Zjednoczonego Królestwa Wielkiej Brytanii i Irlandii do Paryża stan zdrowia Fryderyka nie polepszył się. 3 września 1848 roku zmarł homeopata Jean-Jacques Molin (1797–1848), jeden z niewielu lekarzy, którzy potrafili pomóc artyście („Molin posiadał sekret stawiania mnie na nogi.”). Doktorzy, którzy pojawili się po Molinie, byli zgodni co do „dobrego klimatu, spokoju, odpoczynku”, jednak Fryderyk stwierdził, iż odpoczynek znajdzie „pewnego dnia i bez ich pomocy”. Zalecona dieta, która zabraniała m.in. pić kawę zamieniając ją na kakao, denerwowała Fryderyka, który czuł się przez to śpiący oraz „coraz bardziej tępy”. Sam Chopin wierzył w poprawę swojego zdrowia pisząc do Solange Clésinger:
Sytuacja w Paryżu była wówczas niespokojna. Krótki czas po wyjeździe Fryderyka na Wyspy Brytyjskie miało miejsce powstanie robotników paryskich będące konsekwencją rewolucji lutowej. Artysta wrócił do stolicy Francji krótko przed wyborem Ludwika Napoleona Bonaparte na prezydenta II Republiki Francuskiej. Chopin napisał:
Fryderyk coraz rzadziej udzielał lekcji. Niektórzy z jego uczniów wyjechali, a on sam ze względów zdrowotnych postanowił ograniczyć lekcje tylko do uczniów bardziej zaawansowanych. Wśród nich byli m.in.: Catherine Soutzo, Maria Kalergis, Delfina Potocka, Charlotte de Rothschild oraz (od marca) Marcelina Czartoryska. Pierwszą połowę 1849 Chopin spędził w mieszkaniu na Square d’Orléans. Odwiedzali go Franchomme, Ernest Legouvé, Natalia Obrieskow, Nathaniel Stockhausen, Thomas Albrecht, Charles Gavard i Marie de Rozières, a przede wszystkim Delacroix. Mieszkanie Chopina było wtedy miejscem wieczornych spotkań, na których Delfina Potocka śpiewała, a Chopin i Kalergis grali na fortepianie. Z początkiem wiosny Chopin zaczął wybierać się na przejażdżki kabrioletem, w których towarzyszył mu Delacroix. Rozmawiali wtedy głównie o muzyce. Chopin przedstawił malarzowi własną koncepcję logiki w dziele muzycznym, wyjaśniając również dlaczego jego zdaniem Mozart góruje technicznie nad Beethovenem. Za namową Delacroixa Fryderyk zaczął spisywać swoje uwagi teoretyczne na temat metody gry fortepianowej, które z powodu nietypowego wykształcenia muzycznego pianisty przedstawiają jednak niewielką wartość. Z tego okresu pochodzą mazurki: nr 2 z op. 67 i nr 4 z op. 68. Stan zdrowia artysty pogarszał się. Viardot zauważyła, iż podlegał on ogromnym wahaniom:
Lekarze, którzy zajmowali się Chopinem, stwierdzili, że powinien on opuścić Paryż na okres lata, w którym upał i kurz utrudniałyby mu oddychanie. Innymi argumentami za wyjazdem ze stolicy Francji była szerząca się epidemia cholery oraz zbliżające się wybory, które mogły wywołać zamieszki. Przyjaciele znaleźli dla artysty mieszkanie w Chaillot, gdzie wprowadził się na początku czerwca (obecnie znajduje się tam, położony nad Sekwaną na wzgórzu Trocadero naprzeciwko wieży Eiffla, pałac Chaillot). Mieszkanie było drogie, dlatego w tajemnicy przed Chopinem połowę czynszu opłacała Obrieskow. Pięć okien, jakie znajdowały się w apartamencie dawało widok obejmujący Tuileries, Izbę Deputowanych, wieżę kościoła St-Germain l’Auxerrois, Notre-Dame, Panteon, St-Sulpice oraz kopułę kościoła Inwalidów. Chopin czuł się tak dobrze, że przestał nawet brać lekarstwa. Ofiarami panującej epidemii padli m.in. Friedrich Kalkbrenner oraz Angelica Catalani. Strach przed chorobą spowodował, iż większość znajomych Chopina opuściła Paryż. Przy artyście pozostała m.in. Stirling. Jej częste odwiedziny denerwowały Chopina („One mnie zaduszą nudami”). W obawie przed pozostawieniem Chopina samego w nocy, rodzina Czartoryskich przysłała muzykowi jedną ze swoich nianiek. W ostatnim tygodniu czerwca Chopinowi zaczęły puchnąć nogi, miał on również kilka poważnych krwotoków. Zaniepokojona niańka poinformowała o tym księżną Sapieżynę, która postanowiła wezwać Jeana Cruveilhiera. Lekarz rozpoznał ostatnie stadium choroby. Z leków jakie przepisał Chopinowi artysta zrozumiał, że umiera. Spowodowało to starania o przyjazd do Paryża jego siostry Ludwiki:
Świadek ostatnich tygodni życia i męki Chopina – Cyprian Kamil Norwid zamieścił nekrolog w „Dzienniku Polskim”:
Chopin zmarł w otoczeniu kilkorga bliskich mu osób 6 około 2 w nocy 17 października 1849, a na świadectwie zgonu jako przyczynę lekarz wpisał gruźlicę.
Msza żałobna odbyła się dopiero 30 października w paryskim kościele św. Magdaleny, podczas której grał tamtejszy organista Louis James Alfred Lefébure-Wély, a solowe partie basowe wykonanego na życzenie Chopina "Requiem" Mozarta zaśpiewał Luigi Lablache. Liczny kondukt żałobny, któremu przewodniczył książę Adam Jerzy Czartoryski, przeszedł na Cmentarz Père-Lachaise, gdzie zmarłego artystę pożegnały dźwięki „Requiem” Mozarta oraz Marsza żałobnego jego własnego autorstwa.
Auguste Clésinger wykonał pośmiertny odlew twarzy i dłoni artysty, a następnie zaprojektował też i wykonał jego nagrobek, składający się z cokołu z popiersiem kompozytora, który jest zwieńczony rzeźbą zadumanej Euterpe. Grób kompozytora znajduje się w 11 sekcji cmentarza, linia 1, kwatera Y20, między grobami François-Antoine’a Habenecka i Josepha Lakanala. Dokładnie naprzeciwko grobu Chopina znajduje się skromny grób pary mało znanych francusko-niemieckich poetów Yvan Goll i Claire Goll. Koszty pogrzebu i pomnika nagrobnego pokryła Jane Stirling. Ona też opłaciła powrót do Warszawy siostry Chopina Ludwiki, która przewiozła do kraju serce kompozytora.
Losy serca Chopina.
Umierający Chopin poprosił, by po śmierci otwarto jego ciało i wyjęte z niego serce przesłano do Warszawy. Sekcję zwłok przeprowadził w dzień po jego śmierci anatomopatolog i chirurg Jean Cruveilhier w Instytucie Medycznym w Paryżu. Protokół z sekcji nie zachował się.
Słój z sercem kompozytora zatopionym w alkoholu (prawdopodobnie w koniaku lub spirytusie) został przewieziony potajemnie do Polski w styczniu 1850 przez jego starszą siostrę Ludwikę Jędrzejewiczową. W Warszawie przechowywała go przez jakiś czas w swym mieszkaniu znajdującym się w nieistniejącej obecnie kamienicy przy ul. Podwale. Później (dokładna data nie jest znana) serce zostało powierzone opiece księżom misjonarzom w kościele Świętego Krzyża, który był kościołem parafialnym rodziny Chopinów. Na początku było przechowywane w dolnym kościele. W 1880 serce zostało umieszczone w lewym filarze nawy głównej świątyni. Umieszczono na nim zaprojektowane przez Leonarda Marconiego epitafium w formie tablicy z marmuru karraryjskiego. Na tablicy widnieje napis "Fryderykowi Chopinowi, rodacy". Nad nim cytat z Ewangelii św. Mateusza: "Gdzie skarb twój, tam serce twoje". Epitafium zostało ufundowane z dochodów koncertu charytatywnego zorganizowanego przez kompozytora Władysława Żeleńskiego, prezesa Towarzystwa Muzycznego w Warszawie. Tablicę odsłonięto 5 marca 1880.
W czasie powstania warszawskiego w okolicach kościoła toczyły się ciężkie walki. Kapelan wojsk niemieckich pastor Schulz namówił księży, aby przekazali urnę Niemcom w celu jej uchronienia przed zniszczeniem. 9 września 1944 Niemcy przekazali urnę z sercem biskupowi Antoniemu Szlagowskiemu, filmując to zdarzenie dla celów propagandowych.
Urna z sercem Chopina została przewieziona do Milanówka. Stała na pianinie w salonie-kaplicy, na I piętrze plebanii. 17 października 1945, w rocznicę śmierci Chopina, proboszcz parafii św. Krzyża w Warszawie ks. Leopold Petrzyk, kompozytor prof. Bolesław Woytowicz i muzykolog Bronisław Sydow zawieźli urnę do Żelazowej Woli. Tam oczekiwali na nią m.in. przedstawiciele najwyższych władz państwowych z Bolesławem Bierutem. Urnę ustawiono na postumencie na tle portretu kompozytora. Po przemówieniu Bolesław Bierut przekazał urnę prezydentowi Warszawy Stanisławowi Tołwińskiemu. Po odegraniu przez Henryka Sztompkę nokturnu, dwóch mazurków i poloneza, urnę z sercem przejęła z kolei młodzież. W drodze do kościoła św. Krzyża samochód zatrzymał się jeszcze przy dawnej rogatce wolskiej, gdzie w 1830 Fryderyk Chopin żegnał się z Warszawą.
Po uroczystej mszy w zniszczonej świątyni, przed wmurowaniem urny, kazanie wygłosił Hieronim Feicht.
Wieczorem tego dnia w budynku „Romy” przy ul. Nowogrodzkiej 49 odbyła się uroczysta akademia, na której grano utwory Chopina.
W 2014 zespół naukowców pod kierunkiem prof. dra hab. Michała Witta, kierownika Zakładu Genetyki Molekularnej i Klinicznej Instytutu Genetyki Człowieka Polskiej Akademii Nauk w Poznaniu, przeprowadził analizę wyników oględzin słoja z sercem kompozytora, z których dowiedzieć się można, że bezpośrednią przyczyną jego śmierci było zapalenie osierdzia jako ciężkie powikłanie gruźlicy.
Hipotezy i wątpliwości.
Mukowiscydoza.
Zdaniem niektórych lekarzy kompozytor mógł cierpieć przez całe życie na mukowiscydozę i to właśnie ona była przyczyną jego śmierci. Inne teorie mówią o niedoborze alfa1-antytrypsyny.
Data urodzenia.
Data urodzenia Chopina budzi wątpliwości. W księdze urodzeń oraz księdze chrztów w kościele parafialnym w Brochowie podawany jest dzień 22 lutego: 
 Data ta jest jednak kwestionowana. Fryderyk, jak i jego rodzina, za poprawny dzień zawsze podawali 1 marca – taką datę podał Fryderyk, gdy w styczniu 1833 został członkiem Towarzystwa Historyczno-Literackiego w Paryżu. Kontrowersje dotyczą także daty rocznej – nie ma pewności, czy Fryderyk urodził się w 1810 czy 1809 roku.
Twórczość.
Okresy twórczości.
Do 1830 (wczesne utwory).
Pierwszy młodzieńczy okres twórczości Chopina ukształtował się pod wpływem polskiej tradycji dworskiej i wiejskiej (np. Michał Kleofas Ogiński, Karol Kurpiński), z drugiej strony na europejskim stylu "brillant" wczesnych romantyków (Johann Nepomuk Hummel, John Field, Carl Maria von Weber). Takimi popisowymi, a zarazem błyskotliwymi wirtuozowsko utworami są oba koncerty fortepianowe. W młodzieńczym dorobku Chopina obecne są formy klasyczne, takie jak rondo, wariacje, sonata, koncert, trio. Formy poloneza i mazurka nawiązują do tradycji narodowej i ludowej. Z łączenia form klasycznych i tradycji narodowej powstawały takie utwory jak "Rondo à la Mazur", "Rondo à la Krakowiak".
Do 1839 (dojrzałe utwory).
Drugi dojrzały okres twórczości tak charakteryzuje Józef M. Chomiński:
1840–1849 (późny okres).
Ostatni trzeci etap pracy kompozytorskiej Chopina cechują rozbudowane utwory cykliczne (Sonaty b-moll, h-moll, g-moll). Powiększeniu uległy rozmiary utworów jednoczęściowych (Ballada f-moll, Fantazja f-moll, polonezy fis-moll i As-dur, Polonez-Fantazja, Barkarola). Utwory te często budziły protest pierwszych słuchaczy, uważane za „trudne”, a zwłaszcza – zanadto dysonujące.
Gatunki.
Wczesne polonezy, nawiązujące do popularnych wówczas w Warszawie utworów Ogińskiego, wydane zostały dopiero po śmierci kompozytora (9 utworów dziecięcych i młodzieńczych „op. posth.” = wydane pośmiertnie). W latach dojrzałych Chopin opublikował siedem polonezów, które są już zupełnie inne: bardzo dramatyczne i rozbudowane.
Chopin skomponował 57 mazurków, w nieznany wcześniej sposób nawiązując w nich do muzyki ludowej z Mazowsza.
Chopin napisał 21 nokturnów. Są to liryczne, melodyjne miniatury. Najwcześniejsze są jeszcze dość sentymentalne, późniejsze są wyrazowo coraz bardziej urozmaicone. Na ich melodię szczególnie wpłynęło bel canto, gdyż Chopin był wielbicielem włoskiej opery, zwłaszcza Belliniego. Kantylenę oplata w nich kunsztowna ornamentacja.
Zobacz też: Scherza Fryderyka Chopina
Chopin napisał w późniejszych latach 4 Scherza. Wbrew tytułowi, są one raczej poważne, a nawet dramatyczne. Najpoważniejsze jest Scherzo h-moll op. 20, napisane podobno w okresie powstania listopadowego, z cytatem kolędy "Lulajże, Jezuniu" w części środkowej.
Mówi się, że 4 ballady Chopina: g-moll op. 23, F-dur op. 38, As-dur op. 47 (jedyna z „optymistycznym” zakończeniem) i f-moll op. 52 powstały pod wrażeniem lektury ballad Adama Mickiewicza, lecz nie znaleziono na to dowodów. Ewentualne pokrewieństwo z poezją 
Ukończone zostały dwa cykle etiud: op. 10 i op. 25. Zgodnie z nazwą (fr. "étude" = studium, ćwiczenie) są one utworami pedagogicznymi i mają służyć doskonaleniu techniki pianistycznej. Ale po raz pierwszy w historii tego gatunku nie są to „nudne ćwiczenia”, którymi zamęczano pokolenia początkujących pianistów. Każda etiuda Chopinowska to arcydzieło. Jedną z najbardziej znanych jest, zamykająca opus 10, etiuda c-moll, zwana „rewolucyjną”.
24 Preludia op. 28 powstały jako symboliczny hołd złożony Janowi Sebastianowi Bachowi, którego muzykę Chopin cenił. Są wyraźnym nawiązaniem do "Das Wohltemperierte Klavier", napisanym również we wszystkich 24 tonacjach dur i moll. Zgodnie z zasadą gatunku są krótkie (najdłuższe As-dur nr 17 ma 90 taktów, a najkrótsze E-dur nr 9 ma tylko 12). Harmoniczny porządek cyklu pozwala wykonywać go w całości, lecz sam Chopin grywał tylko po kilka miniatur.
Chopin napisał 3 sonaty na fortepian: Sonatę c-moll op. 4, Sonatę b-moll op. 35 i Sonatę h-moll op. 58 oraz Sonatę g-moll op. 65 na fortepian i wiolonczelę. Spośród 3 sonat fortepianowych najpopularniejsza jest Sonata b-moll, której trzecia część, Marsz żałobny, grywany jest dzisiaj podczas pogrzebów, w rozmaitych transkrypcjach.
Powstały dwa koncerty na fortepian i orkiestrę: f-moll op. 21 z 1829 r. i e-moll op. 11 z 1830 r. – oba z pierwszego okresu twórczości, pisane jeszcze w Polsce, osadzone w formie klasycznej.
Chopin skomponował 19 pieśni na głos z fortepianem, Trio fortepianowe oraz Sonatę wiolonczelową.
Światopogląd i preferencje polityczne.
Chopin przez całe życie był mocno wierzącym katolikiem. Daleki od dewocji, uważał wiarę za kwestię intymną. Znał świetnie na pamięć "Pismo Święte" i potrafił je cytować. Choć zupełnie wyjątkowo zdarzały mu się obrazoburcze sformułowania (np. o Bogu-Moskalu), to generalnie pozostawał zdecydowanie w nurcie religijnej ortodoksji. Przed śmiercią wyspowiadał się, komunikował i błogosławił obecnych, błagając Boga o łaski dla nich.
W kwestiach społecznych Chopin pozostawał umiarkowanym konserwatystą. Nudziły go teorie egalitarystyczne, był przeciwnikiem socjalizmu oraz bez złudzeń oceniał milenarystyczne utopie socjalne. Nie cenił liberalizmu i miał skłonności monarchistyczno-legitymistyczne. Sam twierdził, że „kocham karlistów, nie cierpię filipistów”. Z drugiej strony, w stosunku do salonów arystokratycznych i plutokratycznych, zwłaszcza francuskich, odczuwał niechęć, a nawet pogardę.
Chopin uważał się za Polaka, a za swój kraj ojczysty uznawał Polskę; emocjonalnie był szczegónie związany z Mazowszem. Nie miał antypatii wśród innych narodów, choć zdarzało mu się odnosić z niechęcią np. do Anglików czy do Żydów; szczególnie lubił i cenił Czechów. Nie był zwolennikiem zrywów powstańczych, które uważał za skazane na niepowodzenie; na tym tle doszło np. do ochłodzenia jego stosunków z Mochnackim. Upadek powstań z lat 1830–1831 i 1848 przeżywał jednak bardzo mocno.
Pozostałe informacje.
Józef Sikorski, rówieśnik kompozytora, pisał w swoim „Wspomnieniu Szopena”, że Chopin jako dziecko płakał, gdy matka grała na fortepianie.
W okresie II wojny światowej Niemcy początkowo zabronili wykonywania utworów Chopina, jednakże w późniejszym okresie zaczęli lansować tezę o jego rzekomym niemieckim pochodzeniu (rodzina Chopina pochodziła z pogranicza francusko-niemieckiego, z niezależnego księstwa Lotaryngii wcielonego do Francji dopiero w 1766 r., w którym ludność miejscowa posługiwała się dialektem romańskim, a dziadek Chopina – Franciszek pisał się także z niemiecka – Shopin, jednak Franciszek osiadł tam w pierwszym pokoleniu, a jego rodzina pochodziła w rzeczywistości z Alp na pograniczu szwajcarsko-niemieckim). 27 października 1943 roku w Krakowie Hans Frank otworzył wystawę poświęconą Chopinowi, mającą stanowić zaczątek wielkiego muzeum kompozytora. Ponadto Frank nabył we Francji maskę pośmiertną Chopina, fortepian marki Pleyel, na którym Chopin uczył zakochaną w nim szkocką arystokratkę Jane Stirling (obecnie w Collegium Maius UJ), a także bibliotekę nauczyciela Chopina Józefa Elsnera, autografy utworów i korespondencję kompozytora pochodzące z kolekcji francuskiego miłośnika Chopina Édouarda Ganche.
Znaczenie twórczości Chopina.
W kulturze polskiej.
Chopin do dziś uważany jest za największego kompozytora polskiego, ale rola jego muzyki – acz nawiązywali do niej prawie wszyscy polscy kompozytorzy przez wiele następnych pokoleń – zdecydowanie wykraczała poza samą muzykę. Patriotyczną wymowę jego dzieł odczytywali nie tylko Polacy. Robert Schumann w 1836 roku na łamach „Neue Zeitschrift für Musik” napisał:
Jego utwory, zwłaszcza polonezy i mazurki, traktowane były jak synonim polskości, odwoływano się do niej w momentach o specjalnej wymowie patriotycznej. Ignacy Jan Paderewski powiedział w setną rocznicę urodzin Chopina:
Chopin pozostaje do dzisiaj jednym z najbardziej znanych na świecie Polaków.
W muzyce europejskiej.
Oryginalna harmonia Chopina wywarła pewien wpływ na Richarda Wagnera, a jego miniatury dodatkowo inspirowały Roberta Schumanna i Franciszka Liszta. Przede wszystkim jednak był on właściwie pierwszym wybitnym twórcą tzw. szkół narodowych w muzyce romantycznej. Przykład Chopina zachęcił Edvarda Griega do podobnego wykorzystywania norweskiego folkloru. Z entuzjazmem wypowiadali się na temat muzyki Chopina w Rosji kompozytorzy „Potężnej Gromadki”, we Francji był on wzorem dla takich kompozytorów jak Gabriel Fauré, Claude Debussy i Maurice Ravel, a nawet młody Olivier Messiaen, a w Rosji – Aleksandr Skriabin i Siergiej Rachmaninow. Zdaniem niemieckiego muzykologa Alfreda Einsteina oddziaływanie Chopina było wyjątkowe:
Międzynarodowy Konkurs Pianistyczny im. Fryderyka Chopina.
Od 1927 w Warszawie odbywa się Międzynarodowy Konkurs Pianistyczny im. Fryderyka Chopina, najstarszy na świecie monograficzny konkurs muzyczny. Jego twórcą był Jerzy Żurawlew. Po II wojnie światowej, w latach 1949–2005, konkurs był organizowany przez Towarzystwo im. Fryderyka Chopina w Warszawie. Obecnie jego organizatorem jest Narodowy Instytut Fryderyka Chopina.
Festiwale muzyki Chopina odbywają się także w Dusznikach, Gandawie, Valldemossie, Genewie, Paryżu i innych.
W latach 50. i 60. XX w. dokonano nagrań dzieł wszystkich Chopina z udziałem najlepszych polskich pianistów. Redaktorem naczelnym wydania był Jan Ekier.
Ochrona prawna.
Zgodnie z art. 1 ust. 1 ustawy z dnia 3 lutego 2001 r. o ochronie dziedzictwa Fryderyka Chopina jego utwory i przedmioty z nim związane stanowią dobro ogólnonarodowe podlegające szczególnej ochronie. Ustawa dotyczy wykorzystania wizerunku i nazwiska Chopina w znakach towarowych, natomiast nie dotyczy jego twórczości, która znajduje się w domenie publicznej.
Ochroną dziedzictwa kompozytora zajmuje się Narodowy Instytut Fryderyka Chopina (NIFC), przy wsparciu Urzędu Patentowego RP. Podmioty chcące zarejestrować znak towarowy zawierający nazwisko lub wizerunek Fryderyka Chopina muszą uzyskać wcześniejszą akceptację NIFC. Instytut wymaga, aby produkty opatrzone wizerunkiem lub nazwiskiem kompozytora były wysokiej jakości i kojarzyły się z Polską. Za wyrażenie zgody na używanie takiego znaku towarowego w celach komercyjnych Instytut pobiera opłatę roczną oraz procent od uzyskanych zysków.
Fryderyk Chopin w kulturze masowej.
Nagrody Akademii Fonograficznej – „Fryderyki”.
Nawiązujące do imienia Chopina Nagrody Akademii Fonograficznej – „Fryderyki”, które zaczęto przyznawać w 1995, miały być polskim odpowiednikiem nagrody Grammy. Zamiar ten jednak się nie powiódł, bo w związku z zapaścią w polskiej branży fonograficznej stacje telewizyjne wycofały się z transmitowania gali, na której je wręczano. W 2006 po raz pierwszy nie odbyła się ceremonia wręczenia tych nagród. Listę ich zdobywców ogłoszono jedynie w internecie.
Piosenka Gazebo.
W 1983 roku na listy przebojów w całej Europie wspięła się piosenka "I like Chopin" włoskiego piosenkarza Paula Mazzoliniego (Gazebo). Chociaż Mazzolini śpiewał o muzyce Chopina, fortepianowy motyw, który przewija się w piosence, nie jest kompozycją Polaka. Przebój Gazebo nadal jest nadawany przez komercyjne rozgłośnie, choć najczęściej jako cover.
Początek piosenki miał następujące słowa po angielsku:
"Remember that piano / So delightful unusual / That classic sensation / Sentimental confusion"
"Used to say / I like Chopin / Love me now and again"
Eternal Sonata.
Japońską fascynację muzyką Chopina oraz sztuką anime łączy gra "Eternal Sonata" na konsole Xbox 360 i PlayStation 3, wydana w Japonii w czerwcu 2007. Bohaterem gry jest Fryderyk Chopin, który na trzy godziny przed śmiercią trafia do bajkowej krainy, tam wraz z przyjaciółmi szuka leku na swoją chorobę. Fabularnej grze towarzyszy muzyka Chopina w wykonaniu rosyjskiego pianisty Stanisława Bunina.
Literatura.
Twórczość Fryderyka Chopina znajdowała odniesienie w twórczości literackiej. Ukazały się opracowania tzw. literackich "chopinianów", które opracowali m.in. Bronisław Edward Sydow ("Chopin w literaturze: Poezja. Proza. Scena i film", 1949) i Edmund Słuszkiewicz ("Wiersze o Chopinie. Antologia i bibliografia", 1964).
Filmy o życiu kompozytora.
U kresu II Rzeczypospolitej w 1939 był planowany film opowiadający o życiu Chopina. W późniejszym czasie powstało wiele filmów o życiu kompozytora, w wielu z nich wiodącym wątkiem jest związek kompozytora z George Sand. Niektóre z filmów o Chopinie to:
Filatelistyka.
Poczta Polska wyemitowała 26 znaczków poświęconych pamięci kompozytora:

</doc>
<doc id="1521" url="https://pl.wikipedia.org/wiki?curid=1521" title="Fryderyk Szopen">
Fryderyk Szopen



</doc>
<doc id="1522" url="https://pl.wikipedia.org/wiki?curid=1522" title="Faza gazowa">
Faza gazowa



</doc>
<doc id="1523" url="https://pl.wikipedia.org/wiki?curid=1523" title="Faza ciekła">
Faza ciekła



</doc>
<doc id="1524" url="https://pl.wikipedia.org/wiki?curid=1524" title="Ciało krystaliczne">
Ciało krystaliczne

Ciało krystaliczne – ciało stałe, w którym cząsteczki (w kryształach molekularnych), atomy (w kryształach kowalencyjnych) lub jony (w kryształach jonowych) są ułożone w uporządkowany schemat powtarzający się we wszystkich trzech wymiarach przestrzennych. W objętości ciała cząsteczki zajmują ściśle określone miejsca, zwane węzłami sieci krystalicznej i mogą jedynie drgać wokół tych położeń.
Każdy kryształ zbudowany jest z wielu powtarzających się komórek elementarnych. W zależności od ich rodzaju kryształy tworzą różne układy krystalograficzne. Określenie „ciało krystaliczne” odnosi się do ciał o dwojakiej budowie:

</doc>
<doc id="1525" url="https://pl.wikipedia.org/wiki?curid=1525" title="Flauta">
Flauta

Flauta (sztil, cisza morska) – określenie żeglarskie oznaczające brak wiatru. Pogoda bezwietrzna, prędkość wiatru 0,2 m/s (&lt; 1 km/h), 0° w skali Beauforta). Lustrzana tafla wody.
Etymologia.
Według "Słownika wyrazów obcych" Władysława Kopalińskiego słowo flauta pochodzi z niemieckiego (cisza morska), wywodzącego się od (słaby; ospały).

</doc>
<doc id="1526" url="https://pl.wikipedia.org/wiki?curid=1526" title="Francis Drake">
Francis Drake

Francis Drake, (ur. ok. 1540 w Crowndale koło Tavistock, zm. 28 stycznia 1596) – angielski korsarz, w latach 1577–1580 odbył wyprawę dookoła świata.
Życiorys.
Młodość.
Data jego urodzin nie jest dokładnie znana, możliwe, że urodził się już w 1535, przyjmowany jest też 1540. Był najstarszym z dwunastu synów fanatycznego protestanta, który na początku lat 50. XVI wieku przeniósł się do Kentu. Tam jego liczna i uboga rodzina żyła w skrajnej nędzy na rozpadającym się, przeciekającym wraku statku. Statek ten był pierwszym domem, jaki Drake zapamiętał.
W dwunastym roku życia został chłopcem okrętowym. Wykształcenie uzyskał dzięki pomocy dalekiego krewnego, magnata i znanego żeglarza sir Johna Hawkinsa, w którego wyprawach Drake brał udział we wczesnej młodości. W wieku 18 lat poprowadził po raz pierwszy samodzielnie statek, odbywając rejs z Anglii do Zatoki Biskajskiej. W roku 1567 dowodził statkiem „Judith”, który wraz z innymi jednostkami Hawkinsa dokonywał napadów na Hiszpanów koło wybrzeży Ameryki.
Wyprawy korsarskie.
W II połowie XVI wieku, gdy Anglia walczyła o pokonanie monopolu Hiszpanii i Portugalii w handlu zamorskim i zdobyczach kolonialnych, Drake wsławił się złupieniem wielu miast Nowego Świata oraz zdobyciem wielu statków portugalskich i hiszpańskich. Około 1563 Drake po raz pierwszy pożeglował na hiszpańskie wybrzeża Morza Karaibskiego, skąd wypływały hiszpańskie statki ze srebrem. Hiszpanie uważali go za zwykłego pirata, lecz dla Anglii był przede wszystkim wielkim żeglarzem i korsarzem. Bitwa stoczona z siłami hiszpańskimi podczas jego drugiej wyprawy zaskarbiła mu sympatię królowej Elżbiety. Najbardziej znanym dokonaniem Drake’a z tego okresu było zdobycie hiszpańskiej karawany mułów, transportujących srebro z Panamy do Nombre de Dios, w marcu 1573 roku.
Drake był typem żeglarza-nawigatora, człowiekiem dbającym o precyzyjne wykonywanie map odkrytych lądów, był również piratem-gentlemanem – jeńcom okazywał często wyszukaną gościnność.
Wyprawa dookoła świata.
W 1577 na polecenie królowej Elżbiety Drake podjął wyprawę przeciw hiszpańskim posiadłościom na zachodnim wybrzeżu Ameryki. Wyprawa ta przekształciła się w wyprawę dookoła świata – drugą po wyprawie Magellana. Drake był jednak pierwszym kapitanem, który opłynął kulę ziemską, gdyż Magellan zmarł podczas swojej wyprawy. Drake wypłynął w grudniu 1577 z Plymouth w Anglii z ponad 150 ludźmi załogi i pięcioma statkami:
Po przepłynięciu Atlantyku dwa statki musiały zostać porzucone na wschodnim wybrzeżu Ameryki Południowej. Trzy pozostałe przepłynęły Cieśniną Magellana na Pacyfik. Sztormy zniszczyły jeden statek i spowodowały konieczność zawrócenia drugiego. Drake popłynął dalej na północ, wzdłuż zachodniego wybrzeża Ameryki Południowej na ostatnim statku „Golden Hind” („Złota Łania”). Odebrał Hiszpanom wyłączność na obecność na Pacyfiku (do tej pory wozili złoto na nieuzbrojonych statkach), łupiąc po drodze wszystko, co tylko się dało, zdobywając hiszpańskie statki i atakując porty. Poszukując przejścia północnego na Atlantyk, dopłynął aż do obecnej granicy USA z Kanadą. Nie znajdując oczekiwanego połączenia, skierował się na południe, obejmując oficjalnie (aczkolwiek czysto teoretycznie) w posiadanie królowej angielskiej odkrytą przez siebie Kalifornię (jako Nowy Albion).
Po naprawach statku Drake wyruszył na zachód przez Pacyfik, dopływając po kilku miesiącach do Moluków. Płynąc dalej na zachód z przystankami po drodze, następnie opływając Afrykę, dotarł z powrotem do Anglii we wrześniu 1580, przywożąc ze sobą ładunek przypraw korzennych i hiszpańskie skarby. Wyprawa okazała się olbrzymim sukcesem finansowym.
Szlachcic i admirał.
W 1581 został podniesiony przez królową Elżbietę do godności szlachcica (w uznaniu jego zasług nie stało się to na dworze królowej, a na pokładzie jego statku „Golden Hind”), został również admirałem. Zdobył wielki majątek, przysparzając równie wielkich dochodów Koronie. Początkowo krewni, arystokracja oraz sama królowa potajemnie finansowali wyprawy Drake’a i jemu podobnych, czerpiąc z nich nielegalne i niemałe zyski. Z czasem Francis Drake dostał licencję królowej na swą działalność piracką, stając się korsarzem, a w swoich dalekich morskich podróżach oprócz dokonywania grabieży wypełniał również w imieniu Korony misje dyplomatyczne. W 1586 roku przywiózł z wysp Morza Karaibskiego do Anglii pierwszy ładunek tytoniu.
Był równie dobry w samotnych wyprawach pirackich, jak i w dowodzeniu wielkimi flotami inwazyjnymi z dobrze prowadzonymi operacjami wojsk lądowych. Zawierał doraźne sojusze w wyprawach łupieżczych zarówno z kapitanami statków innych bander, jak i z partyzantką murzyńską w Ameryce Łacińskiej. W 1586 roku zdobył San Domingo, a w roku 1587 zniszczył flotę hiszpańską w Kadyksie, opóźniając w ten sposób przygotowania Hiszpanii do inwazji na Anglię. W 1588 jako wiceadmirał floty angielskiej (pod rozkazami lorda Charlesa Howarda of Effingham) wziął udział w rozgromieniu na kanale La Manche Wielkiej Armady – floty inwazyjnej hiszpańskiego króla Filipa. Napadał także na porty w Hiszpanii, Portugalii i na Azorach – jednak z czasem popadł w niełaskę królowej, oskarżany o zbytnie sprzyjanie prywatnym interesom podczas oficjalnych działań floty Korony – pomimo że nieraz był głównodowodzącym tych sił.
Ostatnie lata.
Jego ostatnie wyprawy wojenno-rabunkowe do Hiszpanii i na Karaiby na czele połączonych flot królowej i inwestorów prywatnych zakończyły się klęskami. Zmarł na morzu na dyzenterię 28 stycznia 1596 roku w czasie kolejnej wyprawy pirackiej do Indii Zachodnich, złożony postępującą chorobą, febrą i załamany brakiem powodzenia w ostatnich wyprawach. Jego ciało wrzucono w ołowianej trumnie do Morza Karaibskiego. Umarł jednak jako człowiek bardzo bogaty, szanowany i otoczony legendarnym podziwem zarówno wśród swoich zwolenników, jak i przeciwników. Był największym żeglarzem swoich czasów.
Upamiętnienie.
Jego imieniem została nazwana cieśnina między Ameryką Południową i Antarktydą.
Pomnik Drake’a, który od 1853 stał w niemieckim Offenburgu, przedstawiał wielkiego korsarza z kwiatkiem w ręku. Był to kwiat ziemniaka. Napis na postumencie głosił „Sir Francisowi Drake’owi, który rozpowszechnił ziemniaki w Europie. Miliony rolników całego świata błogosławią jego nieśmiertelną pamięć. To ulga dla biedaków, bezcenny dar Boży, łagodzący okrutną nędzę”. Pomnik został zniszczony przez nazistów.
Angielski poeta Alfred Noyes napisał epos "Drake. An English Epic".

</doc>
<doc id="1527" url="https://pl.wikipedia.org/wiki?curid=1527" title="Jean-François de La Pérouse">
Jean-François de La Pérouse

Jean-François de La Pérouse, właściwie Jean-François de Galaup comte de La Pérouse (ur. 23 sierpnia 1741 w okolicach Albi, Francja, zm. w 1788 w czasie wyprawy odkrywczej na Pacyfiku) – francuski oficer marynarki, żeglarz, odkrywca i naukowiec.
Życiorys.
W trakcie swej kariery wojskowej wyróżnił się m.in. zniszczeniem fortów Kompanii Zatoki Hudsona podczas wojny z Wielką Brytanią w latach 1778–1783.
Przeszedł do historii jako dowódca francuskiej wyprawy odkrywczo-geograficznej, wysłanej przez króla Francji Ludwika XVI pod koniec XVIII wieku na Pacyfik z poleceniem zbadania nieznanych dotąd rejonów tego oceanu. Wyprawa La Pérouse’a zasługuje na wyróżnienie z powodu wyjątkowo dobrego przygotowania. Zalecenia króla oraz m.in. Akademii Morskiej w Breście (która dorównywała wtedy Akademii Królewskiej w Anglii) złożyły się łącznie na bardzo obszerną dokumentację, w której można było już znaleźć wyraźnego ducha myśli epoki oświecenia. Na wyprawę przygotowano duże i nowoczesne statki „La Boussole” i „L’Astrolabe”, wyposażono je w najnowocześniejszy sprzęt badawczy i nawigacyjny, a na pokładach znalazło się wielu uczonych różnych dyscyplin oraz zespół grafików i malarzy. Misja nie miała zadania dokonania podbojów, oprócz badań nastawiona była na nawiązanie kontaktów (także handlowych) z tubylcami – stąd w ładowniach okrętów znalazło się wiele europejskich roślin, których uprawy zamierzano ich nauczyć.
Wyprawa wyruszyła w sierpniu 1785 r. i była tak dobrze przygotowana, że przebiegała niemal w komfortowych warunkach. Na Pacyfik wpłynięto od strony Przylądka Horn. Zasadniczym celem wyprawy było rozpoznanie północnej części Pacyfiku na potrzeby handlu Francji z Chinami, toteż bardzo dokładnie zbadano hydrografię i opisano linię brzegową zachodniego wybrzeża Ameryki Północnej oraz nie spenetrowanych do tej pory przez Europejczyków obszarów Morza Japońskiego. Wyprawa była nawet goszczona przez gubernatora Pietropawłowska Kamczackiego. W ciągu trzech lat wyprawa przemierzyła bez mała cały obszar Oceanu Spokojnego, badając geografię i przyrodę archipelagów w północnej, środkowej i południowej jego części, jak również wybrzeża Chile, Filipin, Formozy i Makau.
W lutym 1788 roku u wrót Zatoki Botanicznej na wschodnim wybrzeżu Australii doszło do spotkania wyprawy z pierwszym transportem osadników i skazańców z Anglii. To spotkanie z Anglikami jest ostatnią sprawdzoną informacją o wyprawie La Pérouse’a. Do roku 1829 nie było wiadomo, co stało się z oboma statkami i ich załogami.
Pierwsze wieści przywiózł do Francji ze swej wyprawy na Pacyfik Jules Dumont d’Urville, który w 1828 r. odnalazł w rejonie wyspy Vanikoro szczątki fregaty „L’Astrolabe”. W 1959 r. francusko-belgijska wyprawa kierowana przez znanego wulkanologa Harouna Tazieffa odkryła przypadkiem na Vanikoro groby członków załogi okrętu „La Boussole”. Jednocześnie w wyniku poszukiwań podwodnych płetwonurkowie wydobyli przedmioty stanowiące wyposażenie okrętu (m.in. kotwice, armaty i astrolabium). W 1964 r. ekspedycja francuskiej marynarki wojennej na poławiaczu min „Dunkerquoise” odnalazła na głębokości 50-60 m wrak fregaty „La Boussole”, z którego wydobyto m.in. dzwon alarmowy.
Późniejsze badania archeologiczne szczątków wraków i legendy przekazywane w pieśniach tubylców dają dzisiaj dość pewny obraz tego, że wyprawa rozbiła się na rafach atolu Vanikoro w grupie Wysp Santa Cruz, a cała załoga została zjedzona przez tubylców. Ostateczną identyfikacją było wydobycie ze szczątków wraku, w maju 2005, między wieloma innymi przedmiotami, sekstansu firmy „Mercier” o którym istnieją zapisy w zachowanych we Francji dokumentach wyprawy.
Na cześć La Pérouse’a została nazwana jedna z cieśnin łączących Morze Japońskie z Pacyfikiem (Cieśnina La Pérouse’a).

</doc>
<doc id="1528" url="https://pl.wikipedia.org/wiki?curid=1528" title="Fotoskład">
Fotoskład

Fotoskład – sposób i miejsce przygotowania materiałów do druku, chronologicznie umiejscowione pomiędzy tradycyjną zecernią (patrz: zecerstwo) a współczesnym studiem DTP. Dziś jest to tradycyjna, wychodząca z użycia nazwa studia DTP.
Fotoskład był miejscem, gdzie materiały na potrzeby drukarni (tekst i obrazy) były formowane w obrazy kolumn metodą układania zadrukowanych kartek i przeźroczy na stole podświetlanym, gdzie następnie reprodukowano wszystko metodą fotograficzną. Klisze były przekazywane do drukarni w celu kopiowania na docelowe formy drukowe – blachy offsetowe.
Pierwotnie fotoskład dotyczył składu samego tekstu na urządzeniu zwanym fotoskładarką, potem inną metodą utrwalano obrazy całych złamanych kolumn, by wreszcie w czasach nam współczesnych naświetlać kolumny na naświetlarkach postscriptowych. Niemniej w każdym przypadku fotoskład był utrwaleniem informacji na światłoczułej kliszy.

</doc>
<doc id="1529" url="https://pl.wikipedia.org/wiki?curid=1529" title="Fermentacja">
Fermentacja

Fermentacja – enzymatyczny proces przemian związków organicznych w warunkach beztlenowych, prowadzony przez mikroorganizmy, np. bakterie lub drożdże, w efekcie którego następuje dysproporcjonowanie substratu, tj. jego równoczesne utlenienie i redukcja, np.:
Procesowi temu towarzyszy wydzielenie energii, która mazgazynowana jest zazwyczaj pod postacią ATP, powstającego w wyniku fosforylacji substratowej, lub, wyjątkowo, w wyniku działania pomp transbłonowych: wodorowej lub sodowej. Fermentacja umożliwia uzyskanie energii użytecznej metabolicznie organizmom stale lub okresowo żyjącym w warunkach beztlenowych. Wydajność energetyczna fermentacji jest znacznie niższa niż procesów tlenowych. Z jednej cząsteczki glukozy uzyskiwane są 2 (czasem 1–2,5) cząsteczki ATP, podczas gdy w cyklu Krebsa z jednej cząsteczki glukozy uzyskiwane są 32 (lub 38) cząsteczki ATP.
Innym rodzajem beztlenowego uzyskiwania energii jest oddychanie beztlenowe, które różni się od fermentacji tym, że zachodzi z udziałem łańcucha transportu elektronów, który nie jest wykorzystywany w procesach fermentacyjnych.
Przykłady procesów fermentacyjnych:
Nauka badająca procesy fermentacji to zymologia.
W biotechnologii określenie „fermentacja” jest szersze i dotyczy zwyczajowo także niektórych procesów tlenowych z udziałem bakterii. Przykładami są fermentacja octowa i fermentacja cytrynowa.

</doc>
<doc id="1530" url="https://pl.wikipedia.org/wiki?curid=1530" title="Fulereny">
Fulereny

Fulereny, fullereny – cząsteczki składające się z parzystej liczby atomów węgla, tworzące zamkniętą, pustą w środku bryłę geometryczną. Cząsteczki fulerenów zawierają od 28 do około 1500 atomów węgla.
Właściwości chemiczne fulerenów są zbliżone pod wieloma względami do węglowodorów aromatycznych. Fuleren , czyli buckminsterfulleren, podobnie jak inne fulereny, jest odmianą alotropową węgla.
Etymologia nazwy i historia odkrycia.
Za inicjatora tego odkrycia uważa się Harolda Kroto z Uniwersytetu Sussex (w południowej Anglii), który – badając metodami spektroskopowymi w ramach pracy doktorskiej przemiany związków węgla zachodzące w okolicach wygasłych gwiazd – odkrył charakterystyczne wąskie linie spektralne, które odpowiadały aromatycznym związkom węgla.
Mniej więcej w tym samym czasie zespół naukowy z Uniwersytetu Rice’a w Houston (w Teksasie), w skład którego wchodzili James R. Heath, Sean O’Brien, Robert Curl i Richard Smalley, opracował zestaw do syntezy związków organicznych poprzez naświetlanie promieniem lasera obracającej się tarczy grafitowej. Otrzymano w tych warunkach szereg bardzo nietypowych związków o budowie klatkowej. Wzbudziło to zainteresowanie Harolda Kroto, który zauważył, że warunki panujące podczas tych syntez są bardzo podobne do warunków, jakie panują w gwiazdach. Nasunęło to myśl, by wykorzystać to urządzenie do syntezy pochodnych węgla.
Harold Kroto dołączył do tego zespołu w 1985 w ramach stażu podoktorskiego. Wspólnie z Richardem Smalleyem podjęli się badań nad otrzymaniem związków węgla o dużej masie cząsteczkowej. Już pierwszego dnia odkryto tajemniczy związek o masie cząsteczkowej 720 u, który występował w większym stężeniu niż inne. Dokładne przemyślenia doprowadziły ich do struktury „piłki futbolowej”. Następnie na drodze obliczeń kwantowo-mechanicznych dowiedli, że związek taki powinien generować dokładnie jedną linię w widmie 13C NMR, ściśle odpowiadającą widmu związku uzyskanego przez Harolda Kroto i zespołu z Uniwersytetu Rice’a.
Za odkrycie fulerenów Harold Kroto z Uniwersytetu Sussex w Brighton (Wielka Brytania) oraz zespół R.E. Smalley i R.F. Curl jr. z Uniwersytetu Rice’a w 1996 otrzymali Nagrodę Nobla w dziedzinie chemii. Harold Kroto kontynuował badania nad fulerenami na Uniwersytecie Sussex, m.in. wyodrębniając je w bardzo żmudny sposób z sadzy i rozpoczął badania ich własności chemicznych. W 1990 niemieccy badacze W. Kratschmar i D. Huffman opublikowali względnie tanią i wydajną metodę syntezy fulerenów poprzez kontrolowane węgla w łuku elektrycznym w atmosferze helu, która otworzyła drogę do praktycznego zastosowania tych związków, lecz nie zostali uwzględnieni w nagrodzie Nobla.
Zgodnie z opowieściami Harolda Kroto, widok kopuły geodezyjnej skonstruowanej z pięcio- i sześciokątów, którą widział podczas Światowej Wystawy '67 w Montrealu, zainspirował ich obu do wspólnego skonstruowania pierwszego modelu fulerenu . Model taki dla klasterów węglowych zaproponował już w 1970 Eiji Osawa. Inni członkowie zespołu Smalleya zaczęli konstruować podobne modele kolejnych fulerenów sferycznych, a także zauważyli, że można na ich bazie konstruować rurki.
Nazwa „fuleren” pochodzi od nazwiska amerykańskiego architekta, Buckminstera Fullera, który wymyślił pokrycia hal w postaci tak zwanych kopuł geodezyjnych, opartych o kratownice pokryte płytami w kształcie wielokątów foremnych. Na tej konstrukcji oparty był również, zatwierdzony przez FIFA i używany przez 36 lat (1970–2006), wzór piłki nożnej (Buckminster Ball).
Na cześć konstruktora kopuły w Dallas, zaczęli oni między sobą nazywać w żartach tego rodzaju związki „Bucky balls” (czyli w wolnym tłumaczeniu „jaja Buckiego” lub „piłki Buckiego”), co zostało w pierwszej publikacji przerobione na bardziej poważnie brzmiącą nazwę „Buckminster fulleren”, z której to nazwy wywiedziona została nazwa dla całej klasy tego rodzaju związków.
Budowa fulerenów.
Powierzchnia fulerenów składa się z układu sprzężonych pierścieni składających się z pięciu i sześciu atomów węgla. Najpopularniejszy fuleren, zawierający 60 atomów węgla (tzw. ), ma kształt dwudziestościanu ściętego. Natomiast zawiera dodatkowy pierścień atomów węgla.
Szczególnymi izomerami strukturalnymi fulerenów są nanorurki, będące długimi walcami uzyskanymi ze zwinięcia pojedynczej płaszczyzny grafitowej, domknięte z obu stron połówkami fulerenów odpowiedniej wielkości. Najkrótszą nanorurką, z formalnego punktu widzenia, jest , najdłuższe zaś (w 2008) mają ponad dwa centymetry długości.
Do rodziny fulerenów zalicza się:
Właściwości.
Fulereny są czarnymi ciałami stałymi o metalicznym połysku. Mają własności nadprzewodzące i półprzewodnikowe. Ich własności chemiczne są zbliżone do sprzężonych węglowodorów aromatycznych, choć reakcje z ich udziałem wymagają zwykle drastyczniejszych warunków. Ulegają, między innymi, reakcji Friedla-Craftsa (addycji). Gęstość wynosi 1,65 g/cm³
Fulereny należą do związków słabo rozpuszczalnych. Nie rozpuszczają się w polarnych rozpuszczalnikach praktycznie wcale. Najlepiej (choć też nie za dobrze) rozpuszczają się w rozpuszczalnikach aromatycznych (benzen, toluen) oraz w czterochlorku węgla. Tworzą się wtedy kolorowe roztwory. Roztwór w benzenie ma barwę fioletową, zaś – rubinową.
Na początku wydawało się, że są one tylko kolejną „ciekawostką przyrodniczą”, w toku badań okazało się jednak, że mogą znaleźć wiele praktycznych zastosowań. Można je przyłączać do polimerów, uzyskując w ten sposób środki smarujące i tworzywa o unikatowych własnościach elektrooptycznych. Można je funkcjonalizować na powierzchni i łączyć razem, otrzymując układy katalityczne o bardzo rozwiniętej powierzchni.
Wewnątrz fulerenów można zamykać atomy praktycznie wszystkich pierwiastków, a także odpowiednio małe cząsteczki związków chemicznych.
Modyfikowane fulereny dzieli się na:
Otrzymywanie.
Fulereny otrzymuje się poprzez bombardowanie promieniem laserowym obracającej się tarczy grafitowej w supersonicznym strumieniu helu. Obecnie najbardziej popularną i wydajną metodą otrzymywania fulerenów jest metoda płomieniowa. Polega ona na spalaniu substancji organicznych (najczęściej jest to toluen). Dzięki tej metodzie produkcja fulerenów na świecie wynosi obecnie kilkanaście ton. W wyniku tego procesu otrzymywana jest sadza fulerenowa, będącą mieszanką wielu fulerenów. W celu oczyszczenia i rozdzielenia stosuje się wieloetapową ekstrakcję, najczęściej benzenem lub toluenem. Separacja poszczególnych typów fulerenów następuje za pomocą wysokosprawnej chromatografii cieczowej.
Zastosowanie.
Ze względu na swoje właściwości, fulereny stosowane są w technice biomedycznej, optycznej oraz elektronicznej. Fulereny wchodzą też w skład katalizatorów oraz innych urządzeń przemysłu chemicznego.
Występowanie naturalne.
Fulereny występują w niewielkich ilościach w sadzy węglowej. Znajdowane są również w niektórych ziemskich skałach, np. szungicie znajdowanym w Rosji.
Fulereny zostały także wykryte w przestrzeni kosmicznej, w 2010, przy użyciu Kosmicznego Teleskopu Spitzera. Cząsteczki występują jako gaz w przestrzeni międzygwiezdnej i w mgławicach planetarnych pozostałych po śmierci gwiazd. W 2012 teleskopem tym wykryto fulereny także w postaci ciała stałego, czyli fulerytu, w pobliżu gwiazdy podwójnej XX Ophiuchi.

</doc>
<doc id="1531" url="https://pl.wikipedia.org/wiki?curid=1531" title="Fraktal">
Fraktal

Fraktal (łac. "fractus" – złamany, cząstkowy, ułamkowy) w znaczeniu potocznym oznacza zwykle obiekt samopodobny (tzn. taki, którego części są podobne do całości) albo „nieskończenie złożony” (ukazujący coraz bardziej złożone detale w dowolnie wielkim powiększeniu). Ze względu na olbrzymią różnorodność przykładów matematycy obecnie unikają podawania ścisłej definicji i proponują określać fraktal jako zbiór, który posiada wszystkie poniższe charakterystyki albo przynajmniej ich większość:
Na przykład linia prosta na płaszczyźnie jest formalnie samopodobna, ale brak jej pozostałych cech i zwyczajowo nie uważa się jej za fraktal. Z drugiej strony, zbiór Mandelbrota ma wymiar Hausdorffa równy 2, taki sam jak jego wymiar topologiczny. Jednak pozostałe cechy wskazują, że jest to fraktal. Wiele fraktali ma niecałkowity wymiar Hausdorffa, co wyjaśnia etymologię tej nazwy.
Historia.
Pojęcie fraktala zostało wprowadzone do matematyki przez Benoît Mandelbrota w latach 70. XX wieku. Odkryty przez niego zbiór Mandelbrota nie był jednak pierwszym przykładem fraktala. Wcześniej istniała już cała gama zbiorów o niecałkowitym wymiarze Hausdorffa, postrzeganych jednak głównie jako kontrprzykłady pewnych twierdzeń. Bardziej systematycznie fraktalami zajmowała się geometryczna teoria miary, mająca swoje początki w pracach Constantina Carathéodory’ego i Felixa Hausdorffa.
Szczególnymi fraktalami – nie nazywając ich po imieniu – zajmowali się Georg Cantor, Giuseppe Peano, Wacław Sierpiński, Paul Lévy, a także Donald Knuth. Szczególny wkład w rozwój geometrycznej teorii miary wniósł Abraham Bezikowicz, który skonstruował również wiele konkretnych fraktali o paradoksalnych własnościach. Również zbiór Julii, ściśle związany ze zbiorem Mandelbrota, był badany w latach 20. zeszłego wieku. Mandelbrot, używając komputera do wizualizacji, uczynił z fraktali przedmiot intensywnych badań. O ważności tego zagadnienia zadecydowały zastosowania w różnych dziedzinach, zwłaszcza poza matematyką, np. obecnie prawie każdy telefon komórkowy korzysta z wbudowanej anteny fraktalnej. Liczne odpowiedniki fraktali istnieją też w naturze.
Właściwości.
Za jedną z cech charakterystycznych fraktala uważa się samopodobieństwo, to znaczy podobieństwo całości do jego części. Co więcej, zbiory fraktalne mogą być samoafiniczne, tj. część zbioru może być obrazem całości przez pewne przekształcenie afiniczne. Dla figur samopodobnych można określić wielkość zwaną wymiarem samopodobieństwa lub wymiarem pudełkowym. Są to wielkości będące uogólnieniem klasycznych definicji wymiaru.
Wiadomo, że stosunek pól płaskich (wymiaru 2) figur podobnych równa się kwadratowi skali ich podobieństwa. Na przykład figura podobna do innej w skali 3 ma dziewięć razy większe pole od tamtej (formula_1 albo formula_2). W przestrzeni stosunek objętości brył (trójwymiarowych) podobnych jest sześcianem skali ich podobieństwa; bryła podobna do innej w skali 2 ma osiem razy większą objętość od tamtej (formula_3 albo formula_4). Wymiar samopodobieństwa figury daje się zatem określić jako logarytm o podstawie równej skali podobieństwa i liczbie logarytmowej wskazującej, ile razy większa od figury wyjściowej (jaką częścią figury wyjściowej) jest figura podobna do niej w tej skali. Dla fraktali liczba ta może nie być całkowita.
Na przykład zbiór Cantora jest podobny do swoich dwu części w skali 3; wymiar Hausdorffa zbioru Cantora wynosi formula_5 Analogicznie trójkąt Sierpińskiego jest podobny do swoich trzech części w skali 2, a jego wymiar Hausdorffa jest równy formula_6 Dywan Sierpińskiego jest podobny do swoich ośmiu części w skali 3, zatem jego wymiar Hausdorffa to formula_7
Ogólniej, jeżeli fraktal składa się z formula_8 części, które łączą się między sobą na obszarze miary Lebesgue’a zero i są podobne w skali formula_9 do całego fraktala, to wymiar Hausdorffa fraktala będzie równy formula_10 Jeszcze ogólniej, jeśli założymy, że każda część jest podobna do całości w innej skali formula_11 to wymiar Hausdorffa jest rozwiązaniem poniższego równania z niewiadomą formula_12
Niektóre fraktale są zbiorami o mierze Lebesgue’a równej zero. Dotyczy to fraktali klasycznych, np. trójkąt Sierpińskiego i zbiór Cantora mają miarę Lebesgue’a równą zero. Ogólnie każdy fraktal, dla którego wymiar Hausdorffa jest ostro większy od wymiaru topologicznego, będzie mieć tę własność. Z kolei zbiór Mandelbrota i niektóre zbiory Julii mają dodatnie miary Lebesgue’a (na przykład miara Lebesgue’a zbioru Mandelbrota wynosi ok. 1,5).
Generowanie fraktali.
Atraktory IFS.
Najprostszą metodą tworzenia fraktali jest wykorzystanie zbioru przekształceń afinicznych formula_14 będących przekształceniami zwężającymi (kontrakcjami). Transformując dowolny, niepusty zbiór formula_15 zgodnie z regułą (tworząc ciąg zbiorów):
W granicy otrzymujemy:
atraktor układu, który w szczególności może być fraktalem. Zbiór formula_14 nazywamy w tym przypadku systemem przekształceń iterowanych (IFS), zaś otrzymany w powyższej granicy fraktal jest atraktorem tego systemu. Jego istnienie wynika z twierdzenia Banacha o punkcie stałym odwzorowania zwężającego. W ten sposób można wygenerować m.in. następujące fraktale: zbiór Cantora, krzywa Kocha, smok Heighwaya, trójkąt Sierpińskiego, kostka Mengera i paproć Barnsleya.
W praktyce aby wygenerować fraktal stosuje się algorytm iteracji losowej zwany grą w chaos. Polega on na tym, że wybieramy dowolny punkt formula_20 i transformujemy go "wiele" razy, za każdym razem losując odpowiednio przekształcenie formula_21
Procedurę tę powtarzamy np. "kilka tysięcy" razy. W szczególnych przypadkach dla efektu wizualnego może być istotny sposób losowania przekształceń. Np. dla paproci Barnsleya przekształcenia formula_23 (zob. definicję) losuje się z częstościami 85%, 7%, 7%, 1% odpowiednio.
Zbiory Julii i Mandelbrota.
Zbiory takie jak zbiór Mandelbrota, zbiór Julii czy „płonący statek” są podzbiorami płaszczyzny zespolonej. Dla każdego punktu formula_24 określa się pewien ciąg formula_25 Od zbieżności tego ciągu zależy, czy punkt należy do zbioru (fraktala). Ciąg określa się wzorem rekurencyjnym:
Od postaci funkcji formula_28 i formula_29 zależy rodzaj fraktala.
Za punkty należące do danego zbioru uznaje się te, dla których:
Przykłady
W praktyce liczenie ogranicza się do kilkudziesięciu iteracji lub do momentu, gdy formula_36 Uzyskiwane kolory w obrazach fraktali (zwłaszcza zbiorów Julii) realizuje się np. zliczając, jak szybko poszczególne punkty rozbiegają się do nieskończoności i przydzielając im w zależności od tego różne barwy.
W przyrodzie.
Struktury o budowie fraktalnej są powszechnie spotykane w przyrodzie. Przykładem mogą być krystaliczne dendryty (np. płatki śniegu), system naczyń krwionośnych, systemy wodne rzek, błyskawice lub kwiaty kalafiora.
Przykłady.
„Klasycznymi fraktalami”, badanymi (czasem długo) przed powstaniem samego pojęcia fraktala, są m.in.:
Inne ważne przykłady:
Fraktale w grafice komputerowej.
Istnieje wiele programów przeznaczonych do tworzenia obrazów fraktalnych, np. Fractint, Ultra Fractal, XenoDream, Tierazon, FractalExplorer, Apophysis, Sterling, QuaSZ, XaoS i Gimp.

</doc>
<doc id="1532" url="https://pl.wikipedia.org/wiki?curid=1532" title="Fizyka">
Fizyka

Fizyka (z , "physis" – „natura”) – nauka przyrodnicza, zajmująca się badaniem najbardziej fundamentalnych i uniwersalnych właściwości oraz przemian materii i energii, a także oddziaływań między nimi. Do opisu zjawisk fizycznych fizycy używają wielkości fizycznych, wyrażonych za pomocą pojęć matematycznych, takich jak liczba, wektor i tensor. Tworząc hipotezy i teorie fizyki, budują relacje pomiędzy wielkościami fizycznymi.
Z fizyką ściśle wiążą się inne nauki przyrodnicze, szczególnie chemia. Chemicy przyjmują teorie fizyki dotyczące cząsteczek i związków chemicznych (mechanika kwantowa, termodynamika) i za ich pomocą tworzą teorie w ich własnych dziedzinach badań. Fizyka zajmuje szczególne miejsce w naukach przyrodniczych, ponieważ wyjaśnia podstawowe zależności obowiązujące w przyrodzie.
Historia fizyki.
Chwila, od której człowiek zaczął interesować się poznawaniem przyrody, jest trudna do określenia. Najdawniejsze ślady kultur sprzed 5000 lat znalezione w dolinach Nilu, Eufratu i Tygrysu świadczą o prymitywnych próbach wykorzystania natury. Jednak z czasem na podstawie obserwacji ludzkość posiadła sztukę wytwarzania narzędzi, uprawy pól, wytopu metali i sztukę liczenia. Poprzez obserwację powtarzalności zjawisk stworzono kalendarz.
Za pierwsze odkryte prawo fizyki można uznać prawo odbicia światła znane już Euklidesowi w IV w. p.n.e. Pierwszym znanym fizykiem we współczesnym znaczeniu tego słowa był Archimedes z Syrakuz, który w III w. p.n.e. sformułował m.in. prawo dźwigni oraz prawo wyporu. Jednak aż do XIX w. optykę geometryczną oraz mechanikę, w tym statykę i hydrostatykę, zaliczano do matematyki stosowanej, a nie do fizyki.
W starożytności fizyka była traktowana jako część filozofii. Arystoteles dokonał podziału filozofii na fizykę – dział traktujący o zjawiskach przyrodniczych i metafizykę (ontologię oraz epistemologię, czyli nauki dotyczące samej istoty bytu i możliwości jego poznania) oraz etykę i logikę. Fizyka aż do XVI w. była uprawiana, podobnie jak pozostałe działy filozofii, głównie poprzez rozważania teoretyczne. Prace doświadczalne z optyki i z magnetyzmu pojawiały się już w średniowieczu od XIII w. (Witelon, Roger Bacon, Petrus Peregrinus). Jednak dopiero od czasów nowożytnych i XVI w. (Francis Bacon, Galileusz) zaczęła wzrastać rola pomiaru i doświadczenia. Reliktem pozostałym po filozoficznej genezie fizyki jest termin "filozofia naturalna" w języku angielskim, będący długo synonimem fizyki (w Oksfordzie nadawało się stopnie naukowe nie z fizyki, tylko z filozofii naturalnej).
Obecny zakres zainteresowania fizyki ukształtował się w XIX i na początku XX wieku, również wówczas zarysował się podstawowy podział fizyki na klasyczne działy: mechanikę, optykę, termodynamikę, elektryczność i magnetyzm. Fizyka, odkrywając nowe zjawiska, opisując je, tworząc teorie pozwalające przewidywać nowe efekty, stała się motorem napędowym gwałtownego rozwoju techniki i doprowadziła do rewolucyjnych zmian cywilizacyjnych.
Działy fizyki.
Fizyka eksperymentalna a teoretyczna.
Kultura badań fizycznych różni się od innych nauk tym, że istnieje w niej fundamentalny i powszechnie uznawany podział na teorię i eksperyment. Od początku XX wieku większość fizyków pozostaje specjalistami albo w fizyce teoretycznej, albo w fizyce doświadczalnej. Mało fizyków odnosi sukcesy w obu rodzajach badań. Dla porównania, większość wybitnych teoretyków chemii i biologii z powodzeniem pracuje też eksperymentalnie.
Praca fizyków-teoretyków polega na rozwijaniu teorii, za pomocą których można opisać i interpretować wyniki doświadczeń oraz możliwie dokładnie przewidzieć wyniki przyszłych doświadczeń. Z drugiej strony, fizycy doświadczalni wykonują eksperymenty, żeby zbadać nowe zjawiska i sprawdzić przewidywania teoretyczne. Ważną częścią pracy fizyka doświadczalnego jest też często budowanie własnej aparatury, szczególnie w pionierskich gałęziach fizyki, gdzie potrzebny sprzęt jest niedostępny. Mimo że działania teoretyków wydają się czasem oderwane od prac fizyków doświadczalnych, są w istocie ze sobą ściśle powiązane i od siebie zależne. Postęp w fizyce teoretycznej często zaczyna się od doświadczeń, których stara teoria nie potrafi wyjaśnić – i na odwrót, nowatorskie przewidywania teoretyczne stwarzają potrzebę przeprowadzenia nowych doświadczeń, a czasem również nowych technik doświadczalnych. Każdy fakt doświadczalny wymaga uzasadnienia teoretycznego, tak jak każda teoria musi być potwierdzona doświadczalnie, by stać się paradygmatem. Dlatego np. M-teoria pozostaje tylko spekulacją, ponieważ nie dość, że nie potwierdzono jej eksperymentalnie, to nawet nie wymyślono jeszcze żadnego testu eksperymentalnego, który mógłby ją potwierdzić.
Centralnym elementem eksperymentu jest pomiar dobrze określonej wielkości fizycznej, a warunkiem niezbędnym uzyskania z niego wartościowych informacji – prawidłowy dobór przyrządów pomiarowych oraz metod analizy otrzymanych danych. Obróbka danych często opiera się na statystyce, regułach prawdopodobieństwa oraz odpowiednich metodach numerycznych.
Podobnie fizyka teoretyczna ma własny zestaw metod naukowych, które pozwalają stworzyć adekwatne modele i paradygmaty. Opracowane teorie zazwyczaj korzystają z różnych metod matematyki, analitycznych i syntetycznych. Kluczową rolę w rozważaniach teoretycznych odgrywają hipotezy i proces dedukcji.
Główne teorie.
W fizyce część teorii jest uznana przez wszystkich fizyków. Każdą z tych teorii uważa się za fundamentalnie prawdziwą w określonym dla niej zakresie. Na przykład mechanika klasyczna precyzyjnie opisuje ruch ciał pod warunkiem, że są one dużo większe od atomów i poruszają się z prędkościami dużo mniejszymi niż prędkość światła w próżni. Niektóre teorie są nadal obszarami badań – zaskakujący aspekt mechaniki klasycznej znany jako chaos przebadano w XX wieku, trzysta lat po jego sformułowaniu przez Newtona, wprowadzając mechanikę statystyczną.
Działy szczegółowe fizyki.
Współczesne badania fizyczne można podzielić na kilka wyraźnych działów, które zajmują się różnymi aspektami świata materialnego. Fizyka fazy skondensowanej dotyczy własności materii i jej związków z własnościami i oddziaływaniami atomów, z których się składa. Fizyka atomów, cząsteczek i zjawisk optycznych opisuje pojedyncze atomy i cząsteczki oraz ich oddziaływania ze światłem. Fizyka cząstek elementarnych (znana też jako fizyka wysokich energii) z kolei bada cząstki submikroskopowe mniejsze od atomów i poszukuje elementarnych cząstek budujących wszystkie inne jednostki materii. Astrofizyka wykorzystuje prawa fizyki, żeby tłumaczyć zjawiska astronomiczne, na przykład zjawiska związane ze Słońcem, Układem Słonecznym oraz Wszechświatem jako całością.
Działy fizyki są ze sobą ściśle powiązane i zasięg stosowania teorii i modeli często wykracza poza prosty podział zaprezentowany powyżej. Przykładowo fizyka materii skondensowanej zajmująca się układami silnie skorelowanych fermionów jest stosowana do efektów obserwowanych w gwiazdach neutronowych, które są podstawową domeną astronomii. Wynika to stąd, że fizyka jako nauka jest spójna i poszczególne modele i teorie opracowywane w poszczególnych działach mają te same podstawy oraz mogą mieć zastosowanie w innych działach. Podstawowe teorie, takie jak mechanika kwantowa, kwantowa teoria pola, elektrodynamika kwantowa, teoria grawitacji, są sformułowane w sposób ogólny i obowiązują w całej fizyce.
Działy interdyscyplinarne i pokrewne.
Wiele badań łączy fizykę z innym dziedzinami nauki. Dla przykładu, szeroki zakres biofizyki obejmuje wszystkie zagadnienia dotyczące układów biologicznych, w których stosuje się zasady fizyki. W chemii kwantowej z kolei opisuje się i przewiduje zachowania atomów i molekuł na podstawie teorii mechaniki kwantowej.
Ważne prawa.
Dobrze sprecyzowane i powszechnie przyjęte teorie są przedstawiane jako prawa fizyki. Chociaż wszystkie naukowe teorie są w zasadzie tymczasowe i obowiązują tylko w pewnym zakresie, "prawa fizyczne" zostały wielokrotnie sprawdzone, a ich zakres stosowalności jest dobrze określony.
Ważne równania.
Wiele praw fizycznych może być opisana za pomocą relacji odpowiednich wielkości. Zapis matematyczny takich relacji nazywa się równaniem. Jest wiele fundamentalnych równań fizyki opisujących zjawiska, którymi zajmują się poszczególne jej działy. Kilka przykładowych ważnych równań to:

</doc>
<doc id="1533" url="https://pl.wikipedia.org/wiki?curid=1533" title="FreeBSD">
FreeBSD

FreeBSD – system operacyjny z rodziny Unix. Oparty na BSD, gałęzi Uniksa stworzonej przez Computer Systems Research Group (CSRG) na Uniwersytecie Kalifornijskim w Berkeley. 
Pierwsza wersja wydana została 30 listopada 1993. Najnowszą jest wydana w maju 2022 wersja 13.1.
Model dystrybucji i rozwoju.
FreeBSD jest wolnodostępnym i darmowym systemem operacyjnym dostępnym z pełnym kodem źródłowym. Źródła całego systemu – nie tylko jądra – wraz z historią (istnieje możliwość pobrania najstarszych wersji) dostępne są w repozytorium git (do 2021 roku projekt korzystał ze scentralizowanych repozytoriów Subversion a wcześniej do 2008 z CVS). System rozprowadzany na liberalnej, 2-klauzulowej licencji BSD, która nie tylko pozwala na wykorzystanie kodu, jego modyfikowanie i dalszą dystrybucję zmian, ale także na jego integrację do projektów zamkniętych, co czyni ją przyjazną dla przedsięwzięć komercyjnych. Poza własnym kodem na licencji BSD projekt integruje szereg rozwiązań rozprowadzanych na innych licencjach (np. kod systemu plików ZFS na licencji CDDL, zestaw LLVM/Clang na licencji Apache v2, GPLv2). 
Nad rozwojem projektu czuwa demokratycznie wybierana grupa programistów – tzw. "Core Team", który nadzoruje rozwój projektu, a także rozsądza sporne kwestie wśród oficjalnych programistów posiadających prawo do bezpośredniej modyfikacji źródeł (tzw. "committers").
Historia.
Projekt podobnie jak NetBSD narodził się jako kontynuacja systemu 386BSD, pierwszej próby przeniesienia Uniksa BSD na architekturę Intel. Inicjatorami projektu początkowo znanego jako "Unofficial 386BSD Patchkit" byli Nate Williams, Rod Grimes i Jordan Hubbard. 
Nazwę "FreeBSD" wymyślił David Greenman z firmy Walnut Creek CDROM, która od początku wspierała przedsięwzięcie. Maskotką FreeBSD jest "daemon", jednak od 2005 roku oficjalnie projekt posługuje się nowym logo. Oficjalnym hasłem jest "The power to serve".
Pierwsza wersja systemu ukazała się 30 listopada 1993. Do wersji 2.0 bazowała na systemie 4.3BSD, kolejne wersje opierały się na 4.4BSD Lite2, obu stworzonych przez CSRG. W 2005 projekt zdecydował o przejściu na bardziej regularne wydania. Od tej pory nowe wersje ukazują się co 6–8 miesięcy, aby rozwój systemu stał się bardziej przewidywalny dla podmiotów z niego korzystających. Decyzja ta podyktowana była przeciągającymi się pracami nad rewolucyjną w wielu dziedzinach serią 5.x.
Informacje techniczne.
System zgodny z normą POSIX. Powłoka – każda zgodna z normą POSIX, domyślnie dostarczany z csh ("de facto" tcsh) i sh (oparte na ash). System plików – FFS, UFS, UFS z rozszerzeniem "softupdates", UFS2 (w wersjach nowszych od FreeBSD 5.0). Obsługiwane są również nienatywne systemy plików, np. ZFS, XFS (w wersji 7), linuksowy ext2 oraz FAT oraz podsystem FUSE. Format binariów – obecnie ELF, do wersji 2.2.8 a.out. Jądro monolityczne, aczkolwiek z możliwością dołączania i odłączania modułów podczas pracy systemu.
FreeBSD charakteryzuje się dużą ilością nowinek w porównaniu z innymi systemami wywodzącymi się 4.4BSD: NetBSD i OpenBSD. Początkowo inicjatorzy projektu FreeBSD postanowili skupić się na stworzeniu systemu dla najpopularniejszej ówcześnie architektury sprzętowej i386. Z czasem jednak zaczęły się ukazywać także wersje dla architektur NEC PC-9801, DEC Alpha, IA-64 Itanium, Sun UltraSPARC, AMD64, MIPS, ARM, PowerPC i RISC-V. 
Wsparcie projektu podzielone jest na poziomy. Architektury tzw. "tier1" otrzymują wsparcie zespołu bezpieczeństwa, pełne wydania i binarne aktualizacje (bez potrzeby kompilacji), prekompilowane pakiety. Poziom wsparcia zapewnia gotowość wystarczającą dla środowisk produkcyjnych. W wersji 12.2 w "tier1" znajdują się następujące architektury: amd64 i i386. Architektury "tier2" uznawane są za rozwojowe lub niszowe i nie mają bieżącego wsparcia, jednak ich użytkownicy są w stanie zbudować sobie środowisko ze źródeł zarówno systemu jak i portów. Eksperymentalny poziom "tier3" oznacza, że projekt nie daje gwarancji, że źródła będą dla nich kompilowalne.
Kompatybilność ABI.
W trybie zgodności ABI można uruchamiać programy skompilowane dla Linuksa, SCO oraz SVR4 (Solaris). Binaria dla systemów BSDI, NetBSD i OpenBSD są uruchamiane w niezmienionym trybie (natywne ABI).
System portów i pakietów.
FreeBSD korzysta z opartego na źródłach systemu pakietowania znanego jako porty. Szkielet tego rozwiązania opiera się na plikach automatyzujących Makefile dla BSD make. Repozytorium portów zawiera ok. 33 tysiące programów przystosowanych do działania we FreeBSD. Na bazie portów udostępniane są prekompilowane pakiety binarne z domyślnymi opcjami dla aktualnie obsługiwanych wersji i architektur sprzętowych FreeBSD, którym projekt zapewnia wsparcie na poziomie "tier1". Zalety portów przyczyniły się do tego, że stały się bazą lub zainspirowały rozwój zbliżonych rozwiązań wykorzystywanych w pokrewnych systemach OpenBSD, NetBSD (pkgsrc) oraz kilku dystrybucji Linuksa.
Zastosowania.
Ze względu na swą wydajność i niezawodność często stosowany jako serwer internetowy lub zapora sieciowa. FreeBSD używany jest m.in. przez Apache.org, Netflix, FlightAware, Yahoo!, Yandex, Netcraft, Sony Playstation 4, WhatsApp.
Na FreeBSD działa wiele aplikacji znanych z dystrybucji linuksowych – m.in. Open Office, KDE. System jest użyteczny w zadaniach biurowych i multimedialnych. Ułatwia to życie administratorom, którzy mogą połączyć system codziennego użytku z „poligonem” do testowania nowych rozwiązań.
Renderowanie części efektów specjalnych w filmie Matrix zostało wykonane na klastrze 32 maszyn pracujących pod kontrolą FreeBSD.
Ze względu na liberalną licencję fragmenty jego kodu znalazły się w takich projektach jak Microsoft Windows, OS X oraz OS X Server, który powstał w oparciu o mikrojądro XNU i rozwiązania zaczerpnięte z FreeBSD oraz NetBSD. Ponadto wiele zamkniętych urządzeń (np. routery Junipera czy sprzętowe zapory firmy Nokia) działa w oparciu o FreeBSD. System operacyjny konsoli gier PlayStation 4, OrbisOS powstał na bazie FreeBSD.

</doc>
<doc id="1536" url="https://pl.wikipedia.org/wiki?curid=1536" title="Protokół transferu plików">
Protokół transferu plików

Protokół transferu plików, FTP (od ang. "File Transfer Protocol") – protokół komunikacyjny typu klient-serwer wykorzystujący protokół sterowania transmisją (TCP) według modelu TCP/IP (krótko: połączenie TCP), umożliwiający dwukierunkowy transfer plików w układzie serwer FTP–klient FTP.
FTP jest zdefiniowany przez IETF w dokumencie .
FTP jest protokołem 8-bitowym i dlatego nie wymaga kodowania danych do 7 bitów, tak jak w przypadku poczty elektronicznej.
Do komunikacji wykorzystywane są dwa połączenia TCP. Jedno z nich jest połączeniem sterującym, za pomocą którego przesyłane są polecenia, a drugie służy do transmisji danych. Połączenie za pomocą protokołu FTP (krótko: połączenie FTP) może działać w dwóch trybach: aktywnym i pasywnym:
W sieciach chronionych zaporą sieciową komunikacja z serwerami FTP wymaga zwolnienia odpowiednich portów na tej zaporze lub routerze. Możliwe jest zainstalowanie wielu serwerów FTP na jednym routerze. Warunkiem jest rozdzielenie portów przez router dla każdego serwera.
Serwer FTP, zależnie od konfiguracji, może pozwalać na anonimowy, czyli bez podawania hasła uwierzytelniającego, dostęp do jego zasobów. Najczęściej jednak serwer FTP autoryzuje każde połączenie za pomocą loginu i hasła.
Obsługa w przeglądarkach.
Większość współczesnych przeglądarek internetowych umożliwia odczyt i pobieranie plików znajdujących się na serwerach FTP, aczkolwiek mogą nie rozpoznawać rozszerzeń protokołu takich jak FTPS.
W chwili połączenia z adresem FTP zamiast HTTP zawartość dostępna na serwerze zdalnym jest przedstawiana w sposób zbliżony do innych elementów sieci Web. W przeglądarce Firefox można uruchomić pełnoprawnego klienta FTP po zainstalowaniu rozszerzenia "FireFTP".
Wraz z wydaniem Google Chrome w wersji 88 przeglądarka utraciła całkowicie obsługę protokołu. W 2019 roku Mozilla analizowała możliwe opcje, włączając w to usunięcie jedynie wsparcia dla już nieużywanych implementacji FTP celem uproszczenia kodu programu.
W kwietniu 2021 roku Mozilla wydała Firefoksa 88.0, który domyślnie miał wyłączoną obsługę FTP. W lipcu tego samego roku opublikowano wersję 90.0, w której usunięto całkowicie wsparcie dla protokołu.
Składnia.
Składnię URL FTP zdefiniowano w dokumencie , przyjmującą formę (dane podane w nawiasach są opcjonalne).
Przykładowo URL przedstawia dokument tekstowy myfile.txt, znajdujący się w folderze mydirectory na serwerze public.ftp-servers.example.com jako zasób FTP. Adres dodaje specyfikację nazwy użytkownika i hasła wymaganą do uzyskania dostępu do zasobu.
Więcej informacji dotyczących określania nazwy użytkownika i hasła można odnaleźć w dokumentacji samych przeglądarek internetowych (np. Firefox oraz Internet Explorer). Domyślnie większość przeglądarek używa trybu pasywnego (PASV), który łatwiej przedostaje się przez zapory (firewalle) użytkowników końcowych.
Istniały pewne odmiany tego, jak różne przeglądarki traktowały ścieżki dostępu w przypadku gdy katalog „domowy” użytkownika nie był głównym.

</doc>
<doc id="1538" url="https://pl.wikipedia.org/wiki?curid=1538" title="Finlandia">
Finlandia

Finlandia, Republika Finlandii (; ) – państwo w Europie Północnej, powstałe po odłączeniu od Rosji w 1917 roku. Członek Unii Europejskiej. Graniczy od zachodu ze Szwecją, od północy z Norwegią i od wschodu z Rosją. Od zachodu i południa ma ponadto dostęp do Morza Bałtyckiego i do Zatoki Fińskiej. Zaliczana do krajów nordyckich. Jej stolicą są Helsinki, które wraz z miastami Espoo i Vantaa tworzą obszar metropolitalny; kolejnym największym obszarem miejskim jest Tampere, położone około 180 kilometrów na północ od Helsinek.
Niepodległa Finlandia powstała w 1917, po upadku caratu w Imperium Rosyjskim. Wcześniej była pod obcym panowaniem – szwedzkim od średniowiecza do roku 1809, a następnie ponad 100 lat pod panowaniem rosyjskim, jako autonomiczne Wielkie Księstwo Finlandii.
Geografia.
Większość powierzchni kraju stanowią niziny z młodą rzeźbą polodowcową. Naturalne zachodnie i południowe granice Finlandii wyznaczają Zatoka Botnicka i Zatoka Fińska, pomiędzy którymi rozciąga się poprzecinane ciągami wzniesień morenowych Pojezierze Fińskie złożone z około 55 tys. jezior. Jeziora zajmują 18,8 tys. km², bardzo liczne są bagna i torfowiska. W środkowej części kraju ukształtowanie terenu ma cechy południowych pojezierzy i leżących na północ wyżyn. Na północ od Koła Podbiegunowego rozciąga się Laponia Fińska, która stanowi przedgórze Gór Skandynawskich. Linia wybrzeża szkierowego, jest silnie rozwinięta, z dużą liczbą małych wysepek.
Powierzchnia:
Długość granicy lądowej:
Długość wybrzeża:
Jeziora.
Poniższa lista przedstawia 10 największych jezior Finlandii:
W całym kraju występuje 187 888 jezior, które mają powierzchnię większą niż 500 m².
Klimat.
Południowa część Finlandii leży w strefie klimatu umiarkowanego chłodnego przejściowego. Natomiast północ (w tym Fińska Laponia) jest w zakresie klimatu okołobiegunowego subpolarnego. Średnie temperatury lipca na południu wynoszą od +17 do +18 °C i od +14 do +15 °C na północy (w najchłodniejszym miejscu pod Kilpisjärvi +12 °C). W styczniu średnie temperatury wynoszą –2 °C na Wyspach Alandzkich, –4 °C na południowym wybrzeżu, w środkowej części kraju od –8 do –10 °C i –14 °C na północy. Długość okresu wegetacyjnego na południu wynosi 185-200 dni, na północy 120. Najwyższą temperaturę zanotowano w 1914 roku w Liperi, koło Joensuu i wynosiła ona +37,2 °C. Nieoficjalnie najniższa temperatura wystąpiła w Kaaresuvanto (–54 °C). Okres zimowy występuje zwykle od połowy grudnia do końca marca na Wyspach Alandzkich, od końca listopada do końca marca na południowym wybrzeżu, od połowy listopada do połowy kwietnia w środkowej Finlandii i od końca października do końca kwietnia na północy. Liczba dni w roku z temperaturą powyżej +25 °C waha się od 20-25 na południu do 5-7 na północy (w górskiej stacji Kilpisjärvi takie temperatury występują średnio raz na 2 lata). Najwięcej, tj. 48 takich dni zanotowano w mieście Kouvola w 2010 roku. Temperatury powyżej +30 °C zdarzają się rzadko i nie występują co roku. Największą ich liczbę zaobserwowano w 2010 roku w Lappeenrancie (14 dni). Okres bez przymrozków trwa od końca kwietnia do połowy października na Wyspach Alandzkich, na południu kraju od początku maja do końca września, na północy od początku czerwca do końca sierpnia, choć sporadycznie przymrozki mogą występować także w lipcu. Średnie roczne opady w Helsinkach wynoszą 650 mm, a największe ich natężenie przypada na sierpień. Na północy suma opadów wynosi 500 mm.
Polityka.
Finlandia była do 2000 roku republiką półprezydencką, ówcześnie przyjęta konstytucja, uchylając tę z roku 1919, zastąpiła ten system parlamentarno-gabinetowym.
Głową państwa jest prezydent (od 2012 roku ten urząd sprawuje Sauli Niinistö), wybierany przez społeczeństwo w bezpośrednich wyborach na sześcioletnią kadencję i może być wybrany na najwyżej dwie następujące po sobie kadencje. W kompetencjach prezydenta leży, między innymi, mianowanie premiera i na jego wniosek ministrów. Mianuje kanclerza sprawiedliwości, przewodniczącego i członków Sądu Najwyższego oraz Naczelnego Sądu Administracyjnego, a także arcybiskupa i biskupów Kościoła ewangelicko-augsburskiego. Posiada możliwość rozwiązania parlamentu, prawo kontroli administracji, prawo inicjatywy ustawodawczej i veta ustawodawczego. Jest również najwyższym zwierzchnikiem sił zbrojnych.
Parlament (Eduskunta) jest wybierany przez społeczeństwo na czteroletnią kadencję i liczy 200 osób.
Rząd (Rada Państwa)
Konstytucja
Do 1 marca 2000 roku w skład konstytucji wchodziło pięć ustaw konstytucyjnych:
1 marca 2000 roku weszła w życie nowa konstytucja, zaakceptowana przez Eduskuntę wymaganą większością 2/3 głosów dwukrotnie w 1999 roku (zmiana konstytucji musi zostać uchwalona i potwierdzona przez nowo wybrany parlament).
Podział administracyjny.
Od 1 stycznia 2011 Finlandia jest podzielona na 19 regionów, regiony są podzielone na 70 podregionów, podregiony dzielą się na 320 gmin. Dawny podział na 6 prowincji stracił swe znaczenie 1 stycznia 2010 r.
Siły zbrojne.
Finlandia dysponuje trzema rodzajami sił zbrojnych: wojskami lądowymi, marynarką wojenną oraz siłami powietrznymi. Uzbrojenie sił lądowych Finlandii składało się w 2014 roku z: 270 czołgów, 1392 opancerzonych pojazdów bojowych, 72 dział samobieżnych, 70 wieloprowadnicowych wyrzutni rakietowych oraz 689 zestawów artylerii holowanej. Marynarka wojenna Finlandii dysponowała w 2014 roku 19 okrętami obrony przybrzeża oraz 19 okrętami obrony przeciwminowej. Fińskie siły powietrzne z kolei posiadały w 2014 roku uzbrojenie w postaci m.in. 54 myśliwców, 11 samolotów transportowych, 57 samolotów szkolno-bojowych oraz 21 śmigłowców.
Wojska fińskie w 2014 roku liczyły 36,5 tys. żołnierzy zawodowych oraz 357 tys. rezerwistów. Według rankingu "Global Firepower" (2022) fińskie siły zbrojne stanowią 53. siłę militarną na świecie, z rocznym budżetem na cele obronne w wysokości 4,9 mld dolarów (USD).
Gospodarka.
Do XX wieku Finlandia była krajem zacofanym. Po II wojnie światowej gospodarka kraju była silnie związana z gospodarką ZSRR. Z drugiej strony jednak Finlandia dzięki sąsiedztwu Szwecji i Norwegii, stała się państwem socjalnym z prywatną własnością, ale wysokimi podatkami. Podatki stanowią 45% PKB i jest to poziom najwyższy na świecie po innych krajach skandynawskich, które mają podatki nieco wyższe niż 50% PKB. Najbardziej znaną fińską firmą jest Nokia, która jeszcze do 2008 roku posiadała 35% światowego rynku telefonii komórkowej (wówczas 1. miejsce na świecie). Fińskie, nominalne PKB per capita w 2005 wynosiło 40 197 dolarów, dając Finlandii 12. miejsce na świecie, a PKB per capita zmierzone parytetem siły nabywczej 34 tys. 819 dolarów, dając jej także 12. miejsce na świecie. Eksport wynosił nieco ponad 80 mld dolarów. Wskaźnik Giniego, czyli poziom rozpiętości w dochodach, wynosi 26,9 i jest jednym z najniższych na świecie.
Wydobywa się rudy miedzi, cynku, żelaza, chromu i niklu. Podstawowe gałęzie przemysłu, to przemysł:
Rolnictwo to przede wszystkim hodowla bydła typu mlecznego, a na północy reniferów oraz leśnictwo (zob. lasy Finlandii).
Stopa bezrobocia wynosi 8,2%, średnia długość życia: mężczyźni 78 lat, kobiety 84 lata.
W 2004 Finlandia zajęła pierwsze miejsce w rankingu konkurencyjności Światowego Forum Ekonomicznego.
Finlandia produkuje także samochody ciężarowe, wojskowe i specjalistyczne samochody Sisu.
Turystyka.
W 2015 roku kraj ten odwiedziło 2,622 mln turystów (4% mniej niż w roku poprzednim), generując dla niego przychody na poziomie 2,750 mld dolarów. Przychody z turystyki stanowią około 2,5% fińskiego PKB. Stwarza ona miejsca pracy dla 140 tysięcy osób.
Finlandia jest najchętniej odwiedzana przez turystów z Rosji, Szwecji i Niemiec. Kraj stanowi również najpopularniejszy zagraniczny cel wycieczkowy Rosjan.
Telekomunikacja.
Około 79% fińskiego społeczeństwa używa Internetu, a wszystkie fińskie szkoły i biblioteki publiczne posiadają łącza internetowe. W październiku 2009 fiński minister transportu i komunikacji zobowiązał się do zapewnienia od połowy 2010 roku każdemu mieszkańcowi kraju dostępu do Internetu z przepustowością nie mniejszą niż 1 Mbps.
Transport.
Długość sieci drogowej Finlandii wynosi 79 390 km (1.01.2015). 8603 km z nich stanowią drogi krajowe (fiń. "Valtatiet"), z czego 810 km to autostrady.
Sieć kolejowa ma długość 5919 km. W Finlandii obowiązuje rozstaw szyn 1524 mm. Jest to o 4 mm więcej niż w sąsiedniej Rosji, ale pociągi mogą bez zmiany jeździć po obu rozstawach. Finlandia posiada cztery kolejowe przejścia graniczne z Rosją oraz jedno ze Szwecją w Tornio.
Największym lotniskiem w kraju jest port lotniczy Helsinki-Vantaa. W 2017 roku obsłużył 18,9 mln pasażerów na 22,9 mln pasażerów w całym kraju. Drugim co do wielkości lotniskiem jest Oulu (923 tys. pasażerów).
Finlandia posiada dobre połączenia promowe z Estonią, Szwecją, Niemcami i Polską. Przewoźnik Finnlines obsługuje połączenie z Gdyni do Helsinek. Najpopularniejsza trasa promowa to trasa z Estonii przez Zatokę Fińską: Tallinn – Helsinki.
Demografia.
Populacja.
Liczba ludności Finlandii w 2016 roku wynosiła 5 503 297 obywateli. Średnia gęstość zaludnienia wynosi 17 mieszkańców na kilometr kwadratowy, co czyni z Finlandii trzeci (po Norwegii i Islandii) najrzadziej zaludniony kraj Europy. Południowa część zawsze była gęściej zamieszkana niż północna. Ta dysproporcja wzrosła wraz z postępującą urbanizacją w XX wieku. Największymi miastami Finlandii są Helsinki, Espoo, Tampere i Vantaa. Inne duże znaczące miasta to Turku i Oulu.
W Finlandii nie prowadzi się oficjalnych statystyk etnicznych. Dostępne są jednak statystyki ludności fińskiej według języka, obywatelstwa i pochodzenia. W 2018 roku 92,7% ludności ma pochodzenie fińskie, a pozostałe 7,3% to obcokrajowcy. Szacuje się, że 3,9% mieszkańców ma pochodzenie europejskie inne niż fińskie (głównie z Estonii, Rosji, krajów byłej Jugosławii, a nawet Turcji), 2,1% ma pochodzenie azjatyckie (głównie z Iraku, Chin, Wietnamu, Afganistanu, Tajlandii i Iranu), 0,94% afrykańskie (głównie z Somalii) i 0,21% amerykańskie. Pozostałe 0,15% miało inne pochodzenie, bądź nieokreślone.
Miasta.
Największe miasta (2016):
Języki.
Dla większości Finów (92%) język fiński jest językiem ojczystym. Język ten należy do języków bałtycko-fińskich, podgrupy języków uralskich. Morfologicznie rzecz biorąc, jest to język aglutynacyjny. Oznacza to odmianę form rzeczowników, przymiotników, zaimków, liczebników i czasowników w zależności od ich roli w zdaniu. W praktyce oznacza to stosowanie przyrostków i złożeń, zamiast przyimków i zrostków. Szacuje się, że około 65% wszystkich fińskich słów to złożenia. Bardzo blisko z nim spokrewniony jest język estoński. Języki te (fiński i estoński), wraz z węgierskim i baskijskim są najpopularniejszymi językami nieindoeuropejskimi w Europie. Finlandia jest jednym z trzech krajów, w których język uralski jest używany przez większość mieszkańców. Pozostałe dwa to Estonia i Węgry.
Drugim pod względem popularności językiem w Finlandii jest język szwedzki (w tym osobna fińska odmiana języka szwedzkiego), którym posługuje się około 5,6% ludności. Używane są także języki: rosyjski (0,8%), estoński (0,3%), fińsko-romski oraz fiński język migowy (przez około 5000 osób). Na północy kraju, w Laponii, zamieszkuje około 7000 Lapończyków. Około jedna czwarta z nich używa języków lapońskich jako ojczystych. Prawo każdej z mniejszości w Finlandii do pielęgnowania swych odrębnych tradycji i kultury jest chronione przez fińską konstytucję.
W 2005 w badaniach przeprowadzonych w ramach Eurobarometru dotyczących języków Unii Europejskiej 60% dorosłych obywateli deklarowało znajomość angielskiego, 38% szwedzkiego (41% w 2008), a 17% niemieckiego. Ta liczba ludzi znających angielski umieściła Finlandię na piątym miejscu za Maltą (89%), Holandią (86%), Szwecją (85%) i Danią (83%). Język niemiecki jest w Finlandii znacznie bardziej znany niż francuski i hiszpański.
Religia.
Religią dominującą w Finlandii jest protestantyzm.
Główne wyznania:
Od 1923 roku fińska konstytucja gwarantuje wolność religijną. Fiński Kościół Ewangelicko-Luterański i Fiński Kościół Prawosławny mają status Kościołów narodowych. Dzięki temu mają specjalne przywileje, wierni płacą na ich rzecz podatki w wysokości 1% lub 2% dochodów. Tylko 2% luteran chodzi do kościoła co tydzień, a 10% co miesiąc. Odsetek osób wierzących w Boga i uznających podstawowe prawdy religii chrześcijańskiej jest wyższy, niż w innych państwach nordyckich; częstsze są też, pomimo niskiego uczestnictwa w nabożeństwach, prywatne praktyki religijne.
Większość fińskich dzieci jest chrzczona (62,2% w 2019) i konfirmowana (78,4% w 2019). Większość pogrzebów jest chrześcijańska. Większość luteran chodzi do kościoła tylko przy specjalnej okazji jak np. śluby, pogrzeby i święta. Według badań Eurobarometru z 2010 r. 33% Finów twierdziło, że „wierzy w istnienie Boga”, 42% stwierdziło, że „wierzy w istnienie jakiejś siły wyższej”, a 22%, że „nie wierzy w Boga ani żadną inną siłę wyższą”.
Struktura rodzinna.
Życie rodzinne w Finlandii odpowiada modelowi rodziny nuklearnej. Relacje z dalszą rodziną są raczej rzadkie i przypadki łączenia się ludzi w formie rodziny wielopokoleniowej raczej się nie zdarzają. Zgodnie z raportem UNICEF-u, Finlandia zajmuje bardzo wysokie, czwarte miejsce na świecie pod względem zapewnienia dziecku odpowiednich warunków życia.
Opieka zdrowotna.
Na każde 307 osób przypada jeden lekarz. Około 18,9% funduszy przeznaczonych na opiekę zdrowotną pochodzi z sektora prywatnego, a 76,6% z publicznego. Najważniejszymi instytucjami są Ministerstwo Zdrowia oraz Narodowy Publiczny Instytut Zdrowia. Według szwedzkich badań, przeprowadzonych w 16 krajach, Finlandia posiada najwydajniejszy system opieki zdrowotnej.
Przewidywana długość życia wynosi 82 lata dla kobiet i 75 dla mężczyzn. Jeszcze w latach 70. XX wieku Finlandia miała jeden z najwyższych odsetków zgonów spowodowanych chorobami serca. Od tego czasu zaczęto propagować zdrowe odżywianie się oraz ćwiczenia fizyczne, co przyniosło bardzo dobry efekt. Odsetek osób palących jest bardzo niski i wynosi 26% w przypadku mężczyzn i 19% w przypadku kobiet.
Całkowite roczne spożycie alkoholu jest niższe niż w innych europejskich państwach, pomimo tego, że w weekendy bardzo popularne są zabawy z dużą ilością trunków. Wśród ludności w wieku produkcyjnym choroby i wypadki spowodowane przez alkohol stały się w ostatnim czasie najczęstszą przyczyną śmierci.
Narodowy Publiczny Instytut Zdrowia twierdzi, że 54% mężczyzn i 38% kobiet ma nadwagę, inne źródła podają wyższe wartości, odpowiednio 70% i 50%. Przewiduje się, że odsetek osób chorych na cukrzycę wzrośnie do 15% w 2015 roku. Finlandia ma najwyższy odsetek chorych na cukrzycę typu 1.
Edukacja.
W 2003 roku w studiach PISA, prowadzonych przez OECD, fińscy uczniowie we wszystkich dziedzinach poza matematyką, w której byli drudzy, zajęli pierwsze miejsca na świecie. Fiński system edukacji został zreformowany na początku lat 70. z modelu niemieckiego, który został uznany za niewydajny. Prawie wszystkie szkoły w Finlandii to szkoły państwowe, a za szkoły niepaństwowe nie płaci się, gdyż finansuje je państwo. Również wszystkie uniwersytety są państwowe, kilka z istniejących obecnie uniwersytetów to upaństwowione uniwersytety, które kiedyś znajdowały się w rękach prywatnych. W fińskich szkołach nie stawia się ocen przez pierwsze 3 lata kształcenia, nie praktykowane jest pozostawianie ucznia na drugi rok w tej samej klasie (w szkole podstawowej, jeżeli rodzice się nie zgadzają), istnieje zakaz dyskryminacji z jakiegokolwiek powodu: pochodzenia, stanu majątkowego rodziców itd. Bardzo podobny system z ograniczoną liczbą egzaminów istniał przez pewien czas w Związku Radzieckim.
Od poziomu szkoły podstawowej obowiązkowa jest nauka języka angielskiego, od gimnazjum dodatkowo nauka języka szwedzkiego. W gimnazjum można wybrać trzeci język, a w liceum panuje w tej kwestii dowolność (nauka angielskiego i szwedzkiego jest kontynuowana).
W szkołach organizuje się zajęcia wychowania fizycznego, edukacji zdrowotnej oraz gotowania. Jeśli chodzi o wychowanie fizyczne, to fińskie szkoły zajmują jedno z ostatnich miejsc w Unii Europejskiej, przeznaczając na nie najmniej czasu. Według Narodowego Publicznego Instytutu Zdrowia tylko jedna trzecia dorosłych wystarczająco dużo ćwiczy.
Kultura.
Fińscy laureaci Nagrody Nobla:
Fiński epos narodowy:
Najpopularniejszy utwór dziecięcy:
Sport.
Za narodowy sport Finlandii uznaje się pesäpallo, grę zespołową zbliżoną swoimi zasadami do amerykańskiego baseballu. Dużą popularnością cieszą się także sporty zimowe, a w szczególności hokej na lodzie oraz biegi narciarskie. Do niedawna Finowie uchodzili za jednych z najlepszych skoczków narciarskich na świecie, jednak w ostatnich latach coraz częściej mówi się o kryzysie w tej dyscyplinie, co spowodowane jest wysokimi kosztami związanymi z trenowaniem tego sportu, wycofywaniem się dotychczasowych sponsorów oraz brakiem utalentowanych zawodników na miarę Matti Nykänena czy Janne Ahonena, a także brakiem ogólnego zainteresowania tą dyscypliną sportu wśród fińskiej młodzieży.
Poza zimowymi uznanie zyskują także sporty motorowe. Finowie Keke Rosberg, Mika Häkkinen oraz Kimi Räikkönen zdobywali tytuł mistrza świata Formuły 1.
Popularna jest także fińska liga żużlowa.
W Helsinkach odbyły się XV Letnie Igrzyska Olimpijskie. Najwybitniejszym fińskim lekkoatletą pozostaje do dzisiaj biegacz Paavo Nurmi, wielokrotny złoty medalista olimpijski.
Finlandia to jeden z niewielu krajów europejskich, w którym szerokiej popularności nie zdobyła piłka nożna. Mimo iż reprezentanci Finlandii grywali w silnych europejskich klubach, to drużynie narodowej długo nie udało się zakwalifikować do żadnego międzynarodowego turnieju. Dopiero 15 listopada 2019 reprezentacja Finlandii, pokonując 3:0 reprezentację Liechtensteinu, uzyskała pierwszy historyczny awans na Mistrzostwa Europy. Tam pokonała 1:0 reprezentację Danii, przegrała 0:1 z reprezentacją Rosji i przegrała 0:2 z reprezentacją Belgii, odpadając po fazie grupowej.

</doc>
<doc id="1540" url="https://pl.wikipedia.org/wiki?curid=1540" title="Fluor">
Fluor

Fluor (F, od fluorytu) – pierwiastek chemiczny, niemetal z grupy fluorowców w układzie okresowym. Fluor w stanie wolnym występuje w postaci dwuatomowej cząsteczki . Jest żółtozielonym silnie trującym gazem o ostrym zapachu podobnym do chloru.
Jest najaktywniejszym niemetalem o największej elektroujemności, tworzącym związki z większością innych pierwiastków (nawet z gazami szlachetnymi – kryptonem, ksenonem i radonem). W przeciwieństwie do innych fluorowców fluor łączy się wybuchowo z wodorem, tworząc fluorowodór bez dostępu światła i w niskiej temperaturze. W strumieniu gazowego fluoru palą się szkło, metale i woda. Z powodu jego dużej aktywności nie można go przechowywać ani wytwarzać w naczyniach szklanych.
Wchodzi w skład kwasu fluorowodorowego i fluorków, m.in. minerału fluorytu (). W roztworze wodnym najczęściej występuje jako jon fluorkowy . Znane są także jony kompleksowe fluoru, np. , lub .
Zawartość w górnych warstwach Ziemi wynosi 0,054%. Jedynym stabilnym izotopem fluoru jest .
Otrzymywanie.
Zarówno w skali przemysłowej, jak i laboratoryjnej, fluor otrzymuje się praktycznie wyłącznie poprzez elektrolizę ciekłej mieszaniny fluorowodoru i fluorku potasu. Pierwotnie stosowano mieszaninę bogatą w HF i temperaturę niższą od pokojowej, później opracowano proces wysokotemperaturowy, w którym elektrolit miał skład KF·HF i temperaturę topnienia 239 °C. Metoda ta dominowała w pierwszej połowie XX w. Obecnie przeważa proces średniotemperaturowy, prowadzony w 80–110 °C, z elektrolitem o składzie KF·2HF (ttopn. = 71,7 °C).
Zastosowanie.
Gazowego fluoru używa się przy produkcji monomerów, fluorowanych alkenów, z których otrzymuje się teflon i jego pochodne. Oprócz tego jest stosowany do produkcji halonów, które są stosowane jako ciecze chłodzące i hydrauliczne (np. freon). Inne zastosowania:
Historia.
Fluoryt (tj. fluorek wapnia, ) został opisany w 1529 roku przez Georgiusa Agricolę jako topnik obniżający temperaturę topnienia innych minerałów (od właściwości tej pochodzi nazwa minerału: łac. "" = płynąć). W 1670 roku Heinrich Schwanhard odkrył, że w wyniku działania kwasów na fluoryt powstaje gaz trawiący szkło (był to fluorowodór, HF). Właściwości kwasowe HF odkrył w roku 1780 Carl Scheele. Humphry Davy po otrzymaniu pierwiastkowego chloru przez utlenienie chlorowodoru (1810) usiłował – bez powodzenia – wraz z André Ampèrem w podobny sposób wyizolować fluor z fluorowodoru. Badacze ci nadali nowemu pierwiastkowi nazwę "fluorum" od minerału fluorytu, . Polską nazwę – fluor – wprowadził Filip Walter.
Wolnego fluoru przez wiele lat nie udawało się wyodrębnić z powodu jego ogromnej aktywności chemicznej – po wytworzeniu poprzez elektrolizę stopionego fluorytu, w wysokiej temperaturze procesu (temperatura topnienia wynosi ok. 1400 °C) natychmiast reagował z substancjami i materiałami obecnymi w naczyniach reakcyjnych. Próby otrzymania fluoru doprowadziły przy tym do śmierci kilku badaczy. W roku 1885 Edmond Frémy rozpoczął badania nad elektrolizą fluorowodoru, jednak okazało się, że suchy związek nie przewodzi prądu, a związek wilgotny prąd wprawdzie przewodził, ale na elektrodzie wydzielał się tlen z obecnej wody, a nie fluor. Badania te kontynuował jego uczeń, Henri Moissan, który ostatecznie w 1886 roku otrzymał fluor przez elektrolizę fluorowodoru zawierającego dodatek (było to przypadkowe zanieczyszczenie materiału do elektrolizy). Moissanowi otrzymanie nowego pierwiastka przyniosło Nagrodę Nobla w dziedzinie chemii w 1906 roku.
Pierwsza produkcja fluoru na skalę przemysłową została uruchomiona na potrzeby projektu Manhattan. Gazowy fluorek uranu(VI) () był wtedy używany do rozdzielenia izotopów i podczas wzbogacania uranu.
Na 100-lecie odkrycia fluoru Karl Christe otrzymał ten pierwiastek poprzez reakcję heksafluoromanganianu(IV) potasu z fluorkiem antymonu(V) w temp. 150 °C:
Była to pierwsza chemiczna (nie elektrochemiczna) metoda pozwalająca na otrzymanie fluoru pierwiastkowego ze znaczącą wydajnością (ok. 40%).
Związki.
Fluor może zastępować wodór w związkach organicznych, dlatego liczba związków fluoru może być bardzo duża. Związki fluoru z gazami szlachetnymi po raz pierwszy otrzymali Howard Claassen, Henry Selig i John Malm w 1962 roku. Pierwszym z tych związków był tetrafluorek ksenonu. Otrzymano również fluorki kryptonu i radonu.
Fluor otrzymuje się z fluorytu, kriolitu lub fluoroapatytu.
Szkodliwość.
Pierwiastkowy fluor, jak i jony fluorkowe, są silnie toksyczne. Zaburza procesy enzymatyczne w komórkach, hamując oddychanie tkankowe, przemianę węglowodanów, lipidów oraz syntezę hormonów. Sam fluor i niektóre jego związki działają żrąco, powodując głębokie martwice. Wolny fluor ma charakterystyczny drażniący zapach i jest wyczuwalny nawet w stężeniu 20 ppm.
Gazowy fluor łatwo wchłania się przez drogi oddechowe i pokarmowe. Związki fluoru obecne np. w żywności dobrze wchłaniają się z dróg pokarmowych.
Po podaniu doustnym w dużych stężeniach związki fluoru powodują zatrucia ostre, na skutek żrącego działania wydzielającego się fluorowodoru. Dalsze objawy to płytki oddech, kurcz dłoni, drgawki, ślinotok i nudności. Bezpośrednią przyczyną zgonu w wyniku zatrucia fluorem jest porażenie ośrodka oddechowego.
Zatrucia przewlekłe małymi dawkami fluoru, czyli fluoroza, objawiają się zaburzeniami w uwapnieniu kości, brunatnym cętkowaniem zębów, zmniejszeniem ruchliwości.
Dozwolone maksymalne stężenie przy założeniu 8-godzinnej ekspozycji na działanie fluoru to 0,05 mg/m³ (czyli mniej niż w przypadku cyjanowodoru).
Znaczenie biologiczne.
Pomimo znacznej toksyczności, fluor w odpowiednich ilościach jest pierwiastkiem niezbędnym dla prawidłowego rozwoju kości i zębów. Może on modyfikować hydroksyapatyty budujące szkliwo zębów i poprzez substytucję grup wodorotlenowych tworzyć fluoroapatyty. Szkliwo zawierające taką modyfikację ulega wzmocnieniu oraz wykazuje większą oporność na działanie kwasów produkowanych przez bakterie próchnicotwórcze obecne w płytce nazębnej. Te specyficzne właściwości wynikają z faktu, iż fluoroapatyty wykazują lepszą krystaliczność, twardość oraz mniejszą rozpuszczalność w kwasach niż naturalnie występujące hydroksyapatyty.
Środki ostrożności.
Zarówno fluor, jak i fluorowodór muszą być przechowywane z zachowaniem szczególnych środków ostrożności. Powinno się unikać wszelkiego kontaktu ze skórą lub oczami. Fluoru nie przechowuje się w szkle.

</doc>
<doc id="1541" url="https://pl.wikipedia.org/wiki?curid=1541" title="Zapora sieciowa">
Zapora sieciowa

Zapora sieciowa ( – ściana ogniowa) – jeden ze sposobów zabezpieczania sieci i systemów przed intruzami.
Termin ten może odnosić się zarówno do sprzętu komputerowego wraz ze specjalnym oprogramowaniem, jak i do samego oprogramowania blokującego niepowołany dostęp do komputera, na którego straży stoi. Pełni rolę połączenia ochrony sprzętowej i programowej sieci wewnętrznej LAN przed dostępem z zewnątrz, tzn. sieci publicznych, Internetu, chroni też przed nieuprawnionym wypływem danych z sieci lokalnej na zewnątrz. Często jest to komputer wyposażony w system operacyjny (np. Linux, BSD) z odpowiednim oprogramowaniem. Do jego podstawowych zadań należy filtrowanie połączeń wchodzących i wychodzących oraz tym samym odmawianie żądań dostępu uznanych za niebezpieczne.
Najczęściej używanymi technikami obrony są:
Bardzo ważną funkcją zapory sieciowej jest monitorowanie ruchu sieciowego i zapisywanie najważniejszych zdarzeń do dziennika (logu). Umożliwia to administratorowi wczesne dokonywanie zmian konfiguracji. Poprawnie skonfigurowana zapora powinna odeprzeć wszelkie znane typy ataków. Na zaporze można zdefiniować strefę ograniczonego zaufania – podsieć, która izoluje od wewnętrznej sieci lokalne serwery udostępniające usługi na zewnątrz.
Historia.
Termin „firewall” pierwotnie odnosił się do ściany przeznaczonej do zamknięcia ognia w budynku. Dalsze zastosowania odnoszą się do podobnych struktur, takich jak metalowa blacha oddzielająca komorę silnika pojazdu lub samolotu od kabiny pasażerskiej. Termin ten został zastosowany pod koniec lat osiemdziesiątych do technologii sieciowej, która pojawiła się, gdy Internet był dość nowy pod względem globalnego wykorzystania i łączności. Poprzednikami zapór sieciowych były routery używane w późnych latach osiemdziesiątych.
Pierwsza generacja: filtr pakietów.
Pierwszym zgłoszonym typem zapory sieciowej jest filtr pakietów. Filtry pakietów sprawdzają adresy sieciowe i porty pakietów, aby ustalić, czy muszą być przyznane czy odrzucane. Pierwszy artykuł na temat technologii firewall został opublikowany w 1988 roku, kiedy inżynierowie z Digital Equipment Corporation (DEC) opracowali systemy filtrowania znane jako firewall filtrów pakietów. Ten dość podstawowy system to pierwsza generacja, która później stała się ważną funkcją bezpieczeństwa w Internecie. W AT&amp;T Bell Labs, Bill Cheswick i Steve Bellovin kontynuowali swoje badania w zakresie filtrowania pakietów i opracowali działający model dla własnej firmy oparty na oryginalnej architekturze pierwszej generacji.
Filtry pakietów działają, sprawdzając pakiety, które są przesyłane między komputerami w Internecie. Gdy pakiet nie pasuje do zestawu reguł filtrowania pakietu filtrów, filtr pakietów albo dyskretnie odrzuca pakiet, albo odrzuca pakiet i generuje powiadomienie protokołu kontroli Internetu dla nadawcy. I odwrotnie, kiedy pakiet pasuje do jednej lub więcej zaprogramowanych reguł filtrowania, może on przejść. Elementy, które można zdefiniować w regule filtra pakietów, obejmują adres źródłowy i docelowy pakietu, protokół oraz porty źródłowe i docelowe. Większość komunikacji internetowej w XX i na początku XXI wieku wykorzystywała protokół TCP ("Transmission Control Protoco"l) i protokół UDP ("User Datagram Protocol") w połączeniu ze znanym portem, umożliwiając zaporom sieciowym tej epoki rozróżnianie, a tym samym kontrolowanie określonych typów ruchów (takich jak przeglądanie stron internetowych, druk zdalny, wiadomości e-mail, przesyłanie plików), chyba że komputery po obu stronach filtra pakietów używają tych samych niestandardowych portów.
Druga generacja: warstwa transportowa.
W latach 1989–1990 trzech pracowników AT&amp;T Bell Laboratories (Dave Presotto, Janardan Sharma i Kshitij Nigam) opracowali drugą generację firewalli, nazywając je „bramkami na poziomie obwodu” (ang. "circuit-level gateways").
Zapory sieciowe drugiej generacji wykonują pracę poprzedników, ale działają do warstwy 4 (warstwa transportowa) modelu OSI. Osiąga się to poprzez zatrzymanie pakietów, dopóki nie będzie wystarczającej ilości informacji, aby ocenić ich stan. Firewall rejestruje wszystkie połączenia przechodzące przez niego i określa, czy pakiet jest początkiem nowego połączenia, częścią istniejącego połączenia, czy nie jest częścią żadnego połączenia. Chociaż reguły statyczne są nadal używane, mogą teraz zawierać stan połączenia jako jedno z kryteriów testowych.
Niektóre ataki typu „odmowa usługi” bombardują zaporę sieciową tysiącami fałszywych pakietów połączeń, próbując ją opanować poprzez wypełnienie pamięci stanu połączenia.
Trzecia generacja: warstwa aplikacji.
Marcus Ranum, Wei Xu i Peter Churchyard opracowali trzecią generacje firewalla znaną jako Firewall Toolkit (FWTK). W czerwcu 1994 roku Wei Xu rozszerzył FWTK o ulepszenie jądra filtra IP i niewidoczne gniazda. Była ona znana jako pierwsza niewidoczna zapora aplikacji, udostępniona jako produkt komercyjny w zaufanych systemach informatycznych (Trusted Information Systems). Zapora sieciowa Gauntlet została oceniona jako jedna z najlepszych zapór sieciowych w latach 1995–1998.
Kluczową zaletą filtrowania warstw aplikacji jest to, że może ona „zrozumieć” określone aplikacje i protokoły (takie jak protokół FTP, system nazw domen (DNS) lub protokół HTTP (Hypertext Transfer Protocol)). Jest to użyteczne, ponieważ jest w stanie wykryć, czy niechciana aplikacja lub usługa próbuje ominąć zaporę sieciową za pomocą protokołu na dozwolonym porcie lub wykryć, czy protokół jest nadużywany w jakikolwiek szkodliwy sposób.
Począwszy od 2012 roku, tak zwana zapora następnej generacji (NGFW) to nic innego jak „szersza” lub „głębsza” inspekcja na stosie aplikacji. Na przykład istniejąca funkcja głębokiej inspekcji pakietów w nowoczesnych zaporach sieciowych może zostać rozszerzona o takie funkcje jak:
Typy zapór sieciowych.
Współcześnie często pracująca zapora sieciowa jest rozwiązaniem hybrydowym analizującym pakiety od warstwy łącza danych do aplikacji modelu OSI. Umożliwia realizację złożonych polityk bezpieczeństwa oraz integrację z systemami IDS. Skuteczna ochrona zasobów IT oznacza całościowe działania, kierujące ku podstawom bezpieczeństwa. Najważniejsze jego elementy dotyczą infrastruktury, backupu, danych osobowych, zabezpieczenia przed problemami wynikającymi z utraty zasilania czy chociażby awarii chłodzenia.

</doc>
<doc id="1542" url="https://pl.wikipedia.org/wiki?curid=1542" title="Funkcja stanu">
Funkcja stanu

Funkcja stanu – funkcja zależna wyłącznie od stanu układu, czyli od aktualnych wartości jego parametrów, takich jak masa, liczność materii, temperatura, ciśnienie, objętość i inne.
Wartość funkcji stanu z definicji nie zależy od jego historii, tzn. tego co działo się z nim wcześniej. Wynika z tego bezpośrednio inna podstawowa własność funkcji stanu:
Funkcja termodynamiczna zależna od historii (drogi) układu jest nazywana funkcją procesu. 
Całka oznaczona różniczki zupełnej funkcji stanu przedstawia różnicę wartości funkcji w stanach odpowiadających granicom całkowania. Z zasady, że wartość funkcji stanu nie zależy od historii układu wynika, że całka oznaczona tej funkcji obliczona dla dowolnego zbioru przemian, które tworzą powtarzalny cykl jest równa 0.
W praktyce stosuje się zwykle następujące funkcje stanu:
Funkcje stanu są najczęściej wielkościami konceptualnymi, tj. takimi których nie możemy bezpośrednio zmierzyć i dla których określenia konieczna jest pewna procedura zawierająca różne założenia i konwencje.

</doc>
<doc id="1543" url="https://pl.wikipedia.org/wiki?curid=1543" title="Friedrich Paulus">
Friedrich Paulus

Friedrich Wilhelm Ernst Paulus (ur. 23 września 1890 w Guxhagen, zm. 1 lutego 1957 w Dreźnie) – niemiecki feldmarszałek. Dowódca przeprowadzonych na dużą skalę strategicznych symulacji działań na froncie wschodnim II wojny światowej. Współtwórca operacji Barbarossa. Dowódca 6 Armii Polowej, która zapisała się na kartach historii, dzięki krwawym walkom w Stalingradzie. Pierwszy niemiecki feldmarszałek, który podczas działań II wojny światowej skapitulował i dał się wziąć do niewoli.
Młodość i okres przed II wojną światową.
Friedrich Wilhelm Ernst Paulus urodził się 23 września 1890 roku w Breitenau, części miasta Guxhagen. Mimo że potocznie mówi się o Paulusie jako o szlachcicu, dodając przydomek von, jest to błąd historyczny. Był synem biednego i mało znaczącego urzędnika. Gdy był młodzieńcem, nazywano go żartobliwie „lordem”. Później, wraz ze szlifami i nader wartościowymi osiągnięciami, zyskał przydomek „majora z seksapilem”. Było to związane z faktem, że Paulus świetnie się prezentował, był szarmancki i kurtuazyjny. Dbał o elegancję i higienę (ponoć czasami zmieniał bieliznę osobistą 6 razy dziennie) oraz zwracał szczególną i wyjątkową uwagę na prawidłową postawę ciała, dzięki czemu zawsze poruszał się z gracją. Uważał, iż tylko w ten sposób może choć powierzchownie dorównać swym rówieśnikom z lepszych rodzin.
Pomimo trudności finansowych rodziców Paulus uczył się w gimnazjum w Kassel, gdzie w 1909 roku otrzymał świadectwo dojrzałości.
Po nieudanych staraniach o przyjęcie do Marynarki Wojennej, najbardziej elitarnej części armii cesarskiej, młody Paulus zdecydował się na studia uniwersyteckie. Rozpoczął naukę na wydziale prawa na Uniwersytecie Filipa w Marburgu. Uczył się bardzo dobrze i pilnie, gdyż sądził, iż tylko ciężką pracą można osiągnąć dobrą pozycje w hierarchii społecznej. W marcu 1910 roku, skończywszy jeden semestr studiów, wstąpił ochotniczo jako szeregowy do 3 Badeńskiego Pułku Piechoty w Rastatt.
W niedługim czasie awansował na stopień chorążego. Wkrótce też został skierowany do szkoły wojskowej w Emgers, by w sierpniu 1911 roku otrzymać stopień podporucznika. Mając 22 lata, poślubił wywodzącą się z jednego z najznamienitszych rodów rumuńskich Elenę Rosetti-Solescu, siostrę kolegi z pułku. Jego letnia rezydencja mieściła się w Krośnie Odrzańskim.
Podczas I wojny światowej służył w Niemieckim Korpusie Alpejskim, w którym walczył m.in. na Bałkanach oraz pod Verdun. Osiągnął stopień kapitana. Jego służba miała głównie charakter sztabowy. Paulus zawsze przejawiał do niej największe predyspozycje i zdolności.
Po wojnie, w październiku 1922 roku, odesłano Paulusa do Berlina na specjalny kurs dla oficerów sztabu generalnego, zorganizowany przez Ministerstwo Reichswehry. Później młody oficer przeszedł jeszcze jeden kurs w Wyższej Szkole Technicznej. Po ukończeniu szkoleń w 1924 roku powrócił do służby, lecz nie na długo. W 1929 roku wrócił na uczelnię w okręgu Badenia-Wirtembergia, ale już w roli wykładowcy.
W dwudziestoleciu międzywojennym Paulus dowodził eksperymentalną jednostką pancerną. W 1938 roku został szefem sztabu XVI Korpusu Armijnego generała Heinza Guderiana.
II wojna światowa.
Krótko przed wybuchem II wojny światowej Friedrich Paulus objął dowództwo nad 4 Gruppenkommando. Podczas niemieckiego ataku na Polskę został szefem sztabu 10 Armii, która 26 października 1939 roku została przemianowana na 6 Armię Polową. Po udanej operacji „Fall Weiss” Paulus wziął również udział w ataku na Francję i kraje Beneluksu.
29 listopada 1940 roku w Berlinie Paulus – wówczas w stopniu generała majora – został promotorem ściśle tajnych, przeprowadzonych na wielką skalę symulacji działań strategicznych na froncie wschodnim.
W maju 1941 roku następnego awansował i został głównym kwatermistrzem, tj. pierwszym zastępcą szefa wojsk lądowych. Na tym stanowisku przygotowywał i wprowadzał korekty do planu wojny ze Związkiem Radzieckim, zwanego operacją Barbarossa.
Na ziemiach polskich, krótko przed atakiem III Rzeszy na ZSRR, stacjonował prawdopodobnie w Nowej Dębie.
Choć Paulus nie miał doświadczenia w dowodzeniu dużą formacją, wkrótce potem feldmarszałek Walter von Reichenau polecił go na stanowisko dowódcy 6 Armii, którym to Paulus nominalnie został w styczniu 1942 roku. Z tą armią Paulus odniósł szereg sukcesów, m.in. odpierając radziecką ofensywę na Charków w 1942 roku. W sierpniu 1942 roku zdecydowano, iż armia Paulusa zostanie skierowana do walk o zdobycie Stalingradu.
Pomimo wielokrotnie ponawianych ataków, miasto nie zostało ostatecznie zdobyte przez Niemców, chociaż zajęli oni około 90% jego terenu. Przeciwnie – z biegiem czasu 6 Armia została okrążona i niemal całkowicie odcięta od zaopatrzenia, które w niewystarczającym stopniu dostarczane było drogą powietrzną. Paulus zdawał sobie sprawę ze złego położenia swoich wojsk i był zwolennikiem wyrwania się z okrążenia, mając równocześnie świadomość, że będzie się to wiązało z utratą znacznej części ciężkiego sprzętu. Oddziały pancerne pod dowództwem generała Walthera von Seydlitz-Kurzbacha miały przerwać pierścień okrążenia. Adolf Hitler zakazał jednak jakichkolwiek działań odwrotowych, obiecując zaopatrzenie 6 Armii z powietrza, a następnie przywrócenie połączenia z resztą wojsk w wyniku równoczesnego ataku z zewnątrz i wewnątrz kotła.
W bardzo ciężkich zimowych warunkach, przy ostrym mrozie i zamieci śnieżnej, oddziały pod dowództwem feldmarszałka Ericha von Mansteina próbowały 12 grudnia uderzeniem z zewnątrz przerwać okrążenie. Operacja odsieczy z zewnątrz nosiła kryptonim "Wintergewitter" (Burza zimowa). Wyrwanie się z okrążenia wojsk w kotle i wyjście odsieczy naprzeciw nosiło kryptonim "Donnerschlag" (Uderzenie pioruna). Von Manstein odebrał rozkaz Hitlera w ten sposób, iż ma otworzyć korytarz, przez który wydostanie się 6 Armia i przeprowadzona zostanie ewakuacja. Z drugiej strony Paulus dostał rozkazy, które mówiły o utrzymaniu miasta za wszelką cenę. Von Manstein liczył na równoczesne ze swoim uderzenie Paulusa i połączenie obu armii. Jednak Paulus odmawiał rozpoczęcia akcji bez rozkazu Hitlera i padł tym samym ofiarą własnego posłuszeństwa. Rozkazy Hitlera nie nadeszły nigdy. Pomimo lokalnych sukcesów, próby przebicia się przez okrążenie nie powiodły się. W pewnym momencie obie armie dzieliło tylko kilkanaście kilometrów bronionych przez oddziały sowieckie. Cały czas próbowano dostarczać zaopatrzenie drogą powietrzną, lecz wbrew zapewnieniom głównego dowódcy Luftwaffe, Hermanna Göringa, nie zdołano tą drogą (głównie z powodu braku lotnisk i dostatecznej liczby samolotów transportowych oraz niezwykle ciężkich warunków atmosferycznych) należycie zaopatrzyć wojska Paulusa. W efekcie brakowało żywności, materiałów pędnych oraz amunicji.
Nieudane próby przerwania blokady, brak właściwego zaopatrzenia oraz potęgujące się bombardowania i ataki wojsk sowieckich spowodowały, że wojsko Paulusa znalazło się w tragicznym położeniu. Sytuację tę obrazują słowa Paulusa, które wypowiedział do jednego ze swych oficerów: 
Adolf Hitler kategorycznie zakazał jednak kapitulacji i rozkazał bronić miasta, choćby do ostatniego żołnierza. Oprócz obsesyjnego uporu Hitlera przedłużanie się walk wiązało znaczne siły sowieckie, które nie mogły zostać użyte w działaniach ofensywnych na innych kierunkach. W tym kontekście poświęcenie 6 Armii miało istotne znaczenie strategiczne i prawdopodobnie zapobiegło załamaniu się całego frontu południowego.
30 stycznia 1943 roku, Paulus został awansowany przez Hitlera do stopnia feldmarszałka. Adolf Hitler liczył na to, że Paulus nie skapituluje i popełni samobójstwo, bowiem od 1871 roku żaden niemiecki feldmarszałek nie dał się żywcem wziąć do niewoli. W mniemaniu Hitlera samobójstwo Paulusa stanowiłoby symbol nieustępliwości wojsk niemieckich. Paulus nie zamierzał odebrać sobie życia i 31 stycznia 1943 roku wraz ze 100 000 żołnierzy bezwarunkowo poddał się dowódcy 64 Armii – generałowi Michaiłowi Szumiłowowi. Jeszcze tego samego dnia został po raz pierwszy przesłuchany w sztabie 64 Armii, mieszczącym się w domu w Bekietowce. Po przesłuchaniach, w marcu 1943 roku, wraz z innymi wyższymi oficerami Wehrmachtu został zesłany do łagru Krasnogorodskij Nr 27. Hitler, gdy dowiedział się o kapitulacji Paulusa i jego oddaniu się w ręce sowieckie, wpadł we wściekłość, mówiąc, że kobieta pozbawiona godności potrafi popełnić samobójstwo, a nie ma do tego odwagi feldmarszałek niemiecki. Równocześnie ogłosił, że do końca wojny nikogo już nie mianuje feldmarszałkiem.
Przebywającego w więzieniu Paulusa Wilhelm Pieck, późniejszy prezydent NRD, początkowo bezskutecznie próbował nakłonić do wstąpienia do Komitetu Narodowego Wolne Niemcy ("Nationalkomitee Freies Deutschland", "NKFD"), współzałożonej przez siebie w ZSRR antynazistowskiej organizacji. Pod naciskiem Paulus w końcu zgodził się dołączyć do NKFD, a 8 sierpnia 1944 roku podpisał apel do narodu niemieckiego, nawołujący do wyrzeczenia się Hitlera. Później zaczął nadawać w sowieckim radiu programy propagandowe, w których atakował Hitlera i reżim nazistowski oraz wzywał niemieckich żołnierzy do tego, aby raczej poddawali się Sowietom, niż umierali za "führera". Jego postawa spotkała się z oburzeniem wielu niemieckich jeńców wojennych.
Po II wojnie światowej.
Po zakończeniu wojny Friedrich Paulus pozostał w Związku Radzieckim. Na początku 1946 roku przewieziono go do Niemiec, aby zeznawał jako świadek w procesie niemieckich zbrodniarzy wojennych przed Międzynarodowym Trybunałem Wojskowym w Norymberdze. Zrelacjonował wówczas swoją rolę w przygotowaniu operacji Barbarossa, a jako jej głównych architektów wskazał Wilhelma Keitla, Alfreda Jodla i Hermanna Göringa. Chociaż swoimi zeznaniami spełnił oczekiwania radzieckich „opiekunów”, ci odmówili mu spotkania z ciężko chorą wówczas żoną. Zmarła w 1949 roku nie zobaczywszy już więcej męża.
Na zwolnienie z radzieckiej niewoli Paulus musiał czekać do 1953 roku. We wrześniu tego roku do ZSRR przybył przewodniczący Rady Państwa NRD Walter Ulbricht celem omówienia zwolnienia feldmarszałka, zaś 26 października Paulus przyjechał pociągiem do Berlina Wschodniego. Na miejscu został zabrany na oficjalne przyjęcie kierownictwa państwowego i partyjnego NRD. Jako miejsce zamieszkania przydzielono mu willę w drezdeńskiej dzielnicy Oberloschwitz. Otrzymał także przywilej posiadania własnej broni krótkiej i zachodnioniemieckiego samochodu Opel Kapitän. W NRD Paulus pozostawał pod pełną obserwacją Państwowej Służby Bezpieczeństwa, jego korespondencja pocztowa była sprawdzana, zaś telefon i miejsce zamieszkania podsłuchiwane.
Krótko po przyjeździe Paulusa do NRD prasa w RFN spekulowała, czy feldmarszałek będzie uczestniczyć w tworzeniu wojsk lądowych NRD, które wówczas funkcjonowały zakamuflowane jako Kasernierte Volkspolizei (KVP; pol. „Skoszarowana Policja Ludowa”). W rzeczywistości Paulus tylko doradzał jej dowódcom. Został też szefem instytutu historycznego przy akademii KVP w Dreźnie. Na zlecenie władz próbował jednoczyć żyjących w obydwu państwach niemieckich weteranów Wehrmachtu do walki przeciw wstąpieniu RFN do NATO. Plan ten, z uwagi na to, że Paulus na Zachodzie był skompromitowany kolaboracją z Sowietami nie przyniósł spodziewanych rezultatów.
W pewnym momencie Paulus wycofał się z życia publicznego ze względów zdrowotnych – cierpiał na stwardnienie zanikowe boczne. Zmarł w swojej willi 1 lutego 1957 roku nie dokończywszy naukowego opracowania bitwy stalingradzkiej. Został pochowany z wojskowymi honorami na cmentarzu w Dreźnie-Tolkewitz. W późniejszych latach jego ciało zostało sprowadzone do RFN i pochowane obok zwłok żony w Baden-Baden.
Upamiętnienie formacji, której jeńcem był Paulus.
Friedricha Paulusa wzięła do niewoli 1. kompania 255 Wołgogradzko-Korsuńskiego Pułku Zmechanizowanego Gwardii. Pułk ten po II wojnie światowej stacjonował na terytorium Polski, w Pstrążu. Na budynku 1. kompanii widniała pamiątkowa tablica o treści:
"1-a kompania 255 PZ Gw. w Stalingradzie wzięła do niewoli feldmarszałka Paulusa". Tablica ta nie zachowała się po opuszczeniu Polski przez Armię Radziecką.

</doc>
<doc id="1544" url="https://pl.wikipedia.org/wiki?curid=1544" title="Fotografia">
Fotografia

Fotografia (ang. "photography") – zbiór wielu różnych technik, których celem jest zarejestrowanie trwałego, pojedynczego obrazu za pomocą światła. Potoczne znaczenie zakłada wykorzystanie układu optycznego, choć nie jest to konieczne np. przy rayografii.
Etymologia.
Słowo fotografia (ang. "photography") oznacza rysowanie za pomocą światła. Powstało z greckich rdzeni φῶς ("phōs") - światło, dopełniacz φωτός ("phōtós") - światła; γραφή ("graphé") - rysowanie.
Podstawowy podział technik fotograficznych.
Naukowców od zawsze fascynowała możliwość rejestrowania dokładnych obrazów ruchu, przykładem tego jest twórczość Eadwearda Muybridge’a.
Ważniejsze wydarzenia w historii fotografii opartej o materiały światłoczułe.
Fotografia, powstała w XIX wieku, korzystała z wynalazków znanych już wcześniej. Przykładem jest camera obscura, znana już w starożytnej Grecji.
Technika fotografii rozwijała się od tego momentu bardzo szybko. Już wkrótce płytki srebrne lub arkusze papieru zastąpiono płytkami szklanymi powleczonymi warstwą światłoczułą. Płytki te preparowano tuż przed wykonaniem zdjęcia, co wymagało przenośnej ciemni fotograficznej, w której można było dokonać tego zabiegu.
Fotografia w Polsce.
Fotografia pojawiła się na ziemiach polskich równolegle do osiągnięć na zachodzie Europy na tym polu. Już 13 lipca 1839 roku, a więc jeszcze przed ujawnieniem szczegółów techniki dagerotypii, polski inżynier i wynalazca Maksymilian Strasz opublikował w „Wiadomościach Handlowych i Przemysłowych” artykuł poświęcony innej technice fotograficznej: kalotypii dołączając próbki wykonanych przez siebie obrazów wykonanych tą techniką. W tym samym roku "Kurier Warszawski" wydrukował ogłoszenie firmy Fraget oferującej aparaty do dagerotypii. Rok później w Poznaniu i w Warszawie pojawiły się ilustrowane broszury poświęcone dagerotypii.
Pierwszy profesjonalny zakład fotograficzny na ziemiach polskich „Zakład Daguerrotypowy Karola Beyer w Warszawie” na przełomie lat 1844–1845 otworzył w Warszawie Karol Beyer. Początek przemysłu fotograficznego na ziemiach polskich to rok 1887, kiedy chemik Piotr Lebiedziński założył w Warszawie fabrykę papierów fotograficznych FOTON. W 1899 roku, również w Warszawie, powstała wytwórnia aparatów fotograficznych i obiektywów FOS.
Fotografia tradycyjna a analogowa.
Bardzo często fotografię "tradycyjną" (opartą na wykorzystaniu światłoczułych własności halogenków srebra) kompletnie mylnie określa się fotografią "analogową". Jest to poważny błąd merytoryczny, gdyż fotografią analogową można określić proces reprodukcji obrazu z pomocą urządzeń elektronicznych analogowych (np. fotopowielacz). Wynikiem działania światła na taki element światłoczuły jest powstanie sygnału analogowego, a nie efektu fotochemicznego na błonie filmowej czy innym nośniku emulsji światłoczułej.

</doc>
<doc id="1545" url="https://pl.wikipedia.org/wiki?curid=1545" title="Falki">
Falki

Falki () – rodziny funkcji zbioru liczb rzeczywistych w zbiorze liczb rzeczywistych, z których każda jest wyprowadzona z funkcji-matki (z tzw. "funkcji macierzystej") za pomocą przesunięcia i skalowania:
gdzie:
Funkcje te dążą do zera (lub po prostu wynoszą zero poza pewnym przedziałem) dla argumentu dążącego do nieskończoności, zaś ich suma ważona umożliwia przedstawienie z dowolną dokładnością dowolnej funkcji ciągłej całkowalnej z kwadratem, podobnie jak funkcje cosinus o różnych okresach i przesunięciach umożliwiają przedstawienie z dowolną dokładnością każdej całkowalnej funkcji okresowej (zob. transformata Fouriera).
Falki są używane w analizie i przetwarzaniu sygnałów cyfrowych, w kompresji obrazu i dźwięku, do rozwiązywania równań różniczkowych cząstkowych oraz w wielu innych dziedzinach. Najprostsze z nich to falki Haara.

</doc>
<doc id="1546" url="https://pl.wikipedia.org/wiki?curid=1546" title="Falki Haara">
Falki Haara

Falka Haara – pierwsza znana falka, została wprowadzona przez Alfréda Haara w 1909 lub 1910 r. Jest to szczególnie prosta falka, jej funkcja-matka określona jest wzorem:
Falka ta ma zwarty nośnik formula_2 jednak wadą jest jej nieciągłość, a więc nieróżniczkowalność, co w niektórych zastosowaniach ma znaczenie. Falki te są stosowanie w kompresji obrazów i dźwięku (kompresja falkowa).

</doc>
<doc id="1549" url="https://pl.wikipedia.org/wiki?curid=1549" title="Faraon">
Faraon

Faraon () – jedno z określeń władcy starożytnego Egiptu, będące zniekształconą wersją staroegipskiego "Per-āa" – „wielki dom”. Od XVIII dynastii termin ten zaczęto odnosić również do zarządcy tego „wielkiego domu” (pałacu), czyli władcy.
Do najwybitniejszych faraonów należeli: Chefren, Totmes III, Seti I, Ramzes II. Jedynie w przypadku czterech kobiet władczyniom przyznano godność i funkcję faraonów: Nitokris, Neferusobek, Hatszepsut i Tauseret. 
Faraon miał przynajmniej dwa imiona wpisane w kartusz: noszone od urodzenia oraz imię koronacyjne. Tworzyły one królewską tytulaturę, czyli tzw. Królewski Protokół.
Geneza nazwy i tytulatura.
Termin ten pojawił się w XVI wieku p.n.e., gdy starożytny Egipt wkroczył w okres silnej ekspansji terytorialnej pod rządami XVIII dynastii. Wywodzi się od staroegipskiego "Per-āa," dosłownie oznaczającego „wielki dom”. Początkowo było to określenie siedziby (pałacu) władcy, z czasem jednak zaczęło oznaczać jego samego. 
Po zjednoczeniu kraju przez Narmera egipski władca nosił tytuł "nesut-biti" – "Ten-który-należy-do-pszczoły-i-trzciny", bowiem w tym okresie godłem Górnego Egiptu była pszczoła, a Dolnego – trzcina. Z biegiem czasu zaczęto używać tytułu "Król Górnego i Dolnego Egiptu" oraz "Król Południa i Król Północy".
Każdy z faraonów oficjalnie nosił pięć tzw. Wielkich Imion. Trzy pierwsze z nich do czasów XII dynastii mogły brzmieć identycznie. Dwa ostatnie zapisywano w kartuszach, czyli w wyróżniającym otoku. Są to kolejno:
W Egipcie rzymskim okresu cesarstwa w greckojęzycznym zapisie oficjalnym stosowano dla władcy standardową tytulaturę "Autokrator Kaisar Sebastos" (Αύτοκράτορος Καίσαρος Σεβαστός), będącą dokładnym odpowiednikiem rzymsko-łacińskiej "Imperator Caesar Augustus".
Insygnia władzy.
Od poddanego mu Egipcjanina władcę odróżniały regalia – pewne oznaki ściśle zastrzeżone dla godności faraona. Egipski władca Egiptu był też specyficznie przedstawiany w sztuce: zwykle jako nierównie większy od innych postaci, występujący najczęściej w otoczeniu bogów lub w scenach zwycięstw wojennych nad nieprzyjaciółmi Egiptu.
Rola w państwie.
Faraon był „bogiem na ziemi”, czyli niekwestionowanym królem; należał do niebios i był pośrednikiem pomiędzy ludem a panteonem. Faraon to wcielenie "Pana Niebios" i żyznej ziemi Horusa, przedstawianego w postaci sokoła z rozpostartymi skrzydłami, a później również synem boga Re. Po śmierci faraona stawał się Ozyrysem. Mocno nawiązywało to do mitologii egipskiej, w której Horus władał ziemią, a Ozyrys światem zmarłych. Jak stwierdził wezyr Rechmire z czasów Totmesa III: "każdy król Górnego i Dolnego Egiptu jest bogiem, dzięki wskazaniom którego możliwe jest życie, ojcem i matką wszystkich ludzi i nie ma sobie równych".
Pierwszym królem noszącym tytuł "syna Re" był następca Cheopsa, Dżedefre z IV dynastii. Egipski władca pełnił funkcję najwyższego kapłana, sędziego, wodza naczelnego i budowniczego świątyń. Stał na straży harmonii i porządku – Maat, co było jego głównym i najważniejszym zadaniem.
Boskość osoby króla wymagała ścisłego przestrzegania jego nietykalności. Do roli kapłanów, wskutek tego poddawanych częstym ceremoniom oczyszczającym, należało uniemożliwianie mu fizycznego kontaktu z ludźmi i przedmiotami. W odniesieniu do osoby władcy obowiązywało też odpowiednie słownictwo: on sam w wypowiedziach używał formy „Mój Majestat”, podobnie inni zwracali do „Jego Majestatu”, a wobec jego czynności używano oprócz trzeciej osoby także często formy bezosobowej (np. „wydano polecenie”).
Faraon corocznie uczestniczył w wielkim Święcie Opet – procesji barek bogów z Karnaku do Luksoru, podczas którego dokonywało się odnowienie jego władzy. Ponadto jako jedyny pośrednik między ludźmi a bogami, miał codzienny obowiązek odprawiania rytuałów w świątyni. Król miał budzić posąg boga, obmywać go i składać ofiary. W rzeczywistości jednak często zastępowali go kapłani.
Praktycznie władza faraona była realizowana poprzez urzędników, którzy organizowali i nadzorowali prace przy budowie i naprawie kanałów nawadniających, zbierali daniny od chłopów i rzemieślników, określali rodzaj i wielkość upraw, w imieniu faraona sądzili przestępców. Ustrój państwa oparty na władzy faraona określić można jako monarchię despotyczną opartą na teokracji.
W okresie rzymskim żaden z aktualnie panujących cesarzy nie koronował się oficjalnie jako faraon, choć w dekoracjach sztuki świątynnej często występują w stroju faraonów w scenach składania ofiar bogom egipskim. Łączyło się to ze zmianą roli i upadkiem znaczenia roli kolegiów kapłańskich poddanych rzymskiej kontroli, których poparcie dla władzy cesarskiej było zbędne. W Egipcie stanowiącym od czasów Augusta wydzieloną prowincję cesarską (gdzie senatorowie nie mieli prawa wstępu bez specjalnego zezwolenia), faktyczną władzę Rzymu reprezentował w Aleksandrii namiestnik – mianowany spośród ekwitów prefekt Egiptu ("praefectus Aegypti", później "dux Aegypti") jako "praefectus Augustalis", tj. sprawujący ją w imieniu samego cesarza.
Święto Sed.
Najważniejszym świętem królewskim był "sed". Uroczystość miała charakter rytualnej odnowy sił króla: wierzono, że z wiekiem zarówno pierwiastek ludzki, jak i boski we władcy mógł osłabnąć i święto miało na celu zaradzeniu temu. Święto nazywane jest dzisiaj czasem "jubileuszem królewskim". Początkowo odbywało się po prostu po długim okresie panowania władcy, ostatecznie jednak przyjęto, że powinno się odbywać co najmniej raz na trzydzieści lat. Przebieg rytuału nie jest do końca jasny. Prawdopodobnie zawierał ponowną koronację i ponowne objęcie władzy nad Górnym i Dolnym Egiptem. W trakcie święta faraon rodził się na nowo jako istota „w pełni boska”. Nie było to jednak stałe, stąd powtarzalność rytuału.
W niektórych źródłach święto to nosi nazwę Opet.
Polityka budowlana.
Działalność architektoniczna była ważnym elementem składowym rządów faraona. Poprzez tworzenie nowych budowli pokazywał on swoją troskę o losy świata, spełniając obowiązek wobec ludzi, jak i bóstw. Stawianie kolejnych monumentalnych budowli miało podtrzymywać na świecie "maat". W świątyni Hatszepsut zachowały się słowa, jakie rzekomo miał wypowiedzieć do niej Amon: "kiedy jeszcze byłaś jako ta, która jest w swoim gnieździe (w wieku młodzieńczym), wiedziałem, że uczynisz dla mnie monumenty i wypełnisz dla mnie moją świątynię wszystkim tym, co jest dobre z Obu Krajów".
Wznosząc budowle król potwierdzał swe prawo do tronu. Jego dzieła miały być zawsze potężniejsze, niż stworzone przez jego poprzedników. Faktycznie jednak to, jakie budowle powstawały było zależne od sytuacji w państwie. W bogatych okresach powstawały wielkie dzieła, a w trakcie Okresów Przejściowych tworzenie monumentów nie było dla władców kluczowe.
Od czasów Średniego Państwa faraonowie nakazywali umieszczać na ścianach dedykacje, wskazujące kto i dla kogo stworzył dany budynek. Z czasem zaczęły się tam pojawiać inne informacje, dzięki czemu obecnie dość dobrze wiadomo, w jakim celu powstała część z zachowanych budowli.
W powszechnym odbiorze cesarze rzymscy, podobnie jak władcy ptolemejscy i perscy, nadal spełniali utrwaloną tradycją rolę faraonów, m.in. prowadząc przypisaną im sakralną działalność budowlaną.

</doc>
<doc id="1550" url="https://pl.wikipedia.org/wiki?curid=1550" title="Perspektywa pierwszej osoby">
Perspektywa pierwszej osoby

Perspektywa pierwszej osoby, perspektywa pierwszoosobowa, skrótowiec: FPP (od ) – określenie perspektywy graficznej w grach komputerowych, w których gracz widzi świat oglądany oczami głównego bohatera.
Najczęściej gry tego typu utożsamiane są z gatunkiem strzelanek pierwszoosobowych, jednak perspektywa ta występuje również, między innymi, w komputerowych grach przygodowych ("Myst"), fabularnych ("The Elder Scrolls", "Deus Ex") i wyścigowych ("Colin McRae Rally") oraz symulatorach lotu ("Il-2 Sturmovik"). W wielu grach istnieje możliwość wyboru między perspektywą pierwszoosobową a trzecioosobową.

</doc>
<doc id="1551" url="https://pl.wikipedia.org/wiki?curid=1551" title="Frankonia">
Frankonia

Frankonia (niem. "Franken") – kraina historyczna w południowych Niemczech.
Obecnie Frankonia stanowi część Bawarii (trzy rejencje: Górna, Środkowa i Dolna Frankonia), Badenii-Wirtembergii (region Heilbronn-Franken) i Turyngii (powiaty Schmalkalden-Meiningen, Hildburghausen, Sonneberg, Wartburg).
Największe miasta Frankonii to Norymberga, Würzburg, Fürth.
W średniowieczu Frankonia była jednym z pięciu księstw niemieckich, obok Saksonii, Lotaryngii, Szwabii i Bawarii.
Frankończycy są szczególnie dumni z ich piwa. Górna Frankonia posiada największe skupisko browarów w Niemczech. W Górnej Frankonii przetrwała tradycja domowego wyrobu i wyszynku piwa znanego jako zoigl. Także frankońskie białe wino ("Frankenwein") ma długą tradycję i jest produkowane przede wszystkim z odmian müller-thurgau, sylvaner i bacchus.
Turystyka.
Przez Frankonię wzdłuż Menu biegnie Mainradweg – turystyczna trasa rowerowa oceniona pięcioma gwiazdkami jakości przez Powszechny Niemiecki Klub Rowerowy. Ma długość około 600 kilometrów. Zaczyna się w okolicach Bayreuth, prowadzi m.in. przez Bamberg, Würzburg, Aschaffenburg, Frankfurt nad Menem, aż do ujścia Menu do Renu koło Moguncji.

</doc>
<doc id="1552" url="https://pl.wikipedia.org/wiki?curid=1552" title="Flisz (geologia)">
Flisz (geologia)

Flisz – zespół skał terygenicznych cechujący się wielokrotną cyklicznością.
Klasyczny cykl (tzw. sekwencja Boumy) rozpoczyna zlepieniec, powyżej leży piaskowiec, następnie mułowiec, a najwyżej w profilu iłowiec. Flisz powstaje na skutek schodzenia po stoku kontynentalnym podwodnych prądów zawiesinowych i osuwisk oraz segregowania grawitacyjnego ziarn w czasie tego transportu. Prawie zawsze flisz tworzył się w głębokim basenie morskim, u podnóża stoku kontynentalnego, ale znane są też sekwencje fliszowe w głębokich jeziorach.
W zarzuconym dziś ujęciu teorii geosynklin, fliszem nazywano serię osadów morskich powstającą w wyniku wypełnienia geosynkliny, na krótko przed orogenezą. Dziś taka definicja fliszu ma znaczenie tylko historyczne.
Skamieniałości fliszowe należą do rzadkości i są reprezentowane przede wszystkim przez mikrofaunę (np. otwornice) i bardzo liczne miejscami skamieniałości śladowe.
W skład skał fliszowych mogą wchodzić:
Rodzaje fliszu.
Ze względu na oddalenie od obszaru źródłowego materiału detrytycznego wyróżniamy:
Ze względu na skład petrograficzny wyróżniamy:
Ze skał fliszowych zbudowane są m.in. zewnętrzne pasma Alp i Karpat (Karpaty Zewnętrzne, zwane też fliszowymi – tzw. flisz karpacki).

</doc>
<doc id="1553" url="https://pl.wikipedia.org/wiki?curid=1553" title="Firet">
Firet

Firet – pomocnicza jednostka miary stosowana w typografii. Wartość firetu równa 1 odpowiada aktualnie używanemu stopniowi pisma. Wyrażana jest jako kwadrat o boku równym danemu stopniowi pisma.
Mówiąc krócej, firet to bieżący stopień pisma. Jeżeli np. dany fragment tekstu jest składany czcionką 12-punktową, to jeden firet ma też 12 punktów. Jeżeli teraz ten tekst zostanie przeformatowany na 14 punktów, to jeden firet pozostanie jednym firetem, ale będzie teraz wynosił 14 punktów.
Firet jest wygodną, powszechnie stosowaną jednostką, służącą do określania parametrów składu, głównie odstępów. Na przykład w typowym składzie dziełowym zaleca się, aby wcięcie akapitowe wynosiło od 1 do 1,5 firetu.
W typowych krojach pisma, szczególnie w ich odmianach podstawowych, szereg znaków ma utrwalone tradycją typowe rozmiary. Na przykład:
Wszystkie powyższe wielkości odnoszą się do pojęcia szerokości pola znaku, czyli sumy szerokości glifu (oczka) wraz z obiema odsadkami (światłami), co w tradycyjnym zecerstwie odpowiada szerokości płaszczyzny czołowej czcionki.
W informatyce symbolem firetu jest [em]. Jednostkę tę można spotkać przy podawaniu wielkości np. w kodzie stron HTML czy w składni LaTeX.
Nazwa symbolu jednostki pochodzi od szerokości wielkiej litery „M”, która obok litery „W” jest jedną z dwóch najszerszych liter alfabetu. Stąd również spacja firetowa jest nazywana em-spacją, a na zasadzie analogii spacja półfiretowa – en-spacją. Zazwyczaj jednak w krojach pisma szerokości te są trochę mniejsze od firetu czy połowy firetu.
W Unikodzie istnieje szereg spacji o ściśle zdefiniowanej szerokości, są to spacje: firetowa, półfiretowa, trzecianka (1/3), kwarcianka (1/4), jedna szósta, chuda (1/8) oraz włoskowa (1/24). Istnieje nawet kilka spacji o zerowej wartości firetu (posiadających różne inne właściwości, np. rozdzielania lub łączenia sąsiednich znaków). Wartość standardowej spacji jest zbliżona do 1/5 firetu, aczkolwiek jest ona rozciągliwa (justowalna).

</doc>
<doc id="1554" url="https://pl.wikipedia.org/wiki?curid=1554" title="Frédéric Joliot-Curie">
Frédéric Joliot-Curie

Jean Frédéric Joliot-Curie, początkowo Jean Frédéric Joliot (ur. 19 marca 1900 w Paryżu, zm. 14 sierpnia 1958 tamże) – francuski fizyk, laureat Nagrody Nobla w dziedzinie chemii.
Życiorys.
Ukończył "École de physique et chimie" w Paryżu i w 1925 r. zaczął pracować jako asystent Marii Skłodowskiej-Curie w Instytucie Radowym. W 1926 roku poślubił jej córkę, Irène Curie, a niedługo po tym oboje zmienili nazwisko na Joliot-Curie. Wraz z żoną odkrył zjawisko tworzenia par elektron-pozyton (pozytonium) z fotonów; w 1934 r. wspólnie odkryli i badali zjawisko sztucznej promieniotwórczości.
Za te odkrycia zostali wspólnie uhonorowani Nagrodą Nobla z chemii w 1935 r.Później w "Collège de France" pracował nad reakcją łańcuchową i kontrolowaną reakcją nuklearną z użyciem radu i ciężkiej wody. Po inwazji hitlerowskich Niemiec udało mu się wyniki swoich badań przemycić do Anglii.
Brał czynny udział we francuskim ruchu oporu. Po wojnie rozpoczął pracę we francuskim ośrodku badań jądrowych w Orsay, następnie został dyrektorem ośrodka. Był komunistą, od 1942 r. członkiem Francuskiej Partii Komunistycznej. W 1950 r. odsunięty od prac w Orsay, pracował jednak dalej w "Collège de France", a po śmierci żony w 1956 objął katedrę fizyki na Sorbonie. Od 1950 r. do śmierci pełnił funkcję prezydenta Światowej Rady Pokoju. Sygnatariusz apelu sztokholmskiego z 1950 roku. 
Członek wielu prestiżowych towarzystw naukowych. W 1946, pełniąc wówczas stanowisko komisarza do spraw energii atomowej, został odznaczony Komandorią Legii Honorowej. Od 1954 r. był członkiem zagranicznym Polskiej Akademii Nauk. W roku 1950 otrzymał Międzynarodową Stalinowska Nagrodę Pokoju.
Irène i Frédéric Joliot-Curie mieli dwoje dzieci: Hélène i Pierre’a.
Imieniem Frédérica Joliot-Curie nazwano Złoty Medal Pokoju.

</doc>
<doc id="1557" url="https://pl.wikipedia.org/wiki?curid=1557" title="Funkcja holomorficzna">
Funkcja holomorficzna

Funkcja holomorficzna – funkcja zespolona na otwartym podzbiorze płaszczyzny liczb zespolonych (formula_1), która jest różniczkowalna w sensie zespolonym w każdym punkcie tego podzbioru. Funkcje holomorficzne to główny obiekt badań analizy zespolonej.
Holomorficzność funkcji jest warunkiem dużo silniejszym niż różniczkowalność w sensie rzeczywistym, gdyż funkcja o tej własności jest nieskończenie wiele razy różniczkowalna, przez co może być przedstawiona za pomocą wzoru (szeregu) Taylora.
Nomenklatura.
Słowo „holomorficzny” zostało wprowadzone przez dwóch studentów Cauchy’ego, Briota (1817–1882) oraz Bouqueta (1819–1895), i pochodzi od greckiego ὅλος ("holos") oznaczającego „całość” oraz μoρφń ("morfe") oznaczającego „kształt”, „wygląd”.
Często, wymiennie z terminem „funkcja holomorficzna”, stosuje się również nazwę funkcja analityczna, jednak jest ona także używana w szerszym sensie – funkcji (rzeczywistej, zespolonej lub ogólniejszego typu), która jest równa swojemu rozwinięciu w szereg Taylora w dowolnym punkcie swojej dziedziny. Nietrywialny fakt, że klasa funkcji analitycznych pokrywa się z klasą funkcji holomorficznych jest istotnym twierdzeniem analizy zespolonej. W związku z tym wielu matematyków przedkłada termin „funkcja holomorficzna” nad „funkcja analityczna”, choć ten drugi nadal jest szeroko rozpowszechniony. O funkcjach holomorficznych mówi się także, że są regularne (zob. funkcja regularna), z kolei funkcje, które nie są holomorficzne, nazywa się czasem osobliwymi.
Funkcję, która jest holomorficzna na całej płaszczyźnie zespolonej nazywa się funkcją całkowitą ("całkowitość" oddaje tu „całość”, dlatego funkcji tej nie należy mylić z funkcją określoną w liczbach całkowitych). Z kolei wyrażanie „holomorficzna w punkcie formula_2” oznacza funkcję nie tylko różniczkowalną w punkcie formula_3 ale różniczkowalną wszędzie wewnątrz pewnego otwartego koła o środku w formula_2 na płaszczyźnie zespolonej.
Definicja.
Niech formula_5 będzie otwartym podzbiorem formula_6 zaś formula_7 będzie funkcją zespoloną określoną na formula_8 O funkcji formula_9 mówi się, że jest "różniczkowalna w sensie zespolonym" lub "ma pochodną zespoloną" w punkcie formula_10 jeżeli istnieje granica
którą nazywa się pochodną zespoloną funkcji formula_9 w punkcie formula_13
Powyższa granica jest wzięta po wszystkich ciągach "liczb zespolonych" zbiegających do formula_14 i dla wszystkich takich ciągów iloraz różnicowy ma zbiegać do tej samej liczby formula_15 Intuicyjnie, jeżeli formula_9 jest różniczkowalna w sensie zespolonym w formula_14 z kierunku formula_18 to obrazy będą zbiegać do punktu formula_19 z kierunku formula_20 gdzie ostatni iloczyn jest mnożeniem liczb zespolonych. To pojęcie różniczkowalności dzieli kilka wspólnych własności z różniczkowalnością w sensie rzeczywistym: jest liniowe i spełnia reguły iloczynu, ilorazu i łańcuchową.
Jeżeli formula_9 jest różniczkowalna w sensie zespolonym w "każdym" punkcie formula_10 to funkcję formula_9 nazywa się holomorficzną na formula_8 Funkcja formula_9 jest "holomorficzna w punkcie" formula_26 jeżeli jest holomorficzna w pewnym otoczeniu formula_13 Funkcja formula_9 jest "holomorficzna na" pewnym nieotwartym "zbiorze" formula_29 jeżeli jest holomorficzna na zbiorze otwartym zawierającym formula_30
Związek między różniczkowalnością w sensie rzeczywistym i w sensie zespolonym jest następujący:
Twierdzenie odwrotne nie jest prawdziwe. Prostym odwróceniem tego wyniku jest, że
Bardziej zadowalającym odwróceniem, które nastręcza więcej trudności przy dowodzie, jest twierdzenie Loomana-Menchoffa:
Własności.
Ponieważ różniczkowanie w sensie zespolonym jest liniowe i spełnia reguły iloczynu, ilorazu i łańcuchową, to sumy, iloczyny i złożenia funkcji holomorficznych są holomorficzne, a iloraz dwóch funkcji holomorficznych jest holomorficzny tam, gdzie mianownik jest różny od zera.
Utożsamienie formula_44 z formula_45 sprawia, że funkcje holomorficzne pokrywają się z tymi funkcjami dwóch zmiennych rzeczywistych o ciągłych pierwszych pochodnych, które są rozwiązaniami równań Cauchy’ego-Riemanna, układu dwóch równań różniczkowych cząstkowych.
Każda funkcja holomorficzna może być przedstawiona jako suma swoich części rzeczywistej i urojonej, a każda z nich jest rozwiązaniem równania Laplace’a na formula_46 Innymi słowy, jeżeli wyrazi się funkcję holomorficzną formula_47 jako formula_48 to tak formula_49 jak i formula_33 są funkcjami harmonicznymi.
Tam gdzie pierwsza pochodna nie zeruje się, funkcje holomorficzne są konforemne (równokątne) w tym sensie, iż zachowuje kąt i kształt (ale nie rozmiar) małych figur.
Wzór całkowy Cauchy’ego zapewnia, że każda funkcja holomorficzna wewnątrz pewnego koła jest całkowicie określona przez wartości na brzegu tego koła.
Każda funkcja holomorficzna jest analityczna. Oznacza to, że funkcja holomorficzna ma pochodne dowolnego rzędu w każdym punkcie formula_2 swojej dziedziny i pokrywa się ze swoim szeregiem Taylora względem punktu formula_2 w otoczeniu formula_53 Rzeczywiście, formula_9 pokrywa się ze swoim szeregiem Taylora względem formula_2 w dowolnym kole o środku w tym punkcie, które leży wewnątrz dziedziny tej funkcji.
Z algebraicznego punktu widzenia zbiór funkcji holomorficznych określonych na zbiorze otwartym jest pierścieniem przemiennym i zespoloną przestrzenią liniową. Rzeczywiście, jest to lokalnie wypukła przestrzeń liniowo-topologiczna, gdzie półnormami są suprema na podzbiorach zwartych.
Przykłady.
Holomorficzne na formula_44 są wszystkie funkcje wielomianowe zmiennej formula_57 o współczynnikach zespolonych, funkcja wykładnicza, a także funkcje trygonometryczne sinus i cosinus, które mogą być definiowane przez funkcje wykładniczą za pomocą wzoru Eulera. Ogólniej każdy szereg potęgowy o niezerowym promieniu zbieżności jest funkcją analityczną w swoim otwartym kole zbieżności.
Główna gałąź logarytmu zespolonego jest holomorficzna na zbiorze formula_58 Pierwiastek kwadratowy funkcji może być określony jako
i stąd jest on holomorficzny tam, gdzie holomorficzny jest logarytm formula_60
Funkcja formula_61 jest holomorficzna na zbiorze formula_62
Holomorficzna funkcja o wartościach rzeczywistych musi być stała, co jest konsekwencją równań Cauchy’ego-Riemanna. Stąd moduł liczby zespolonej formula_57 oraz argument liczby zespolonej formula_57 nie są holomorficzne.
Przypadek kilku zmiennych.
Zespolona analityczna funkcja kilku zmiennych zespolonych jest definiowana jako analityczna i holomorficzna w punkcie, jeżeli jest lokalnie rozwijalna (wewnątrz wielokoła/polidysku, iloczynu kartezjańskiego kół o środku w tym punkcie) jako zbieżny szereg potęgowy tych zmiennych. Warunek ten jest silniejszy niż równania Cauchy’ego-Riemanna; rzeczywiście, może być on również wyrażony następująco:
Uogólnienie w analizie funkcjonalnej.
Pojęcie funkcji holomorficznej może być rozszerzone na przestrzenie nieskończeniewymiarowe rozważane w analizie funkcjonalnej. Przykładowo pochodne Frécheta lub Gâteaux mogą być wykorzystane do zdefiniowania pojęcia funkcji holomorficznej na przestrzeni Banacha nad ciałem liczb zespolonych.

</doc>
<doc id="1558" url="https://pl.wikipedia.org/wiki?curid=1558" title="Funkcja (ujednoznacznienie)">
Funkcja (ujednoznacznienie)

Funkcja (łac. "functio, -onis", „odbywanie, wykonywanie, czynność”) – wyraz mogący oznaczać jedno z następujących pojęć:

</doc>
<doc id="1559" url="https://pl.wikipedia.org/wiki?curid=1559" title="Funkcja">
Funkcja

Funkcja ( „odbywanie, wykonywanie, czynność”) – dla danych dwóch zbiorów formula_1 i formula_2 przyporządkowanie każdemu elementowi zbioru formula_1 dokładnie jednego elementu zbioru formula_2. Oznacza się ją na ogół formula_5 itd.
Jeśli funkcja formula_6 przyporządkowuje elementom zbioru formula_1 elementy zbioru formula_8 to zapisujemy to następująco:
Zbiór formula_1 nazywa się dziedziną, a zbiór formula_2 – przeciwdziedziną funkcji formula_12 Zbiór wszystkich funkcji ze zbioru formula_1 do zbioru formula_2 oznacza się często formula_15.
Ponadto:
Wykres funkcji.
Wykresem funkcji formula_35 nazywa się zbiór formula_36 Z definicji funkcji wynika, że dla każdego formula_37 istnieje dokładnie jeden taki formula_38 że formula_39 Jeśli formula_40 jest funkcją ciągłą, to jej wykres jest krzywą w układzie współrzędnych na płaszczyźnie.
Wykres funkcji jednoznacznie ją określa. Jeśli formula_41 to formula_42 przy czym formula_43 jest jedynym takim elementem.
Definicja Peana funkcji (za pomocą wykresu).
W teorii mnogości często stosuje się następującą definicję funkcji, pochodzącą od Peana:
Faktycznie utożsamia się w niej funkcję z jej wykresem. Jest użyteczna w tworzeniu systemów aksjomatycznych pewnych teorii, bowiem funkcja jest wtedy pojęciem pochodnym względem aksjomatyki teorii mnogości.
Funkcje liczbowe.
Ważną klasą funkcji są funkcje
nazywane funkcjami o wartościach liczbowych.
W zbiorze funkcji liczbowych określonych na ustalonym zbiorze formula_1 można zdefiniować działania arytmetyczne:
Funkcja formula_6 jest ograniczona, jeśli istnieje taka liczba rzeczywista dodatnia formula_73 że dla każdego formula_52 spełniona jest nierówność formula_75
Jeśli funkcja liczbowa formula_6 przyjmuje jedynie wartości rzeczywiste
to nazywa się ją funkcją o wartościach rzeczywistych.
Dla funkcji o wartościach rzeczywistych wyniki powyżej zdefiniowanych czterech działań arytmetycznych są funkcjami o wartościach rzeczywistych. Wyjątkiem jest mnożenie przez stałą, która powinna być rzeczywista, aby w wyniku mnożenia funkcji o wartościach rzeczywistych przez tę stałą uzyskać funkcję o wartościach rzeczywistych.
Funkcjami liczbowymi nazywamy:
Można także mówić o funkcjach liczbowych wielu zmiennych (rzeczywistych lub zespolonych):
których dziedzina jest podzbiorem iloczynu kartezjańskiego zbioru liczb rzeczywistych lub zbioru liczb zespolonych, które zapisuje się:
Sposoby określania funkcji.
Jeżeli dziedzina formula_1 jest skończona, wystarczy wymienić wszystkie pary (argument, wartość). Można to zrobić za pomocą grafu (przykład obok).
Funkcje liczbowe można definiować za pomocą wzorów. Jest to sposób analityczny. W tym celu wykorzystuje się pewien zasób funkcji (wielomiany, funkcje elementarne itp.), działania algebraiczne, złożenie funkcji i operację przejścia do granicy (w tym operacje analizy matematycznej, takie jak różniczkowanie, całkowanie i sumowanie szeregów).
Klasa funkcji, które można przedstawić za pomocą szeregu (potęgowego, trygonometrycznego itp.) jest bardzo szeroka. Każdą funkcję elementarną można przedstawić za pomocą szeregu potęgowego zwanego szeregiem Taylora.
Przedstawić analitycznie funkcję można w sposób jawny, tzn. jako formula_18 lub jako tak zwaną funkcję uwikłaną, tzn. za pomocą równania formula_92.
Czasem funkcja jest dana kilkoma wzorami, na przykład:
Do określenia funkcji można też stosować metodę opisową. Na przykład funkcja Dirichleta jest funkcją, która dla argumentów wymiernych przyjmuje wartość 1, a dla argumentów niewymiernych 0.
Funkcja może na ogół być określona na wiele sposobów. Na przykład funkcję "sgn" ("x") można określić w taki sposób:
albo w taki:
Dla funkcji rzeczywistych o wartościach rzeczywistych stosowano tabelaryczny sposób określania funkcji. Obecnie w dobie kalkulatorów i arkuszy kalkulacyjnych tabele wartości funkcji logarytmicznych i trygonometrycznych i innych nie są już niezbędne, ale bywają wykorzystywane.
Ważnym sposobem przedstawiania i badania funkcji jest jej wykres, który dla funkcji formula_40 w przypadku funkcji ciągłej jest krzywą na płaszczyźnie.
Funkcja jako związek między zmiennymi.
Zamiast mówić o funkcji jako o relacji między zbiorami, można też mówić o zależności (związku) między dwiema zmiennymi formula_16 i formula_107 gdzie pierwsza z nich przyjmuje wartości ze zbioru formula_108 a druga przyjmuje wartości ze zbioru formula_109 wtedy formula_16 nazywa się zmienną niezależną, a formula_111 – zmienną zależną. Taka interpretacja funkcji jest często używana w analizie matematycznej i zastosowaniach matematyki w innych naukach. W tym wypadku niezależność zmiennej formula_16 oznacza, że może się ona zmieniać w dowolny sposób, a zależność zmiennej formula_111 oznacza, że jej zmiany są zależne od zmian zmiennej formula_114 Na przykład droga formula_115 w ruchu jednostajnym o prędkości formula_116 jest zależna od czasu formula_117 ruchu i wyraża się wzorem:
W praktyce często się zdarza, że zbiór formula_1 jest opisywany przez kilka zmiennych niezależnych formula_120 Mówimy wtedy, że zmienna formula_111 jest funkcją zmiennych formula_120 Na przykład siła formula_123 działająca na ciało jest zależna od masy formula_124 ciała i jego przyspieszenia formula_125
Przykłady funkcji.
W matematyce.
Definicję funkcji spełniają na przykład:
W fizyce.
Wszystkie wielkości fizyczne rozpatruje się jako funkcje innych zmiennych:
W innych dziedzinach.
Funkcja może wyrażać własność pewnego obiektu, dlatego obejmuje bardzo wiele pojęć z nauk empirycznych. Jako funkcję można też traktować każdą relację równoważności zachodzącą między dokładnie dwoma obiektami – jest to tzw. inwolucja.
Astronomia:
Chemia:
Biologia:
Medycyna i fizjologia:
Geografia fizyczna, geodezja i inne nauki o Ziemi:
Geografia społeczna, demografia i socjologia:
Ekonomia:
Psychologia:
Pojęcia.
Złożenie. Iteracja.
Mając dwie funkcje formula_35 i formula_129 można utworzyć funkcję złożoną formula_130 określoną wzorem formula_131
Wielokrotne złożenie funkcji formula_132 nosi nazwę iteracji. Ściśle: formula_133-tą iteracją funkcji formula_6 nazywa się funkcję
Funkcja różnowartościowa.
Funkcję formula_35 nazywa się funkcją różnowartościową lub iniekcją, gdy dla każdych dwóch różnych argumentów przyjmuje różne wartości, tzn. dla dowolnych dwóch formula_137 zachodzi warunek
Przykładem funkcji różnowartościowej jest funkcja określona wzorem formula_140
Funkcja „na”.
Funkcję formula_35 nazywa się funkcją „na” lub suriekcją, jeżeli jej przeciwdziedzina formula_2 jest równocześnie jej zbiorem wartości funkcji. Oznacza to, że dla każdego formula_143 istnieje co najmniej jeden taki formula_144 że formula_145
Funkcja wzajemnie jednoznaczna.
Funkcję będącą jednocześnie różnowartościową i „na” nazywa się funkcją wzajemnie jednoznaczną lub bijekcją. Innymi słowy, bijekcja przyporządkowuje każdemu formula_52 dokładnie jedno formula_143 (i na odwrót). Bijekcja formula_35 może istnieć tylko wtedy, gdy zbiory formula_1 i formula_2 mają tyle samo elementów (są równej mocy). Bijekcję formula_132 nazywa się permutacją.
Funkcja odwrotna.
Dla każdej funkcji wzajemnie jednoznacznej można określić funkcję formula_152 taką, że formula_153 którą nazywa się wówczas funkcją odwrotną.
Zawężenie i przedłużenie.
Dla funkcji formula_35 można określić jej zawężenie, nazywane też obcięciem lub ograniczeniem, do zbioru formula_155 Jest to funkcja formula_156 taka, że formula_157 dla każdego formula_158 Nazywa się ją też funkcją częściową dla funkcji "f".
Jeżeli formula_35 jest funkcją, a formula_156 jest jej zawężeniem do zbioru formula_161 to dla dowolnego zbioru formula_31 mamy formula_163
Z drugiej strony, dla formula_161 można przedłużyć funkcję formula_165 zachowawszy często pewną regułę, otrzymując w ten sposób funkcję formula_166 Można np. wymagać, by przedłużenie formula_167 funkcji formula_6 było ciągłe, różniczkowalne lub okresowe.
Rys historyczny.
Poszukiwaniem wzajemnych zależności między różnymi wielkościami zajmowali się już starożytni Grecy, którzy badali dość szeroki krąg zależności funkcyjnych. Pojęcie funkcji w postaci początkowej pojawiało się w średniowieczu, lecz dopiero w pracach matematyków XVII wieku, Fermata, Kartezjusza, Newtona i Leibniza, zaczęło być traktowane jako obiekt badań. Newton używał terminu "fluenta". Terminu "funkcja" użył po raz pierwszy Leibniz w pracy "Odwrotna metoda stycznych lub o funkcjach". Po raz drugi Leibniz użył tego terminu w dość wąskim znaczeniu w pracy opublikowanej w czasopiśmie „Acta Eruditorum” w 1692 roku i dwa lata później w „Journal des Sçavans”. Następnie w tym samym 1694 roku Johann Bernoulli w „Acta Eruditorum”, nie używając co prawda słowa "funkcja", oznaczył mimochodem literą "n" "„dowolną wielkość utworzoną z nieoznaczonych i stałych”". Po trzech latach, w tym samym piśmie, Bernoulli wielkości te oznaczał przez X i formula_169 a w liście do Leibniza z 26 kwietnia 1698 roku stwierdził, że symbole te są lepsze, bo "„od razu jest widoczne, od jakiej zmiennej jest funkcja”". Jeszcze w 1698 roku w korespondencji między oboma uczonymi funkcja była rozumiana jako wyrażenie analityczne i weszły do użytku terminy "wielkość zmienna" i "wielkość stała".
Określenie funkcji jako wyrażenia analitycznego było po raz pierwszy sformułowane w druku w artykule Johanna Bernoulli opublikowanym w 1718 roku. Napisał on:
W tym samym artykule zaproponował on jako „charakterystykę” funkcji grecką literę formula_170 zapisując argument jeszcze bez nawiasów formula_171 Zarówno nawiasy, jak literę "f" wprowadził Leonhard Euler w 1734 roku.

</doc>
<doc id="1560" url="https://pl.wikipedia.org/wiki?curid=1560" title="Floryda (ujednoznacznienie)">
Floryda (ujednoznacznienie)



</doc>
<doc id="1561" url="https://pl.wikipedia.org/wiki?curid=1561" title="Fraszka">
Fraszka

Fraszka (wł. "frasca" – gałązka, drobiazg, bagatela, błahostka) – krótki utwór liryczny, zazwyczaj rymowany lub wierszowany, o różnorodnej tematyce, często humorystycznej lub ironicznej (satyrycznej). Często kończy się wyraźną puentą. Fraszka wywodzi się ze starożytności, swoimi korzeniami sięga do epigramatu, którego twórcą był Symonides z Keos. Były to krótkie napisy na kamieniach nagrobnych (epitafium) i przedmiotach codziennego użytku.
Autorami fraszek było wielu znanych polskich poetów, między innymi Jan Kochanowski i Wacław Potocki, w literaturze współczesnej Julian Tuwim czy Jan Sztaudynger.
Fraszkę do literatury polskiej wprowadził Jan Kochanowski. Oprócz tematyki żartobliwej, np. fraszka "O doktorze Hiszpanie", pisywał także fraszki refleksyjne ("O żywocie ludzkim"), pochwalne, biesiadne, miłosne ("Do Kasi"). Bardzo znane są: "Na dom w Czarnolesie", "Na lipę", "Raki". Do dziś zachowało się ok. 485 fraszek Jana Kochanowskiego. Poeta ten był również autorem polskiej nazwy tego rodzaju wiersza. "Fraszki" Kochanowskiego zostały wydane po raz pierwszy w 1584.

</doc>
<doc id="1562" url="https://pl.wikipedia.org/wiki?curid=1562" title="FUD">
FUD

FUD ( „strach, niepewność, wątpliwość”) – strategia ograniczenia zdolności zajmowania rynku przez konkurenta, polegająca na podawaniu w mediach lub bezpośrednio klientom nieprawdziwych lub niejasnych informacji o konkurencie i jego produktach. Zakłada, że nawet jeśli ktoś wie, że informacje te są nieprawdziwe, to jego przekonania zostaną zachwiane, co działa na korzyść stosującego tę strategię.

</doc>
<doc id="1563" url="https://pl.wikipedia.org/wiki?curid=1563" title="Furigana">
Furigana



</doc>
<doc id="1564" url="https://pl.wikipedia.org/wiki?curid=1564" title="Frank Herbert">
Frank Herbert

Frank Herbert (ur. 8 października 1920 w Tacoma, zm. 11 lutego 1986 w Madison) – amerykański pisarz science fiction. Znany przede wszystkim jako autor wielokrotnie nagradzanej serii powieści "Kroniki Diuny".
Podczas II wojny światowej służył w amerykańskiej marynarce wojennej. Po wojnie pracował m.in. jako operator telewizyjny, fotograf, komentator radiowy, reporter i redaktor gazet, był również poławiaczem małży i degustatorem win.
Twórczość.
Twórczość Herberta określana jest jako "soft science fiction" i łączy zagadnienia z zakresu ekologii, filozofii, teologii, psychologii i ekonomii. Herbert koncentruje się na naturze człowieka, na jego możliwościach i niebezpieczeństwach, które mu grożą ze strony wymykającej się spod kontroli techniki, systemów totalitarnych i narkotyków zmieniających stan świadomości. Inspirowały go religie: buddyzm, którego był wyznawcą, a także islam. Pisarstwo Herberta bardziej skupia się na przyszłości samego człowieka niż tworzonej przez niego techniki.
Jego pierwsze opowiadanie nie należało do gatunku SF, a ukazało się w magazynie "Esquire". W prasie fantastycznonaukowej zaczął publikować od 1952, nie odnosząc wielkich sukcesów. Zaistniał dopiero w 1956 po wydaniu powieści "Dragon in the Sea", która została określona jako „fantastycznonaukowy kryminał ze skomplikowanym śledztwem psychologicznym”.
Saga Diuny.
Następną powieścią była "Diuna". Początkowo drukowana w czasopiśmie "Analog Science Fact and Fiction" w latach 1963–1965, wydania książkowego doczekała się w 1965 roku, zdobywając uznanie czytelników i krytyków, którzy uhonorowali ją nagrodami Hugo i Nebula.
Zachęcony olbrzymim sukcesem, Frank Herbert stworzył całą serię powieści Kroniki Diuny, których akcja rozgrywała się głównie na pustynnej planecie Arrakis. Kroniki Diuny są najbardziej poczytną i zdaniem wielu krytyków najlepiej opracowaną serią książek fantastycznonaukowych na świecie. Są to kolejno:
Śmierć Herberta przerwała cykl, jednak autor pozostawił znaczną ilość materiałów, a w skrytkach depozytowych szkic kolejnej powieści, rozwijającej wątki rozpoczęte w tomie piątym i szóstym. Na kontynuację pracy nad sagą zdecydował się syn Herberta, Brian Herbert, który wspólnie z Kevinem J. Andersonem stworzył także liczne spin-offy serii.
Ponadto cykl "Kroniki Diuny" uzupełniają następujące pozycje:

</doc>
<doc id="1565" url="https://pl.wikipedia.org/wiki?curid=1565" title="Francis Bacon">
Francis Bacon



</doc>
<doc id="1566" url="https://pl.wikipedia.org/wiki?curid=1566" title="Frank Lloyd Wright">
Frank Lloyd Wright

Frank Lloyd Wright (właśc. Frank Lincoln Wright, "FLW", ur. 8 czerwca 1867 w Richland Center w stanie Wisconsin, zm. 9 kwietnia 1959 w Phoenix, stan Arizona, USA) – amerykański architekt modernistyczny, jeden z najważniejszych projektantów XX wieku. Tworzył projekty mebli, lamp i witraży.
Czerpał natchnienie z natury, a do swych prac wykorzystywał naturalne materiały budowlane. Choć jego projekty stały się inspiracją dla rozwoju stylu międzynarodowego, nie identyfikował się z nim i nie podzielał fascynacji funkcjonalistyczną sztuką architektoniczną – można go nazwać prekursorem architektury organicznej, czyli wkomponowanej i zespolonej z naturą. Jego budowle charakteryzowała przy prostocie bryły i dużej funkcjonalności rzutu ornamentyka, także w postaci ukrytego ornamentu, nie stronił też od form monumentalnych (Civic Center w Marin County, Kalifornia).
Życiorys.
W 1885 rozpoczął studia inżynierskie na Uniwersytecie Stanowym Wisconsin, był członkiem bractwa Phi Delta Theta. Przerwał naukę w 1887, w 1889 uzyskał jednak dyplom. Szkołę opuścił, by przenieść się do Chicago i podjąć pracę w studio architektonicznym Adler and Sullivan. Od 1890 wykonywał całą pracę projektową w firmie. Odszedł po siedmiu latach i w 1893 założył własną praktykę w Chicago. Do 1901 miał na swoim koncie około 50 ukończonych projektów. Pierwszą żoną Wrighta była Catherine Lee „Kitty” Tobin (1871–1959), z którą miał sześcioro dzieci i którą opuścił w 1909 dla Marthy „Mamah” Borthwick (1869-1914), tragicznie zmarłej podczas pożaru w ich domu Taliesinie. Historia ich związku opisana jest w książce Nancy Horan:"Kochając Franka”. W 1922 roku poślubił Maude „Miriam” Noel, swoją współpracownicę. Małżeństwo rozpadło się już po roku. W 1928 roku Wright ożenił się z Olgivanną Hinzenberg, uczennicą G.I. Gurdżijewa, który gościł parę razy w Taliesinie. Jego spotkanie z Wrightem opisuje Robert Lepage w „Geometrii cudów”.
W latach 1901–1911 projektował domy w stylu preriowym („Prairie Houses”), nazywane tak ze względu na ich dopasowanie do krajobrazu w okolicach Chicago. Wright odegrał również ważną rolę w tworzeniu koncepcji „planu otwartego” dla wnętrz.
W 1911 r. zaprojektował własny dom-studio tzw. Taliesin (od imienia walijskiego poety z VI w., dosłownie oznacza „błyszcząca brew”), w pobliżu Spring Green w stanie Wisconsin. Kompleks ten był jednokondygnacyjną strukturą na planie litery L, z jednej strony wychodzącą na jezioro, w drugiej części mieszczącą studio Wrighta. Taliesin było dwukrotnie niszczone przez pożar. Obecny budynek zwany jest Taliesin III. W latach trzydziestych Wright zaprojektował zimowe studio w Arizonie – Taliesin West.
Jedną ze słynniejszych konstrukcji autorstwa FLW jest Fallingwater, budowany w latach 1935-1939 dla E.J. Kaufmanna w Bear Run, w stanie Pensylwania. Według pomysłu Wrighta domownicy powinni znaleźć się jak najbliżej natury; stąd pod częścią domu, na który składa się seria wolnonośnych balkonów i tarasów, przepływa strumień. Do budowy elementów pionowych wykorzystano kamień, do elementów poziomych – beton.
Najbardziej znanym zrealizowanym projektem Wrighta było Muzeum Guggenheima w Nowym Jorku. Budowla w kształcie białej spirali powstawała przez 17 lat (1942-1959). Wznosi się przy Piątej Alei. Niezwykła geometria scentralizowanego wnętrza umożliwia zwiedzającym przemieszczanie się po łagodnie opadającej spiralnej rampie podczas oglądania wystaw czasowych.

</doc>
<doc id="1567" url="https://pl.wikipedia.org/wiki?curid=1567" title="Floryda">
Floryda

Floryda (ang. "State of Florida") () – stan na południowym wschodzie Stanów Zjednoczonych, na półwyspie Floryda otoczonym przez wody Zatoki Meksykańskiej na zachodzie i Oceanu Atlantyckiego na wschodzie. Floryda graniczy na północy z Alabamą i Georgią.
Floryda jest trzecim pod względem liczby ludności (21,7 mln) i ósmym pod względem gęstości zaludnienia (121 mieszk./km²) stanem USA.
Obszar metropolitalny Miami, liczący ponad 6 mln mieszkańców, jest największą aglomeracją Florydy i dziewiątą co do wielkości w Stanach Zjednoczonych. Inne duże aglomeracje to: Tampa (3,2 mln), Orlando (2,6 mln) i Jacksonville (1,5 mln). Stolicą stanu jest Tallahassee.
Floryda to najpopularniejszy cel podróży na świecie, a turystyka wnosi do gospodarki stanu ponad 65 miliardów dolarów rocznie. Stan znany jest jako "Sunshine State" (Słoneczny Stan).
Geografia.
Teren jest nizinny, płaski lub pofałdowany, liczne jeziora i bagna, głównie na południu (Everglades). Lasy zajmują około połowy obszaru Florydy; większość z nich leży na północ od Orlando. Płaskowyż na północy Florydy to zazwyczaj otwarte lasy zdominowane przez sosny.
Floryda jest w większości położona na wielkim półwyspie pomiędzy Zatoką Meksykańską, Oceanem Atlantyckim i Cieśniną Florydzką. Graniczy ze stanami Georgia oraz Alabama na północy. Floryda leży niedaleko Karaibów, Kuby i Wysp Bahama. Rozległy pas przybrzeżny Florydy był postrzegany przez rząd Stanów Zjednoczonych jako potencjalne miejsce ataku w trakcie II wojny światowej, dlatego wybudowano lądowiska i pasy startowe na terenie całego stanu oraz dookoła niego. Do dziś około 400 z nich jest wciąż w użyciu.
Klimat stanu jest podrównikowy i zwrotnikowy morski. Prąd Zatokowy sprawia, że stan ten jest jednym z najbardziej wilgotnych w kraju z częstymi letnimi burzami i sporadycznie niszczycielskimi huraganami. Średnia roczna suma opadów dochodzi do 1500 mm. Roślinność jest śródziemnomorska, na południu zwrotnikowa.
Na terenie Florydy znajdują się 3 parki narodowe:
Stan leży w dwóch strefach czasowych: i Florida Panhandle 
Większe miasta.
Największe metropolie Florydy (dane z 2017 roku):
Demografia.
Spis ludności z roku 2020 stwierdza, że stan Floryda liczy 21 538 187 mieszkańców, co oznacza wzrost o 2 736 877 (14,6%) w porównaniu z poprzednim spisem z roku 2010. Dzieci poniżej piątego roku życia stanowią 5,3% populacji, 19,7% mieszkańców nie ukończyło jeszcze osiemnastego roku życia, a 20,9% to osoby mające 65 i więcej lat. 51,1% ludności stanu stanowią kobiety.
Rasy i pochodzenie.
W 2019 roku 74,5% mieszkańców stanowiła ludność biała (53%, nie licząc Latynosów), 16% to czarnoskórzy Amerykanie lub Afroamerykanie, 2,9% miało rasę mieszaną, 2,8% to Azjaci, 0,28% to rdzenna ludność Ameryki, 0,08% to Hawajczycy i mieszkańcy innych wysp Pacyfiku. Latynosi stanowią 26,4% ludności stanu.
Poza osobami pochodzenia afroamerykańskiego, do największych grup należą osoby pochodzenia niemieckiego (8,6%), „amerykańskiego” (8,3%), irlandzkiego (8,1%), kubańskiego (7,4%), angielskiego (6,5%), włoskiego (5,7%) i portorykańskiego (5,5%). Istnieją także duże grupy Meksykanów (743 tys.), Haitańczyków (533,4 tys.), Francuzów (511,8 tys.), Polaków (500,2 tys.), Kolumbijczyków (415 tys.), Szkotów i Jamajczyków (304,6 tys.).
Mieszka tu największa populacja Kubańczyków w całych Stanach Zjednoczonych. Hrabstwa o największej koncentracji Kubańczyków, to: Miami-Dade, Broward, Hillsborough i Palm Beach. Hrabstwa te razem skupiają ponad połowę wszystkich Kubańczyków w USA.
Wśród ludności azjatyckiej największe grupy stanowią Hindusi, Filipińczycy i Chińczycy.
Język.
Najpowszechniej używanymi językami są:
Religia.
Struktura religijna w 2014 roku:
70% mieszkańców Florydy wyznaje chrześcijaństwo, a Kościół katolicki pozostaje największą pojedynczą organizacją.
Do największych ewangelikalnych kościołów (z ponad 100 tys. członków) należą: Południowa Konwencja Baptystów, lokalne kościoły bezdenominacyjne (np. "Christ Fellowship"), Kościoły zielonoświątkowe, Kościoły Chrystusowe i Kościół Adwentystów Dnia Siódmego. Protestantyzm głównego nurtu reprezentowany jest głównie przez Zjednoczony Kościół Metodystyczny, który jest trzecią co do wielkości organizacją religijną, a czarni protestanci to przeważnie różne kościoły baptystyczne i metodystyczne.
Floryda jest domem dla czwartej co do wielkości społeczności żydowskiej w Stanach Zjednoczonych.
Gospodarka.
Gospodarka Florydy jest niezwykle zróżnicowana, a turystyka i rolnictwo należą do najważniejszych gałęzi przemysłu. Produkt krajowy brutto Florydy osiągnął 1 bilion dolarów w 2018 roku. Gdyby Floryda była niepodległym państwem, miałaby 17-ste co do wielkości PKB na świecie, plasując się tuż za Indonezją. Na Florydzie nie płaci się podatku dochodowego. W obszarze rolnictwa głównym towarem eksportowym jest mięso. Jednak w ogólnym eksporcie USA pochodzącym z Florydy, największy udział mają samoloty cywilne i części do nich.
Energia.
W 2019 roku Floryda jest drugim co do wielkości producentem energii elektrycznej w kraju, po Teksasie. Za trzy czwarte produkcji energii odpowiada gaz ziemny, który zasila 7 z 10 największych elektrowni w stanie. Floryda posiada niewielką ilość własnego gazu ziemnego, a większość odbierana jest międzystanowymi rurociągami.
Drugim co do wielkości źródłem jest energia jądrowa. Dwie stanowe elektrownie jądrowe, zlokalizowane na wybrzeżu Atlantyku, zazwyczaj generują więcej niż jedną dziesiątą energii elektrycznej. Na trzecim miejscu jest węgiel, który odpowiada za mniej niż jedną dziesiątą produkcji (w 2003 odpowiadał za jedną trzecią). Odnawialne źródła energii, głównie biomasa i energia słoneczna, oraz koks naftowy stanowiły prawie całą pozostałą produkcję energii elektrycznej na Florydzie.
Rolnictwo.
Floryda jest największym na świecie producentem grejpfrutów i produkuje ponad 70% cytrusów w kraju. Stan regularnie zajmuje pierwsze miejsce w krajowej produkcji pomarańczy, a także jest czołowym producentem ogórków, pomidorów, papryki, truskawek, arbuzów, świeżej kapusty targowej, rynkowej słodkiej kukurydzy, fasoli szparagowej, orzeszków ziemnych i trzciny cukrowej.
Ważną rolę w stanie odgrywa hodowla bydła, a najwyżej pod tym względem w rankingu plasują się hrabstwa Okeechobee, Highlands i Osceola. Pospolite są także hodowla drobiu, ogrodnictwo, mleczarstwo, oraz produkcja siana.
Edukacja.
Administracją szkół podstawowych i średnich na Florydzie zajmuje się Florydzki Departament Edukacji. System szkolnictwa wyższego na Florydzie obejmuje kilkanaście uniwersytetów stanowych i prawie 30 szkół wyższych. Niektóre z godnych uwagi uniwersytetów publicznych to Uniwersytet Florydy w Gainesville i Uniwersytet Stanu Floryda w Tallahassee. Uniwersytet Miami jest instytucją prywatną.
W 2000 roku gubernator stanu podjął decyzję o zlikwidowaniu Florida Board of Regents, która kierowała systemem uniwersytetów stanowych (SUS). Na jej miejsce powstały komisje które zarządzały każdym uniwersytetem oddzielnie. Nowy system się jednak nie przyjął i w 2002 roku pierwszy gubernator stanu oraz senator Stanów Zjednoczonych Bob Graham zwołali referendum na temat przywrócenia systemu board-of-regents, ostatecznie w 2003 r. powstała Florydzka Komisja Gubernatorska, której podlegają uczelnie.
Fauna.
Floryda jest miejscem, na którym osiedliło się wiele gatunków zwierząt:

</doc>
<doc id="1568" url="https://pl.wikipedia.org/wiki?curid=1568" title="Fala sejsmiczna">
Fala sejsmiczna

Fale sejsmiczne – fale sprężyste rozchodzące się w Ziemi. Mogą powstać wskutek wielu czynników, np. trzęsień ziemi, eksplozji materiałów wybuchowych, działalności górniczej, sztormów na morzu.
Prędkość fal sejsmicznych zależy od typu fal, gęstości ośrodka i jego współczynników sprężystości, które z kolei zależą od temperatury i ciśnienia. W warunkach normalnych prędkość fal podłużnych (P) w suchym piasku wynosi 400–1200 m/s, w dolomitach 3500–6500 m/s, granitach 4500–6000 m/s, w bazaltach 5000–6000 m/s, w wodzie 1450–1500 m/s, i w lodzie 3400–3800 m/s. W głębi Ziemi prędkość z reguły rośnie z uwagi na zwiększanie się współczynnika sprężystości ze wzrostem ciśnienia. Jednak na granicy z jądrem zewnętrznym prędkość fal P gwałtownie maleje z głębokością z uwagi na stopienie jądra. Jądro Ziemi, jako że jest płynne, nie przenosi fal poprzecznych.
Charakterystyka.
Rodzaje fal sejsmicznych:

</doc>
<doc id="1569" url="https://pl.wikipedia.org/wiki?curid=1569" title="Francuski kalendarz rewolucyjny">
Francuski kalendarz rewolucyjny

Francuski kalendarz rewolucyjny, wł. francuski kalendarz republikański (fr. "calendrier républicain", "calendrier révolutionnaire français") – kalendarz wprowadzony 5 października 1793 (14 Vendémiaire roku II) przez Konwent republikański w rewolucyjnej Francji. Obszerną instrukcję uzupełniającą wydano 24 listopada 1793 (4 Frimaire roku II). Napoleon Bonaparte 9 września 1805 (22 Fructidor roku XIII) podpisał akt prawny przywracający kalendarz gregoriański od 1 stycznia 1806 (11 Nivôse roku XIV). Wprowadzony jeszcze, ale na krótko w 1871 przez Komunę Paryską (miesiące germinal i floréal roku LXXIX). Projekt kalendarza opracował Gilbert Romme, a nazwy miesięcy i dni wymyślił poeta Philippe Fabre d’Églantine. Era chrześcijańska zastąpiona została "erą republikańską".
Lata liczono od 22 września 1792, daty ustanowienia pierwszej republiki francuskiej. Co czwarty rok miał być przestępnym, ale w praktyce starano się utrzymać początek roku w dniu równonocy jesiennej. Długość roku pozostała ta sama co w kalendarzu gregoriańskim. Rok zawierał 12 miesięcy po 30 dni oraz 5 (6 w latach przestępnych) dodatkowych dni świątecznych, tak zwanych "Dni Sankiulotów" ("les Sans-cullotides"). Czteroletni cykl nazwano "francjadą" ("le franciade") na wzór olimpiady. Rok przestępny był trzecim rokiem "francjady" (zatem rok III, VII i XI), dodatkowy dzień przypadał 22 września 1795 i 1799 oraz 23 września 1803.
Na miejsce świąt kościelnych wprowadzono święta narodowe z dniem zdobycia Bastylii jako najważniejszym. Każdy dzień w roku został nazwany na cześć zwierząt, roślin, narzędzi pracy, minerałów czy też zjawisk.
Podstawą kalendarza był almanach napisany w 1788 przez Pierre Sylvain Maréchala pt. "Kalendarz dzielnych ludzi" (fr. "Almanach des Honnêtes Gens").
Dni dekady.
Miesiąc, zamiast na tygodnie, podzielono na 3 dekady – po 10 dni. Każdy z dziesięciu dni nazywany był po prostu liczebnikami porządkowymi:
Dziesiąty dzień był świętem. 8 kwietnia 1802 (18 germinal roku X) zniesiono podział na dekady jako uciążliwy dla społeczeństwa, przywracając tygodnie.
Planowany był także podział doby na 10 godzin, godziny na 100 minut po 100 sekund. Nowa godzina liczyłaby 144 dawne minuty, nowa minuta 86,4 dawnych sekund zaś nowa sekunda 0,864 dawnej. Art. 22 dekretu o miarach i wagach z 7 kwietnia 1795 (18 germinal roku III) zawiesił tę zmianę na czas nieokreślony.
Miesiące.
Jesień (nazwy miesięcy kończą się na -aire):
Zima (nazwy miesięcy kończą się na -ôse):
Wiosna (nazwy miesięcy kończą się na -al):
Lato (nazwy miesięcy kończą się na -dor):
Dni Sankiulotów.
Podział roku na 12 równych miesięcy i dni dodatkowe przypominał kalendarz egipski.

</doc>
<doc id="1570" url="https://pl.wikipedia.org/wiki?curid=1570" title="Fortran">
Fortran

Fortran (od wersji 90 do aktualnej) a dawniej FORTRAN (do wersji 77 włącznie) (od ang. "formula translation") – język programowania pierwotnie zaprojektowany do zapisu programów obliczeniowych, był niegdyś językiem proceduralnym, obecnie jest nadal rozwijanym językiem ogólnego przeznaczenia. Umożliwia programowanie strukturalne, obiektowe (Fortran 90/95), modularne i równoległe (Fortran 2008). Jego zastosowaniami są, między innymi, obliczenia naukowo-inżynierskie, numeryczne, symulacja komputerowa itp. Początkowe wersje Fortranu miały mocno ograniczone możliwości, ale dzięki łatwości opanowania Fortran stał się najpopularniejszym językiem do obliczeń numerycznych.
Specyfika.
Z pierwszych wersji języka pochodzi zasada braku rozróżniania małych i wielkich liter w słowach kluczowych języka oraz używanych zmiennych, a także bogate zasady tworzenia formatów zapisywanych i drukowanych danych.
Fortran dysponuje wielką liczbą bibliotek, które pozwalają rozwiązać praktycznie każde zadanie numeryczne. Najważniejsze przyczyny, z powodu których Fortran jest wykorzystywany i rozwijany do dziś, to szybkość obliczeń oraz wysoka wydajność kodu generowanego przez kompilatory Fortranu, wynikająca m.in. z jego długiej obecności na rynku programistycznym, znakomita skalowalność i przenośność oprogramowania (pomiędzy różnymi platformami sprzętowymi i systemami operacyjnymi), a także dostępność bibliotek dla programowania wieloprocesorowego i równoległego oraz bibliotek graficznych. Obliczenia aerodynamiczne, wytrzymałościowe i cieplne obecnie często prowadzone są z użyciem tego języka. 
Do niedawna te zalety były okupione brakiem dobrych metod wizualizacji czy niskopoziomowej komunikacji z systemem operacyjnym, gdyż te zagadnienia są pominięte w kolejnych standardach języka (aż do Fortranu 95 włącznie). Obecnie producenci kompilatorów (zwłaszcza niezależni), wzbogacają je o możliwość stosowania grafiki celem umożliwienia wizualizacji wyników obliczeń (wykresy, wizualizacja tablic wielowymiarowych) poprzez dostarczanie odpowiednich bibliotek. 
Historia i standardy.
Pierwszy kompilator Fortranu stworzył zespół Johna Backusa, który w latach 1954-1957 pracował dla IBM. Kompilator ten był pierwszym w historii kompilatorem języka wysokiego poziomu. Został starannie zoptymalizowany, ponieważ autorzy obawiali się, że nikt nie będzie go używał, jeśli szybkość programów nie będzie porównywalna z szybkością programów napisanych w asemblerze.
Pierwsza standaryzacja nastąpiła w 1960 roku, kiedy opisano pierwszy standard języka znany jako Fortran IV. Kolejnym standardem był Fortran 66. Standard ten był zbyt ubogi i implementacje musiały zawierać wiele rozszerzeń. W latach siedemdziesiątych American National Standard Institute (ANSI) opracowała kolejny standard nazwany Fortran 77, który w roku 1980 stał się standardem międzynarodowym. Jego struktura przystosowana jest do używanych wówczas powszechnie kart perforowanych, ale zawierała elementy programowania strukturalnego. W Polsce bardzo popularna była odmiana Fortranu na maszyny ICL/Odra ("FORTRAN 1900"; kompilator taśmowy #XFAM oraz dyskowe #XFAT i #XFAE z konsolidatorem #XPCK), a także "Watfor 77", "Lahey 77" i "MS Fortran" na PC czy Fortran 80 dla 8-bitowych komputerów z systemem CP/M-80. W praktyce większość tych kompilatorów miała wiele rozszerzeń, które dopiero po latach weszły do kolejnego standardu (struktury rekordowe, alokowalne tablice, więcej instrukcji pozwalających na programowanie strukturalne itp.). Następcą Fortranu 77 stał się Fortran 90. Standard ma całkowicie zmienioną składnię dostosowaną do współczesnych języków programowania, przykładowo Fortran 90 nie wymusza już na użytkowniku stosowania etykiet i instrukcji skoku. Kolejnym standardem jest Fortran 95, zmiany w stosunku do poprzedniej wersji są niewielkie. Pomimo wprowadzenia nowych standardów, Fortran 77 jest nadal w użyciu (głównie przez starszych programistów przyzwyczajonych do tej wersji), co uwzględniają producenci praktycznie wszystkich kompilatorów – na przykład kompilator Compaq Visual Fortran (wcześniej Digital Visual Fortran) kompiluje programy napisane w standardach 66/77/90/95. Najnowszy standard języka nosi nazwę Fortran 2008.
Przykłady.
Najkrótszy program:
 END
Hello world, kod dla F77:
PROGRAM HELLO
 WRITE (*,*) 'hello, world'
END
Standard nie wymusza stosowania deklaracji zmiennych, wówczas niezadeklarowane zmienne o nazwach rozpoczynających się od liter A-H i O-Z (nie są rozróżniane małe i duże litery) mają typ real (rzeczywisty, zmiennoprzecinkowy o precyzji zależnej od implementacji i opcji) a zmienne rozpoczynające się od I-N są typu integer (liczby całkowite o zakresie zależnym od implementacji i opcji); jest to równoważne deklaracjom „implicit real (A-H,O-Z)” i „implicit integer (I-N)”.
Dyrektywa kompilatora codice_2 zastosowana na początku programu zapobiega wykorzystaniu niezadeklarowanych zmiennych. Przykładowy poprawny kod programu liczącego sumę dwóch liczb rzeczywistych (podwójnej precyzji – real(8)), w języku F95:
IMPLICIT NONE
REAL(8) :: a,b
READ *,a,b
PRINT *,'wynik',a+b
END
w przypadku kodu:
IMPLICIT NONE
REAL(8) :: a
READ *,a,b
PRINT *,'wynik',a+b
END
kompilator wyświetla błąd o niezadeklarowaniu zmiennej „b” wykorzystywanej w programie.
Fakt domyślnej deklaracji zmiennych niejednokrotnie powodował kłopoty programistów, nie mogących odnaleźć źródła nieoczekiwanych kaprysów pozornie poprawnych programów, np. nagłówek pętli wykonywanej 25 razy dla zmiennej codice_3:
DO 100 I=1,25
napisany z kropką zamiast przecinka:
DO 100 I=1.25
nie czyni tej instrukcji niepoprawną, ponieważ kompilator uzna ten zapis za domyślną deklarację zmiennej codice_4 typu rzeczywistego (nazwa zaczyna się od litery D) i przypisze jej wartość codice_5, co zmieni zamierzony przez programistę przepływ sterowania.

</doc>
<doc id="1571" url="https://pl.wikipedia.org/wiki?curid=1571" title="Fritz Haber">
Fritz Haber

Fritz Haber (ur. 9 grudnia 1868 we Wrocławiu, zm. 29 stycznia 1934 w Bazylei) – chemik niemiecki pochodzenia żydowskiego, laureat Nagrody Nobla w dziedzinie chemii w 1918 roku za syntezę amoniaku z azotu i wodoru.
Życiorys.
W latach 1886–1891 studiował na uniwersytecie w Heidelbergu u Roberta Bunsena, potem na uniwersytecie w Berlinie. W 1892 r. wyjechał na jeden semestr do Politechniki w Zurychu aby uzupełnić wiedzę w obszarze technologii chemicznej pod kierunkiem prof. Georga Lungego. Jesienią tego samego roku przeniósł się na Uniwersytet w Jenie, do laboratorium prof. Ludwiga Knorra. W Jenie przebywał dwa lata. W tym okresie przeszedł z judaizmu na protestantyzm. Nie wyjaśnił powodów tej decyzji, prawdopodobnie miała ona na celu ułatwienie kariery uniwersyteckiej. W roku 1894 został asystentem prof. Hansa Buntego w Technische Hochschule Karlsruhe. W 1898 r. uzyskał tam stanowisko profesora chemii, a od 1911 do 1933 r. był profesorem w "Kaiser-Wilhelm-Institut für Physikalische Chemie und Elektrochemie" (obecnie "Fritz-Haber-Institut der Max-Planck-Gesellschaft").
Prowadził badania w dziedzinie elektrochemii i katalizy. Od 1904 rozpoczął prace nad równowagą reakcji powstawania amoniaku z atomów azotu i wodoru w wysokich temperaturach i wysokim ciśnieniu. Badania związane były z prognozowanym na lata 30. XX wieku głodem w Europie (gdzie w ciągu dwóch stuleci liczba mieszkańców zwiększyła się czterokrotnie). W badaniach wykorzystał odkrycie z lat 40. XIX wieku Justusa von Liebiga, który zauważył, że wzrost roślin pobudzają związki azotu. Na początku wieku XX producenci rolni sprowadzali z Ameryki Płd. guano i importowali saletrę z Chile. Obydwie substancje były stosunkowo drogie i było ich zbyt mało, aby zaspokoić potrzeby rolników europejskich. W latach 1905–1910 opracował metodę syntezy amoniaku, nazwaną później metodą Habera i Boscha. W 1908 Haber otrzymał po raz pierwszy ciekły amoniak metodą półprzemysłową. Proces, opatentowany przez Habera, polegał na wpompowywaniu do reaktorów azotu i wodoru, które pod wpływem wysokiej temperatury i ciśnienia w obecności katalizatorów łączyły się tworząc amoniak. W latach 1908–1913 Haber pracował – wspólnie ze współtwórcą BASF, Carlem Boschem (laureatem Nagrody Nobla w roku 1931 za rozwój wysokociśnieniowych procesów technologicznych) – nad wdrożeniem technologii w skali przemysłowej. W roku 1913 przedsiębiorstwo BASF wybudowało w Ludwigshafen fabrykę, w której rozpoczęto produkcję amoniaku z wodoru i azotu z atmosfery. W prasie pisano w tym czasie, że Haber „zrobił chleb z powietrza”. Dziś światowa produkcja nawozów azotowych, wytwarzanych metodą Habera-Boscha, osiąga kilkaset tysięcy ton rocznie. Dzięki produkcji nawozów azotowych Haber został milionerem. Poza majątkiem odkrycie dało mu sławę i zaszczyty. Stanął na czele Instytutu chemii fizykalnej im. Cesarza Wilhelma w Berlinie (dziś Instytut im. Habera).
Okres I wojny światowej.
Po wybuchu I wojny światowej amoniak zastąpił importowaną z Chile saletrę do produkcji materiałów wybuchowych i amunicji. Niemcy szybko i tanio uzupełniały swoje arsenały zbrojeniowe. Haber na zlecenie niemieckiego Sztabu Generalnego rozpoczął pracę nad wykorzystaniem chemii na potrzeby armii (konwencja haska z 1907 zabraniała stosowania broni chemicznej). W odpowiedzi na zapotrzebowanie niemieckiej armii, zamiast toluenu, zaproponował do produkcji trotylu stosowanie ksylenu i innych pochodnych ropy naftowej.
Traktował on swoją pracę jako patriotyczny obowiązek. Mówił, że nauka w czasie pokoju należy do całego świata, a w czasie wojny musi służyć państwu. Mówił swoim współpracownikom, że jeśli uda się szybko pokonać wrogów Niemiec, konflikt pochłonie mniej ofiar (Francuzi w tym czasie też pracowali nad rozwojem broni chemicznych). Podczas I wojny światowej był jednym z głównych organizatorów produkcji i zastosowania gazów bojowych przez armię niemiecką m.in. pod Langemark i Ypres (II bitwa pod Ypres) i pod Bolimowem. Pod Ypres Haber osobiście nadzorował atak chemiczny. Notował spostrzeżenia objawów u konających żołnierzy. Generalicja niemiecka nie wykorzystała potencjału broni chemicznej, a naukowcy z Paryża i Londynu szybko rozszyfrowali skład chemiczny toksycznych substancji niemieckich. Zaangażowanie Habera w produkcję gazów bojowych doprowadziło do samobójstwa jego żonę Clarę Haber, która nie mogła znieść obciążenia psychicznego, związanego z przyczynianiem się jej męża do śmierci żołnierzy (zastrzeliła się z pistoletu służbowego Habera). Nazajutrz po pogrzebie żony Haber wyruszył na front wschodni ponownie testować gazy bojowe.
W 1917 roku ponownie się ożenił, tym razem z Charlotte Nathan, z którą miał dwójkę dzieci. Po zakończeniu I wojny zapadł na depresję.
Po I wojnie światowej.
W roku 1918 Haber został laureatem Nagrody Nobla w dziedzinie chemii za opracowanie metody syntezy amoniaku. W tym czasie działał w odbudowie niemieckiego przemysłu wojennego. Kontynuował prace w tajnej fabryce w pobliżu Wittenbergi. Stanął na czele Degesch – Niemieckiego Towarzystwa do Walki ze Szkodnikami. Szukał środków, którymi łatwo byłoby zabijać pchły, wszy i pluskwy, lęgnące się w magazynach czy ładowniach statków oceanicznych. To właśnie wtedy, w latach 20. XX wieku, jego współpracownicy pod jego nadzorem opracowali technologię produkcji cyklonu B, który chociaż początkowo był przewidziany jako środek do dezynfekcji i dezynsekcji, został później zastosowany przez Niemców w komorach gazowych podczas II wojny światowej. Granulki cyklonu zawierały toksyczny cyjanowodór, który uwalniał się stopniowo, skutecznie niszcząc szkodniki. Cyklon B osiągnął sukces, a jego produkcja wynosiła setki ton. Haber pomagał też produkować iperyt Hiszpanii, która używała go w walce z powstańcami w Maroku. Jednocześnie próbował metodami elektrochemicznymi uzyskać złoto rozpuszczone w wodzie morskiej, by Niemcy mogły spłacić gigantyczne kontrybucje. Próby z uzyskaniem złota nie zakończyły się powodzeniem, miały jednak duże znaczenie naukowe (według wyliczeń badaczy zespołu Habera z 1923 r. w jednej tonie morskiej wody znajdowało się średnio 0,008 mg złota, co czyni wydobycie złota tą drogą nieopłacalnym).
Po dojściu Hitlera do władzy w 1933 roku, w ramach czystek antyżydowskich Haber zmuszony został do zwolnienia swoich współpracowników pochodzenia żydowskiego, po czym sam złożył rezygnację. Wyjechał do Wielkiej Brytanii, gdzie dostał ofertę pracy na University of Cambridge. Po dwóch miesiącach opuścił Anglię ze względu na stan zdrowia i wyruszył w podróż w kierunku południowym, w poszukiwaniu cieplejszego klimatu. Miał ofertę Chaima Weizmana (późniejszego pierwszego prezydenta Izraela) posady w tworzonym właśnie Instytucie Badawczym Daniela Sieffa. Stan zdrowia nie pozwalał mu jednak na tak daleką podróż. 29 stycznia 1934 zmarł w Bazylei w Szwajcarii.
Albert Einstein napisał o nim „Była to tragedia niemieckiego Żyda, tragedia wzgardzonej miłości”.
Cyklon B.
Cyklon B jako środek do zabijania ludzi odkryto przypadkiem. Stosowano go do odwszawiania ubrań skonfiskowanych więźniom przywożonym do obozów koncentracyjnych. Szybko okazało się, że cyklon szybciej działał na ludzi niż na owady. Cztery kilogramy cyklonu były w stanie zabić ok. 1000 ludzi. Kilogram cyklonu kosztował ok. 5 marek niemieckich. Po wojnie jeden z uczniów Habera Bruno Tesch, szef przedsiębiorstwa sprzedającego go do obozów, został skazany na śmierć. Udowodniono mu, że wiedział o przeznaczeniu gazu.

</doc>
<doc id="1572" url="https://pl.wikipedia.org/wiki?curid=1572" title="FAQ">
FAQ

FAQ ( – często zadawane pytania) – zbiory często zadawanych pytań i odpowiedzi na nie, mające na celu udzielenie danemu użytkownikowi serwisu internetowego pomocy bez konieczności angażowania do tego jakichkolwiek osób.
FAQ były wykorzystywane przez administratorów pierwszych uczelnianych serwerów komputerowych w USA, tworzących zrąb ówczesnego Internetu, którzy zaczęli tworzyć takie dokumenty i wysyłać je jako odpowiedzi na ciągle powtarzające się pytania początkujących użytkowników. Szczególnie duże ilości FAQ stworzyli stali użytkownicy grup newsowych.
Oprócz tego FAQ są często stosowane w wielkich korporacjach (wręcza się je świeżo zatrudnianym pracownikom), na stronach internetowych, są dołączane do programów komputerowych, wykorzystują je kanały IRC.
FAQ, początkowo stosowane dla wygody administratorów serwerów, stały się także wygodnymi dokumentami ułatwiającymi standaryzację wielu rozwiązań technicznych stosowanych w Internecie.
W początkowym okresie rozwoju Internetu stała wymiana FAQ między administratorami i uzgadnianie jednolitych odpowiedzi na niektóre podstawowe pytania była formą ujednolicania wielu z tych standardów. Obecnie rolę tę przejęły takie organizacje jak IETF i W3C, które stworzyły swoją własną, jednolitą w formie wersję rozbudowanych FAQ-ów zwanych Request For Comments.

</doc>
<doc id="1573" url="https://pl.wikipedia.org/wiki?curid=1573" title="Fidonet">
Fidonet

FidoNet – amatorska sieć komputerowa łącząca BBS-y na całym świecie.
Sieć ta oparta jest na zwykłych liniach telefonicznych i modemach. BBS-y należące do tej sieci działają w zasadzie zupełnie niezależnie, a sieć służy głównie do wymiany wiadomości między BBS-ami.
Logo FidoNetu autorstwa Johna Madila
Organizacja sieci.
Fidonet nie posiada stale połączonych serwerów, a przekaz wiadomości odbywa się w "sesjach" poprzez połączenia między wybranymi BBS-ami tworzącymi "węzły" sieci. Przez długi czas sesje nawiązywane były przez łącza telefoniczne za pomocą modemów. 
Wysokie koszty międzymiastowych i międzynarodowych połączeń telefonicznych w istotny sposób wpłynęły na organizację Fidonetu. Sieć ma strukturę hierarchiczną, opartą przede wszystkim na geograficznym położeniu węzłów, co miało zmniejszać koszty przesyłania danych.
Sesje wymiany wiadomości między odległymi węzłami odbywały się niezbyt często i głównie w godzinach nocnych, kiedy obowiązywała tańsza taryfa. Wskutek tego przesłanie listu nawet przez prawidłowo funkcjonującą sieć mogło trwać kilka dni. W przypadku awarii nie istnieją automatyczne mechanizmy zmiany konfiguracji sieci (rerouting), konieczne są ingerencje operatorów węzłów. W ramach upowszechniania się Internetu, większość telefonicznej komunikacji między węzłami została zastąpiona łącznością internetową. Znacznie przyspieszyło to czasy przesyłania wiadomości przez sieć oraz zmniejszyło koszty, zwłaszcza dla połączeń międzynarodowych i międzykontynentalnych.
Wśród osób pełniących funkcje zarządzające można wymienić:
W strukturze sieci wyodrębnia się następujące jednostki organizacyjne:
Przykładowy adres węzła Fidonet wygląda następująco: 2:480/124, czyli:
Adres punktu powstaje przez dodanie kropki i numeru po adresie węzła, np. 2:480/124.5
Sieć udostępnia następujące usługi:
Historia sieci.
Sieć FidoNet powstała w 1982 roku w USA i była w zasadzie pierwszą na świecie rozległą siecią komputerową, z rozproszonym zarządzaniem i dostępną dla każdego właściciela komputera. Swoją nazwę wzięła od imienia jamnika twórcy sieci – Toma Jenningsa. W szczytowym okresie swojego rozwoju (ok. roku 1995) FidoNet łączył ponad 30000 BBS-ów, co oznacza co najmniej dwa-trzy razy tyle pojedynczych użytkowników. Obecnie liczba ta nieustannie maleje (ok. 10000 węzłów w 2003 roku, ok. 1300 w 2020 roku), a cała sieć na skutek powszechnego dostępu do Internetu, powoli zanika. Pozostające węzły są w większości utrzymywane z sentymentu i jako historyczna ciekawostka.
Polonijny FidoNet w Stanach Zjednoczonych.
Początki FidoNetu w Stanach Zjednoczonych w j. polskim zaczęły się pod koniec 1987 roku. 26 października 1987 roku, Marek Majewski, The PainFrame BBS (FidoNet 1:261/1004), założył lokalnie konferencje POLISH echo.
W 1990 roku dołączyli do niego, tworząc POLISH echo jako oficjalną konferencję w FidoNet:
20 grudnia 1990 roku, Greg Kochaniak zakończył instalowanie bramki UFGATE, pozwalającej wymianę korespondencji pomiędzy POLISH a grupą Usenet, "soc.culture.polish".
Spis polskich BBS-ów tworzył Zbigniew Tyrlik (Freenet. Edu).
Po nawiązaniu kontaktu z Janem Stożkiem z Polski, rozszerzono POLISH echo na Polskę.
FidoNet w Polsce.
Historia polskiego regionu FidoNet sięga roku 1986, kiedy to dwaj współpracownicy miesięcznika "Komputer", Tomasz Zieliński i Tadeusz Wilczek, zostali zaproszeni przez działające w tym czasie Stowarzyszenie ABAKUS do wygłoszenia prelekcji na organizowanej przez ABAKUS Wystawie Sprzętu Mikrokomputerowego w warszawskim Muzeum Techniki. Po raz pierwszy w Polsce zaprezentowali oni zgromadzonej publiczności połączenie z BBS-em. Ich znajomość niespotykanej wtedy w Polsce dziedziny zastosowań komputerów osobistych była owocem kontaktów, jakie utrzymywali z Polakiem będącym członkiem władz holenderskiego Hobby Computer Club - prężnej organizacji gromadzącej amatorów spędzania czasu przy klawiaturze komputera. Jedną z dziedzin, którą się wtedy Hobby Computer Cluby zajmowały, było uczestnictwo w nowo powstałej amatorskiej sieci wymiany informacji - FidoNet. 
BBS, z którym połączyli się Wilczek i Zieliński był pierwszym działającym w Polsce systemem tego typu. Znajdował się w firmie należącej do Tadeusza Wilczka i wykorzystywał oprogramowanie otrzymane od członka władz Hobby Computer Clubu oraz modem o zawrotnej na tamte czasy prędkości 1200 bps. BBS nazywał się po prostu "Fido". Choć konfiguracja BBS-u nie zawierała części odpowiedzialnej za obsługę poczty Fido, dzięki kontaktom z Hobby Computer Clubem Wilczek z Zielińskim uzyskali nawet wpis do listy adresowej FidoNet jako polski region tej sieci. 
BBS działał tylko kilkanaście godzin tygodniowo (od poniedziałkowego popołudnia do wtorku rano), ale też jego obciążenie nie było zbyt duże - osób posiadających wtedy w Polsce modem nie było wiele. Być może dlatego kiedy jakiś czas po pokazie w Muzeum Techniki BBS został przeniesiony do redakcji "Komputera", a jego Sysopem został Tomasz Zieliński, sposób działania początkowo pozostał niezmieniony. "Fido" BBS przeszedł na pracę codzienną dopiero w okresie między wrześniem 1986 a styczniem 1987 roku. 
Jeszcze zanim to nastąpiło, we wrześniu 1986 roku ruszył w Gdańsku BBS prowadzony przez Jacka Szelożyńskiego i jego kolegów z firmy "Kontakt" - pierwszy w Polsce system działający regularnie przez cały tydzień. W pierwszych miesiącach 1987 roku Sysopem znajdującego się w redakcji "Komputera" BBS-u został Jan Stożek. Do grona polskich BBS-ów jako trzeci dołączył BBS znajdujący się w gdyńskiej firmie "Samba". 
Wiosną 1987 roku Jacek Szelożyński nawiązał kontakt z Arjenem Lentzem - wówczas nastolatkiem z Amersfoort w Holandii, który prowadził własny BBS, posiadający status węzła sieci FidoNet. Otrzymał od niego oprogramowanie potrzebne do uczestniczenia w wymianie poczty i uzyskał u niego adres punktowy. Od niego z kolei potrzebne programy otrzymał Jan Stożek; wkrótce też obaj zaczęli zabiegać o uzyskanie statusu węzłów FidoNet i utworzenie polskiego regionu tej sieci, tym razem już w zgodzie z wszelkimi wymogami. Równocześnie z nimi o status węzła FidoNet ubiegali się dwaj następni Sysopi, Janusz Buchała z Krakowa i Andrzej Bursztyński z Warszawy. 
Polski region FidoNet, w którym wszyscy oni uzyskali status węzłów, został utworzony jesienią 1987 r. Pierwszy zgodny z wymogami FidoNet polski segment listy adresowej FidoNet zawierał więc cztery adresy: 
Pierwszym koordynatorem regionu (RC) i zarazem sieci (NC) został Jan Stożek, który prowadził polską sieć do początku lat 90., a polski region - do 1995 r.
W kolejnych latach, wraz ze wzrostem liczby węzłów, wydzielono sieci regionalne - North Net o numerze 481, Galicja Net (486), Silesia Net (484) i West Net (482). 
Największy rozkwit FidoNetu w Polsce datuje się na lata 1995–1996, kiedy to sieć miała ponad 300 węzłów i kilka tysięcy użytkowników. Dzięki prostocie i niskich kosztach budowy i działania, technologia FidoNetu była w Polsce w latach 90. stosowana do uruchamiania pierwszych dedykowanych sieci rozległych. Jednym z przykładów była transmisja szpitalnych statystyk medycznych w województwie katowickim. Liczba węzłów działających w Polsce w 2003 r. była mniejsza niż 100.
Pod koniec listopada 2014 r. zrezygnował ze stanowiska Przemysław Kwiatkowski, który przez wiele ostatnich lat podtrzymywał działanie FidoNetu w Polsce, będąc koordynatorem regionu (RC) i prowadząc węzeł 2:480/127. W porozumieniu z Romanem Mandziejewiczem (2:484/8), jednym z pierwszych i do końca aktywnych użytkowników FidoNetu, podjęto decyzję o likwidacji regionu, który funkcjonował praktycznie dla kilku osób. Dnia 31 grudnia 2014 r. o godzinie 23:59 (w praktyce kilka godzin po północy) nastąpiło wyłączenie ostatnich hostów i region 48 formalnie zakończył istnienie.
Ciekawostką jest, że od 1988 r. do polskiego regionu należał BBS w Moskwie, Kremlin FIDO BBS, założony w mieszkaniu Rosjanina polskiego pochodzenia, Tadeusza Radiusza we współpracy z redakcją miesięcznika "Komputer". Przejściowo było w nim także kilka pierwszych BBS-ów w Bułgarii, przed wydzieleniem odrębnego regionu bułgarskiego.
FidoNet na świecie.
Współcześnie FidoNet na całym świecie jest praktycznie wyparty przez Internet. Wyraźnie to widać na wykresie ilości węzłów przestrzeni lat. W Ameryce Północnej, gdzie FidoNet się narodził i święcił swoje pierwsze sukcesy popularności, obecnie prawie o FidoNet zapomniano.
FidoNet a Internet.
W miarę rozwoju Internetu zorganizowano bramki do przesyłania wiadomości przez Internet jako poczta elektroniczna oraz do sprzęgania konferencji z grupami dyskusyjnymi. Niektóre konferencje trafiły na ogólnodostępne serwery Usenetu, np. do hierarchii "fido.*". Ze względu na zamknięty, prywatny charakter wielu konferencji użytkownicy Fidonetu uruchamiają też własne serwery NNTP z ograniczonym dostępem.
Od lat 2000 łączność telefoniczna była stopniowo wypierana na rzecz komunikacji przez Internet.

</doc>
<doc id="1575" url="https://pl.wikipedia.org/wiki?curid=1575" title="Fructidor">
Fructidor

Fructidor (z łac. "fructus" = 'owoce' + z gr. "doron" = 'dar') – dwunasty miesiąc we francuskim kalendarzu rewolucyjnym, trzeci i ostatni miesiąc lata – tym samym ostatni miesiąc roku. Trwał od 18 sierpnia do 16 września.

</doc>
<doc id="1576" url="https://pl.wikipedia.org/wiki?curid=1576" title="Fortunki">
Fortunki

Fortunki (, znaczenie pierwotne: wróżby) – zbiory cytatów popularne na Uniksie.
Do czytania ich służy program "fortune", który wybiera losowo jedną fortunkę (można ograniczyć zakres, np. tylko do krótkich fortunek lub fortunek na jakiś konkretny temat).
Zestawy fortunek tworzą często informatyczne kanały na IRC-u.

</doc>
<doc id="1577" url="https://pl.wikipedia.org/wiki?curid=1577" title="Felix Klein">
Felix Klein

Felix Christian Klein (ur. 25 kwietnia 1849 w Düsseldorfie, zm. 22 czerwca 1925 w Getyndze) – niemiecki matematyk, autor programu erlangeńskiego. Laureat Medalu Copleya.
Życiorys.
Felix Klein urodził się 25 kwietnia 1849 roku w Düsseldorfie jako drugie z czworga dzieci wysokiego urzędnika państwowego, radcy księgowego (niem. "Rechnungsrat") Caspara Kleina (1809–1889) i jego żony Sophie Elise Kayser (1819–1890). Po początkową kształceniu w domu przez matkę i ponad dwóch latach w szkole podstawowej, uczył się w latach 1857–1865 w gimnazjum humanistycznym w Düsseldorfie. W 1865 roku zdał maturę i powziął decyzję o podjęciu studiów matematycznych i przyrodniczych. 
W latach 1865–1870 studiował matematykę na uniwersytetach w Bonn, Getyndze i Berlinie. Na uniwersytecie w Bonn był uczniem Juliusa Plückera (1801–1868), który zaznajomił go z odkryciami w zakresie geometrii analitycznej. Został asystentem Plückera na zajęciach z fizyki eksperymentalnej. Po śmierci Plückera w 1868 roku rodzina profesora zleciła mu przygotowanie do druku drugiego tomu dzieła "Liniengeometrie". W 1868 roku uzyskał w Bonn stopień doktora – pracę doktorską napisał pod kierunkiem Rudolfa Lipschitza (1832–1903). 
W 1869 roku studiował u Alfreda Clebscha (1833–1872) na uniwersytecie w Getyndze, a następnie brał udział w seminariach prowadzonych przez Karla Weierstrassa (1815–1897) i Ernsta Eduarda Kummera (1810–1893) na uniwersytecie Humboldtów w Berlinie. W Berlinie poznał norweskiego matematyka Mariusa Sophusa Liego (1842–1899), z którym w 1870 roku wyjechał do Paryża, gdzie dzięki Camille’owi Jordanowi (1838–1922) zaznajomili się z teorią Galois. Wybuch wojny francusko-pruskiej zmusił go do powrotu do Prus. Uznany za niezdolnego do służby wojskowej, służył na froncie przez kilka tygodni jako paramedyk, po czym zaraził się tyfusem i wrócił do rodzinnego domu w Düsseldorfie. W 1871 roku ukończył pracę habilitacyjną pod kierunkiem Clebscha na uniwersytecie w Getyndze, gdzie objął stanowisko wykładowcy (niem. "Privatdozent"). W latach 1872–1875 pracował na uniwersytecie w Erlangen, a w 1875 roku związał się z Uniwersytetem Technicznym w Monachium. 
W 1875 roku Klein poślubił Annę Hegel (1859–1927), córkę historyka Karla Hegla (1813–1901) i wnuczkę filozofa Georga Wilhelma Friedricha Hegla (1770–1831). Para miała syna i trzy córki.
W Monachium Klein zainicjował powstanie Instytutu Matematyki i opracował obszerny program dla swoich własnych zajęć dydaktycznych z matematyki wyższej, który został wprowadzony na innych uniwersytetach technicznych. W 1880 roku Klein wraz ze swoim asystentem Waltherem von Dyckiem (1856–1934) przeniósł się na uczelnię w Lipsku. W latach 1881–1882 oddał się intensywnej pracy badawczej, po czym poświęcił się pisaniu podręczników. W 1886 roku powrócił do Getyngi, gdzie pracował do emerytury w 1913 roku. Jego uczniem był m.in. Arnold Sommerfeld (1868–1951). Klein zmarł 22 czerwca 1925 roku w Getyndze.
Działalność naukowa.
Klein zajmował się geometrią nieeuklidesową, teorią grup ciągłych, teorią funkcji eliptycznych i automorficznych i teorią grup, a także dydaktyką i reorganizacją nauczania matematyki. Był jednym z inicjatorów powstania encyklopedii "Encyklopädie der mathematischen Wissenschaften" (1898). Od 1874 roku redagował czasopismo Mathematische Annalen. W latach 1908–1920 był przewodniczącym Międzynarodowej Komisji ds Nauczania Matematyki (ang. "International Commission on Mathematical Instruction", ICMI).
W 1872 roku, obejmując stanowisko profesora na uniwersytecie w Erlangen, wygłosił wykład – „Rozważania porównawcze dotyczące najnowszych badań geometrycznych”, którego tezy stały się później programem erlangeńskim – podstawą nowoczesnej klasyfikacji pojęć i twierdzeń geometrii. W wykładzie tym usystematyzował geometrię – uznał, że geometrię można podzielić na rozmaite działy w zależności od tego, jaką grupę przekształceń weźmie się za podstawę. Wyróżnił m.in.:
Każda takich geometrii bada tylko te własności figur, które nie zmieniają się (są niezmiennikami) przy dokonywaniu transformacji przestrzeni należących do określonej grupy przekształceń.
Upamiętnienie.
Od 1999 roku European Mathematical Society przyznaje co cztery lata nagrodę im. Felixa Kleina (ang. "Felix Klein Prize"). Od 2003 roku, co dwa lata, Międzynarodowa Komisja ds Nauczania Matematyki (ang. "International Commission on Mathematical Instruction", ICMI) przyznaje nagrodę im. Felixa Kleina (ang. "Felix Klein Award") za życiowe osiągnięcia w dziedzinie matematyki. Na cześć Kleina nazwano jedną z planetoid z pasa głównego asteroid – (12045) Klein.

</doc>
<doc id="1578" url="https://pl.wikipedia.org/wiki?curid=1578" title="Frans Eemil Sillanpää">
Frans Eemil Sillanpää

Frans Eemil Sillanpää (ur. 16 września 1888 w Hämeenkyrö, zm. 3 czerwca 1964 w Helsinkach) – jeden z najwybitniejszych powieściopisarzy i nowelistów fińskich XX wieku, przedstawiciel ruchu modernistycznego w literaturze oraz laureat literackiej Nagrody Nobla za 1939 rok, którą − z uzasadnienia jury − otrzymał za „głębokie przeniknięcie w życie fińskich chłopów i wyborne opisanie ich obyczajów w więzi z przyrodą”.
Życiorys.
Pochodził z niezamożnej rodziny – kosztem wyrzeczeń ojca ukończył szkołę w okolicy Tampere. Dzięki finansowemu wsparciu przyjaciół rozpoczął w 1908 studia przyrodnicze na Uniwersytecie w Helsinkach. Poznał wówczas środowisko artystyczne stolicy Finlandii, m.in. Jana Sibeliusa, Eero Järnefelta i Juhani Aho'ego. Członkowie tych kręgów silnie oddziaływali na jego formację artystyczną. W grudniu 1913, po rozczarowaniach miejskim życiem, przerwał studia, których już nigdy nie ukończył i wrócił do wsi rodzinnej. Pierwszą powieść ("Słońce życia") wydał w 1916.
Twórczość.
Był autorem realistycznych, nasyconych liryzmem powieści o wymowie humanistycznej, osadzonych głównie w hermetycznym środowisku wiejskim miejsc znanych Fransowi Sillanpää z dzieciństwa.
Pomimo realizmu powieści fińskiego pisarza, prezentowany w nich obraz życia na wsi często podnoszony jest do rangi mistycznej oraz kosmogonicznej. Sillanpää mocno akcentuje przy tym kwestię Jedności wszechrzeczy, przeczucie doskonałego zjednoczenia człowieka z wszechświatem, które przejawia się w najprostszym bezpośrednim kontakcie jednostki ludzkiej z naturą. W powieściach takich jak "Słońce życia" (1916) czy "Sierpień" (1941) życie na wsi jest jednocześnie dotknięciem "unio mystica"; platonicznym symbolem pradawnego obcowania człowieka z Bogiem, powrotem do utraconego Raju, w którym panuje bezczas, doskonałość i pełnia w doświadczeniu zarówno indywidualnej jak i uniwersalnej egzystencji.Osobną kategorię stanowią późne powieści Fransa Sillanpää, które powstały na gruncie osobistych przeżyć autora związanych z załamaniem psychicznym oraz chwilową utratą talentu pisarskiego. Utwory te − przesiąknięte typowo dekadenckim nastrojem melancholii, rozpaczy i nihilizmu − opowiadają głównie o „wypalonych” artystach pozbawionych nadziei oraz ludziach z marginesu społecznego.
Napisał też kilka poematów.

</doc>
<doc id="1579" url="https://pl.wikipedia.org/wiki?curid=1579" title="Frankowie">
Frankowie

Frankowie (, ) – zachodniogermańska federacja plemion, u swoich uchwytnych źródłowo początków, tj. w III w. n.e., zamieszkująca tereny na północ i wschód od dolnego Renu. Między trzecim a piątym wiekiem część Franków najeżdżała terytorium Cesarstwa Rzymskiego, gdy inna część weszła w skład rzymskich wojsk w Galii. Tylko Frankowie saliccy utworzyli królestwo na terenach rzymskich. Pod wodzą rodzimej dynastii Merowingów podbili niemal całą Galię. Pod władzą Karolingów państwo to stało się wiodącą siłą chrześcijańskiego Zachodu. Jego rozpad dał początek dwóm wiodącym siłom średniowiecza: królestwu Francji i Świętemu Cesarstwu Rzymskiemu.
Powstanie.
Frankowie powstali w III wieku n.e. z wymieszania mniejszych plemion germańskich (Ampsivariów, Brukterów, Chamawów, Chattuariów) i Franków salickich, mieszkających nad dolnym Renem. Pierwsza wzmianka o Frankach w źródłach pisanych pochodzi z roku 253 lub 257. W połowie IV wieku Frankowie osiedli w północnej Galii, w tzw. Toksandrii (ziemie położone pomiędzy rzekami: Moza i Skalda, część późniejszej Geldrii i Brabancji).
Podboje.
Frankowie saliccy, pokonując inne plemiona, podbili w V wieku Galię. Podbój rozpoczął Childeryk, ojciec Chlodwiga, który miał swoją siedzibę w Tournai (gdzie w 481 roku został pochowany). Około roku 486 Chlodwig dokonał zwycięskiego ataku na tereny określane jako "Belgica Secunda". W 491 ogłosił się panem ludu, z którego pochodziła jego matka (Turyngów) i odniósł zwycięstwo nad Alamanami, którzy byli w konflikcie także z innym odgałęzieniem Franków – Frankami rypuarskimi. Chlodwig interweniował też w Burgundii, gdzie usiłował wzmocnić stronnictwo swego sojusznika, Godigisela. Wojna doprowadziła jednak do umocnienia Gundobada, wroga Franków.
W 507 roku wybuchł konflikt z królestwem Wizygotów, których ziemie sięgały Loary. W tym konflikcie wspierał Chlodwiga król Burgundii. Bitwa stoczona z Wizygotami pod Vouillé w okolicach Poitiers przyniosła zwycięstwo Chlodwigowi. Król Wizygotów, Alaryk II został zabity, a akwitańskie prowincje królestwa wizygockiego znalazły się w rękach Chlodwiga. Mógł on osiągnąć więcej, gdyby nie interwencja króla Ostrogotów Teodoryka.
Wiadomo, że ostatni okres panowania Chlodwiga wypełniła eliminacja innych małych królestw frankijskich z ośrodkami w Kolonii, Cambrai, Thérouanne i Le Mans, z których większość była rządzona przez jego krewnych. Chlodwig umarł 27 listopada 511 roku.
Tak jak w przypadku Teodoryka ważna była dla panowania Chlodwiga nie rywalizacja z Rzymem, ale zdominowanie rywalizujących ze sobą plemion: Franków, Burgundów, Sasów i potomków rzymskich przywódców. Ze względu na oddalenie od Bizancjum cesarz wschodni łaskawszym okiem patrzył na Chlodwiga i Franków niż Teodoryka i Ostrogotów. Bizancjum (cesarz Anastazy I) uznało władzę Chlodwiga i w 508 roku przyznało mu konsulat honorowy. Być może wiązało się to z przejściem króla na chrześcijaństwo. Ta różnica w postawie wobec Franków i Ostrogotów wpłynęła zasadniczo na dalsze losy obu królestw.
Źródła wiedzy o Frankach.
Informacji na temat Franków jest mało, pochodzą praktycznie jedynie z dzieła "Dziesięć ksiąg historii kościelnej Franków", napisanej przez biskupa Grzegorza z Tours (538–594).

</doc>
<doc id="1580" url="https://pl.wikipedia.org/wiki?curid=1580" title="Florence Nightingale">
Florence Nightingale

Florence Nightingale (ur. 12 maja 1820 we Florencji, zm. 13 sierpnia 1910 w Londynie) – angielska pielęgniarka, statystyk, działaczka społeczna i publicystka. Była zwana „Damą z lampą” (ang. "The Lady with the Lamp"). Jest uważana za twórczynię nowoczesnego pielęgniarstwa.
Młodość.
Urodziła się w bogatej arystokratycznej rodzinie. Jej ojcem był William Edward Nightingale. Florence i jej siostra Parthepone urodziły się podczas długiej podróży poślubnej rodziców po Europie. Młodość spędziła w Londynie, latem przebywając w wiejskiej posiadłości w Hampshire.
Kariera zawodowa.
W wieku 24 lat ogłosiła, że pragnie zostać pielęgniarką. Swoją decyzją przeraziła rodzinę: pielęgniarki wówczas rekrutowały się zazwyczaj spośród prostytutek i osób z niskich warstw społecznych. W opinii rodziny żadna szanująca się dama nie mogłaby wykonywać tak haniebnego zajęcia. Florence była jednak stanowcza i w 1845 zaczęła odwiedzać szpitale i inne placówki medyczne, gromadząc informacje na ich temat.
W 1851 zaczęła uczyć się zawodu pielęgniarki w Ewangelickim Zakładzie Diakonijnym w Kaiserswerth. Spotkała tam Amalie Sieveking, którą poznała prawdopodobnie podczas pobytu w Londynie. Do spotkania doszło dzięki przyjacielowi Amalie, Christianowi von Bunsenowi. W 1853 Florence przyjęła stanowisko przełożonej w Zakładzie Opieki dla Chorych Dam w Londynie i wykorzystała okazję, aby uczynić z owej instytucji wzorowy szpital swoich czasów. Podjęła się również szkolenia pielęgniarek, którym zapewniała odpowiednie wykształcenie zawodowe i dbała o ich poziom moralny, co miało przyciągać do zawodu kobiety o nieposzlakowanej reputacji.
Wojna krymska.
W czasie wojny krymskiej na prośbę Sidneya Herberta zorganizowała od podstaw opiekę nad rannymi żołnierzami, zwalczając uprzedzenia i sprzeciwy lekarzy, urzędników i oficerów. Wielu żołnierzy brytyjskich rannych w czasie wojny zawdzięczało jej życie. Jako przełożona zespołu 38 angielskich pielęgniarek w szpitalu w Scutari (obecnie: Üsküdar, azjatycka dzielnica Stambułu) zdołała dzięki swej energii i zaangażowaniu poprawić fatalny stan sanitarny brytyjskich szpitali polowych. Odkryła, że ranni żołnierze umierają nie tylko z powodu obrażeń, lecz także na skutek szoku pourazowego, że potrzebują nie tylko leczenia, ale i opieki. Do Anglii wróciła po wojnie pod przybranym nazwiskiem, załamana tym, że pomimo jej wysiłków, nie udało jej się powstrzymać wysokiej śmiertelności spowodowanej chorobami zakaźnymi. Później została ekspertem Armii Brytyjskiej ds. wojskowej służby pielęgniarskiej.
Wpływ na rozwój pielęgniarstwa.
Nightingale wypracowała podstawy, na których opiera się współczesny wizerunek pielęgniarki. Jej zasługą jest zdefiniowanie metod i sposobów pielęgnacji chorych i poszkodowanych. W 1860 założyła w Londynie przy Szpitalu św. Tomasza pierwszą szkołę pielęgniarstwa – The Nightingale Training School (obecnie: The Florence Nightingale School of Nursing and Midwifery).
W 1907 roku przyznano jej jako pierwszej kobiecie brytyjski Order Zasługi, a pięć lat po jej śmierci wzniesiono ku jej czci pomnik na placu Waterloo w Londynie.
Medal Florence Nightingale.
W 1912 Międzynarodowy Komitet Czerwonego Krzyża ustanowił Medal Florence Nightingale, będący prestiżowym odznaczeniem, przyznawanym zasłużonym pielęgniarkom z całego świata.
Efekt Florence Nightingale.
 nazywa się sytuację, w której w opiekunie następuje rozwój uczuć romantycznych, także pociągu seksualnego do swoich pacjentów, nawet przy bardzo małej komunikacji. Uczucia mogą zanikać, gdy pacjent nie potrzebuje opieki.

</doc>
<doc id="1583" url="https://pl.wikipedia.org/wiki?curid=1583" title="FFTW">
FFTW

FFTW () – biblioteka do obliczania dyskretnych transformat Fouriera.
FFTW jest najszybszą niezależną od sprzętu biblioteką tego typu. Inne biblioteki o porównywalnych osiągach składają się z ręcznie optymalizowanego kodu asemblera, natomiast większość kodu FFTW jest generowana z zapisu w języku OCaml. Ponadto FFTW w czasie wykonania w fazie zwanej „planowaniem” dostosowuje się do właściwości danej maszyny – nie tylko procesora, ale również wykorzystuje cechy pamięci cache. Wykorzystuje do tego optymalizator, który stara się zdekomponować problem na prostsze podproblemy. FFTW wykorzystuje poza standardowymi wariantami algorytmu FFT Cooley-Tukeya (dobry dla potęg 2), również algorytmy przydatne dla potęg dużych liczb pierwszych – takie jak algorytm FFT Radera oraz algorytm FFT Bluesteina.
FFTW jest biblioteką języka C, ale można jej używać także z Fortrana, C++ oraz D. Istnieją wersje tej biblioteki dla architektury SMP, a także dla obliczeń rozproszonych. FFTW od wersji 1.3 jest dostępna na licencji GPL (wcześniej była darmowa dla użytku niekomercyjnego); autorzy umożliwiają również uzyskania jej na innej, niewolnej licencji. Programy używające FFTW to m.in. GNU Octave i MATLAB.
Przykład użycia.
Przykładowy kod (dla wersji 2 FFTW):
int main()
 fftw_plan pl1,pl2;
 fftw_complex in[128], mid[128], out[128];
 int i;
 for (i=0; i&lt;128; i++)
 in[i].re = sin (M_PI*i/16);
 in[i].im = sin (M_PI*i/24);
 pl1 = fftw_create_plan (128, FFTW_FORWARD, FFTW_ESTIMATE);
 pl2 = fftw_create_plan (128, FFTW_BACKWARD, FFTW_ESTIMATE);
 fftw_one (pl1, in, mid);
 fftw_one (pl2, mid, out);
 for (i=0; i&lt;128; i++)
 out[i].re /= 128;
 out[i].im /= 128;
 fftw_destroy_plan (pl2);
 fftw_destroy_plan (pl1);
 for (i=0; i&lt;128; i++)
 printf ("%d: in=(%f,%f), out=(%f,%f), d=(%f,%f)\n", i, in[i].re, in[i].im,
 out[i].re, out[i].im, out[i].re - in[i].re, out[i].im - in[i].im);
 return 0;

</doc>
<doc id="1584" url="https://pl.wikipedia.org/wiki?curid=1584" title="Fantasy">
Fantasy

Fantasy – gatunek literacki lub filmowy używający magicznych i innych nadprzyrodzonych form, motywów, jako pierwszorzędnego składnika fabuły, myśli przewodniej, czasu, miejsca akcji, postaci i okoliczności zdarzeń. Fantasy jest na ogół odróżniane od science fiction oraz od horroru, przy założeniu, że względnie nie wchodzi w tematykę naukową (w sensie SF) lub tematykę grozy (ang. "macabre"). Jednak gatunki te w znacznym stopniu przenikają się. Wszystkie trzy mieszczą się w pojęciu fantastyki (ang. "speculative fiction").
W kulturze popularnej gatunek fantasy zdominowany jest przez odmianę mediewistyczną, zwłaszcza od czasu ogólnoświatowego sukcesu "Władcy Pierścieni" i innych dzieł J.R.R. Tolkiena wchodzących w skład uniwersum Śródziemia. W szerszym znaczeniu, fantasy obejmuje prace wielu pisarzy, artystów, filmowców, muzyków, począwszy od starożytnych mitów i legend, aż po wiele ostatnich tytułów, znanych obecnie szerokiej publiczności.
Historia.
Pierwiastki mityczne i inne, które stanowią źródło fantasy i jego licznych podgatunków, były częścią niektórych największych i najbardziej sławnych dzieł literatury takich jak "Epos o Gilgameszu" i innych najdawniejszych zapisów znanych ludzkości. Od "Odysei" po "Beowulfa", od "Mahabharaty" do "Księgi tysiąca i jednej nocy", od "Ramajany" do "Podróży na Zachód", i od Legend arturiańskich i średniowiecznych romansów po poemat epicki "Boska komedia" – fantastyczne opowieści o odważnym bohaterze i bohaterce, o strasznych, śmiercionośnych smokach i tajemniczych światach, inspirowały wielu słuchaczy i autorów. W tym sensie historia fantastyki i historii literatury są nierozerwalnie związane. Istnieje wiele dzieł, gdzie granica pomiędzy fantastyką a innymi gatunkami jest zatarta, a pytanie czy autorzy wierzyli w możliwość istnienia opisywanych cudów w utworach takich, jak "Sen nocy letniej" czy "Pan Gawen i Zielony Rycerz", sprawia, że trudno rozróżnić, gdzie i kiedy fantastyka w nowoczesnym sensie ma swój początek.
W 1841 ukazało się "The King of the Golden River" Johna Ruskina, jednak za prekursora nowoczesnej fantasy uznaje się raczej George MacDonalda – szkockiego autora powieści takich jak: "Królewna i Goblin" (w innym tłumaczeniu: "Księżniczka i koboldy") czy "Phantastes" (1858). MacDonald miał duży wpływ zarówno na J.R.R. Tolkiena, jak i na C.S. Lewisa. Innym istotnym autorem w tym gatunku był William Morris, znany angielski poeta, który napisał kilka powieści u schyłku stulecia, między innymi "The Well at the World’s End" (1896). Według historyków gatunku to Morris jako pierwszy wpadł na pomysł stworzenia odrębnego, niemimetycznego świata przedstawionego jako miejsca akcji.
Mimo że "Na skrzydłach Północnej Wichury" MacDonalda (1871) miało duży wpływ na przyszłość, i mimo że był wówczas popularny, literatura fantasy nie była szeroko znana lub ceniona aż do końca stulecia. Dopiero Edward Plunkett, znany jako Lord Dunsany, sprawił, że gatunek stał się popularny zarówno w postaci powieści, jak i opowiadania; duże znaczenie miały zwłaszcza jego powieści z lat 1905–1919 usytuowane w fantastycznym świecie zwanym Pegana. Wielu znanych „mainstreamowych” współcześnie autorów również zaczęło pisać w tym gatunku – np. Henry Rider Haggard, Rudyard Kipling czy Edgar Rice Burroughs. Ci właśnie, obok Abrahama Merritta, stworzyli to, co znane jest jako podgatunek „zaginionego świata”, który był najbardziej popularny na początku XX wieku. W tym samym czasie ukazały się również utwory dla dzieci, takie jak "Piotruś Pan" Jamesa Matthew Barrie czy "Czarnoksiężnik z Krainy Oz" Lymana Franka Bauma.
Fantastyka w literaturze dla dzieci była akceptowana łatwiej niż fantastyka przeznaczona dla dorosłych, co powodowało, że pisarze chcący tworzyć fantastykę często decydowali się ubrać swoje dzieło w postać książki dla dzieci. Wiele wczesnych prac Nathaniela Hawthorne’a znajdowało się na granicy fantastyki, ale książka "A Wonder-Book for Girls and Boys", przeznaczona dla dzieci, to fantasy.
W 1923 w USA powstał pierwszy magazyn poświęcony fantasy pt. "Weird Tales". Termin ten został jednak użyty po raz pierwszy prawdopodobnie dopiero w 1927 za sprawą E. M. Forestera. Nieco później wykorzystywany był między innymi przez C. S. Lewisa, który szukał odpowiednich słów dla określenia twórczości swojej i J. R. R. Tolkiena.
W ślad za "Weird Tales" pojawiło się wiele podobnych wydawnictw, np. "The Magazine of Fantasy &amp; Science Fiction". Ten właśnie magazyn był w tamtym czasie bardzo popularny i przybliżył fantasy, ale także i fantastykę naukową szerszej publiczności w USA i Wielkiej Brytanii. Wówczas już, te dwa gatunki zaczęły być kojarzone ze sobą.
W latach czterdziestych i pięćdziesiątych XX w. szeroką popularność zdobył gatunek "sword &amp; sorcery", zapoczątkowany przez Roberta E. Howarda serią opowieści o Conanie Barbarzyńcy (1932-1936), które kontynuowane były i naśladowane przez innych autorów (Henry Kuttner, L. Sprague de Camp, Lin Carter), a także dzięki opowiadaniom Fritza Leibera pt. "Przygody Fafryda i Szarego Kocura" (od 1958). Było to przed nadejściem high fantasy i przed nadejściem największej popularności prac Tolkiena: "Hobbit" (pierwsze wydanie w 1937) i "Władca Pierścieni" (1954). Popularność w późnych latach 60. pozwoliła fantasy w pełni wkroczyć do głównego nurtu kulturowego. Kilka innych serii, takich jak "Opowieści z Narni" Clive’a Staplesa Lewisa czy "Ziemiomorze" Ursuli K. Le Guin utwierdziło pozycję gatunku.
W późniejszym czasie popularność fantasy utrzymywała się, by jeszcze wzrosnąć w XXI wieku, czego dowodem jest bestsellerowy status prac J.K. Rowling, czy ogromna popularność filmów, takich jak ekranizacja "Władcy Pierścieni" autorstwa Petera Jacksona.
Cechy gatunku.
Najbardziej charakterystyczne cechy gatunku to motywy fantastyczne (przede wszystkim obecność magii) występujące w wewnętrznie spójnym miejscu akcji, oraz typowe przedmioty i postacie. Akcja utworów rozgrywa się zazwyczaj w świecie odrębnym od świata realnego (w świecie fantastycznym lub w odległej, mitologicznej przeszłości), najczęściej, choć nie zawsze, wykreowanym na wzór feudalnego średniowiecza i wykorzystującym legendy, podania bohaterskie i mity. Ten wtórny świat może być całkiem odseparowany od świata realnego lub w jakiś sposób być z nim połączony, pozwalając postaciom na wędrówkę między różnymi rzeczywistościami.
Świat przedstawiony w danym utworze posiada własne zasady, które muszą być przestrzegane, ale jego elementy uwarunkowane przez akcję muszą również posiadać ograniczenia, dając zarówno bohaterom pozytywnym, jak i negatywnym powody do walki. Motywy magiczne muszą być obarczone kosztem, a zarazem obdarzone wartością, by opowieść zachowała swoją wewnętrzną konstrukcję. Wydarzenia przedstawione mogą być nierealistyczne, ale winny rządzić się wewnętrzną logiką, być posłuszne ustalonym w danym utworze prawom oraz posiadać spójne tło i postacie.
Utwory tego nurtu są najczęściej umiejscowione w fikcyjnym świecie, w którym poza zwykłymi prawami fizyki i biologii działa też magia oraz inne nadnaturalne moce. Do świata przedstawionego należy przeważnie rozległy zestaw rekwizytów swobodnie zaczerpniętych ze średniowiecza i starożytności, rzadziej z okresu renesansu lub epoki wczesnego przemysłu. Te dwie ostatnie epoki częściej obecne są w stylach „dark fantasy”, „steam fantasy” i „science fantasy”. Charakterystyczne jest także występowanie obok gatunku ludzkiego gatunków istot mitycznych lub baśniowych, takich jak: elfy, krasnoludy, trolle, gobliny, smoki, wiedźmy, fauny, koboldy, gargulce, centaury, jednorożce, driady. Zazwyczaj psychologia postaci, ich motywacje i zachowania są przedstawione realistycznie – dotyczy to także postaci o pochodzeniu mitycznym lub baśniowym. Świat w książkach fantasy jest fikcyjny, mitologiczny lub baśniowy, czasem mniej lub bardziej umowny, ale potraktowany przez autora jak świat historyczny, który kiedyś istniał, podobnie jak kronika. Niezwykła moc oddziaływania tego gatunku istnieje poprzez uprawdopodobnienie postaci i precyzję opisu ich świata.
W fantasy unika się sformułowań takich jak „dawno temu” lub „w odległym kraju”. Są one typowe dla bajek i baśni tradycyjnych, a fantasy jest przeciwieństwem nieokreśloności i braku precyzji, bo one nie wystarczają odbiorcy „współczesnych baśni”. Niejednokrotnie wśród magicznego i heroicznego świata ukryte są aluzje do współczesności i jej realnych problemów. Te „współczesne baśnie” są gatunkiem nastawionym, zaprogramowanym na dylematy moralne oraz na kontestację różnych przejawów i zjawisk realnej współczesności. Ich często występującą cechą jest swoiste proekologiczne nastawienie i dowartościowywanie myślenia irracjonalnego, instynktownego oraz świata uczuć i religijności przeciwnego zaawansowanej technologii i stechnicyzowaniu życia. Mówi się nawet, że w tym gatunku, tak bardzo eskapistycznym, mniej ważne jest to, „o czym on jest”, a bardziej to, „o czym on nie jest”. Czego używania i opisywania unika, tego istnieniu się sprzeciwia.
Fantasy to opowieści, które czerpią z mitologii różnych ludów. Imiona starożytnych bóstw i bohaterów przewijają się w wielu utworach. Najczęściej autorzy inspirują się mitami: skandynawskimi, celtyckimi, greckimi, rzymskimi, egipskimi, prekolumbijskimi, anglosaskimi, słowiańskimi, jak również judeochrześcijańskimi czy bliskowschodnimi. Również legenda arturiańska jest jedną z najważniejszych podstaw i źródeł tego gatunku. Utwory odtwarzają starożytny świat tych mitologii, nadając imionom znanych postaci mitycznych (Baldur, Artur, Odys, Thor, Morgana, Merlin) współczesny i bardziej ludzki wymiar. Ale nie do końca odtwarzają, równie często przetwarzają, przerysowują, parodiują (Terry Pratchett, Andrzej Sapkowski) i wyśmiewają. Znajdziemy w nich cały przekrój gatunków literackich, nastrojów i idei, od religijnej przypowieści, baśni chrześcijańskiej, przez romans heroiczny, epos, dramat obyczajowy, fantazję polityczną, aż do horroru, love story, powieści psychologicznej, komedii, parodii, a nawet manifestów religii neopogańskich.
Zaczątki fantasy widać było już w drugiej połowie XIX wieku, u końca epoki romantyzmu, lecz jej złoty wiek przypada na lata 60. i 70. XX wieku, podczas gdy złoty wiek jej „starszej siostry” – fantastyki naukowej – przypada na lata 50. i 60.
Fantasy w Polsce i za granicą.
Na rynku anglosaskim książki fantasy to te historie, w których występuje wątek nadprzyrodzony, bądź technologia tak zaawansowana, że trudno wyjaśnić obecnie sposób jej działania. Stanisław Lem wydzielił zaś fantasy jako odmianę współczesnej baśni, zawężając ten termin do dzieł, które są w jakiś sposób powiązane z klasycznymi opowieściami. Z tego powodu polskie znaczenie tego słowa może wydawać się znacznie stabilniejsze niż jego anglosaski odpowiednik, pozwalając na łatwiejsze znalezienie gatunkowej tożsamości pozornie zupełnie innych od siebie dzieł.
Podgatunki.
"Fantasy" posiada wiele podgatunków, ważniejsze z nich to: low fantasy (quasi-historyczna), dark fantasy, high fantasy, urban fantasy, space fantasy oraz heroic fantasy (magia i miecz).
Podgrupy owe nie oznaczają większych różnic w samej treści, czy w ideach propagowanych przez autorów, lecz raczej w stylu pisania, źródłach inspiracji oraz w ogólnym „nastroju” dominującym w utworach, na przykład mrocznym, baśniowym, heroicznym, pseudohistorycznym, dziejów alternatywnych, postapokaliptycznym, czy religijnym. Utwory, które można opisać tą ogólną nazwą, są dość rozmaite i adresowane do czytelników w różnym wieku, od dzieci, przez młodzież, do dorosłych. Większość klasycznych dzieł tego gatunku przeznaczona jest raczej dla dorosłych czytelników. Stąd fantasy niekiedy nazywana jest też, dość obrazowo, ale w dużym uproszczeniu, „baśnią dla dorosłych”.
Niektóre z utworów fantasy trudno jednoznacznie sklasyfikować, sporna bywa też w ogóle przynależność do tego nurtu: przykładem może być cykl "Kroniki Amberu" Rogera Zelazny’ego, autora, który deklarował się jako piszący wyłącznie fantastykę naukową.
Twórcy powiązani z fantasy.
Literatura fantasy posiada długą kolekcję okołogatunkowych działań, w których udział biorą autorzy oraz ich fani. Powstają więc kluby, gry fabularne (przy czym wiele utworów fantasy jest pisanych na potrzeby gier fabularnych i osadzonych w konkretnych światach), dramaty sceniczne, poezje, opowiadania, komiksy oraz tzw. fanfiction, filmy fabularne, gadżety, grafika komputerowa, malarstwo fantasy oraz muzyka inspirowana tą twórczością – są to najczęściej gatunki muzyki popularnej, głównie folk, heavy metal, power metal, rock, metal symfoniczny, gotyk oraz muzyka poważna i etniczna. Muzyka inspirowana twórczością fantastyczną zwana jest niekiedy muzyką fantastyczną lub muzyką tolkienowską.

</doc>
<doc id="1585" url="https://pl.wikipedia.org/wiki?curid=1585" title="Fanfiction">
Fanfiction



</doc>
<doc id="1586" url="https://pl.wikipedia.org/wiki?curid=1586" title="Fanfic">
Fanfic



</doc>
<doc id="1588" url="https://pl.wikipedia.org/wiki?curid=1588" title="Foton">
Foton

Foton (gr. φῶς – "światło", w dopełniaczu – φωτός, nazwa stworzona przez Gilberta N. Lewisa) – cząstka elementarna z grupy bozonów, będąca nośnikiem oddziaływań elektromagnetycznych (bozon cechowania). Nie posiada ładunku elektrycznego ani momentu magnetycznego, jego masa spoczynkowa jest zerowa ("m"0 = 0), a liczba spinowa "s" ma wartość 1. Wykazuje dualizm korpuskularno-falowy, więc równocześnie ma cechy cząstki i fali elektromagnetycznej.
W fizyce foton jest kwantem pola elektromagnetycznego, np. światła widzialnego. W mechanice kwantowej pole elektromagnetyczne zachowuje się jak zbiór cząstek (fotonów). Z kwantowego punktu widzenia światło jest dużym strumieniem fotonów. Bardzo czułe instrumenty optyczne potrafią rejestrować pojedyncze fotony.
W zależności od energii fotonów, promieniowanie, na które się składają, ma inną nazwę. I tak mówi się (poczynając od najwyższej energii fotonu) o promieniowaniu gamma, rentgenowskim (promieniowaniu X), nadfiolecie, świetle widzialnym, podczerwieni, mikrofalach, falach radiowych (promieniowaniu radiowym). Jednak z fizycznego punktu widzenia wszystkie te rodzaje promieniowania mają jednakową naturę.
Fotony w próżni poruszają się z prędkością światła. W ośrodkach przezroczystych ta prędkość jest mniejsza i zależy od energii. W próżni fotony mogą pokonywać dystanse wielu miliardów lat świetlnych, poruszając się po torach lekko tylko zakrzywianych przez pola grawitacyjne ciał niebieskich. Zakrzywienie to, przy odpowiedniej konfiguracji źródła i masy powodującej zakrzywienie, może prowadzić do efektu soczewkowania grawitacyjnego. Jedynie czarne dziury mają wystarczająco silne pole grawitacyjne, by móc uwięzić światło wewnątrz horyzontu zdarzeń.
Historia.
Aż niemal do końca XVII wieku większość teorii zakładała, że światło składa się z cząstek. Ponieważ model cząsteczkowy nie może łatwo wyjaśnić załamania, dyfrakcji i dwójłomności, powstały teorie o falowej naturze światła, zaproponowane przez Kartezjusza (1637), Roberta Hooke’a (1665) oraz Christiaana Huygensa (1678). Pomimo to wciąż dominowały modele cząsteczkowe, głównie z powodu wpływu Isaaca Newtona. Na początku dziewiętnastego wieku Thomas Young i Augustin-Jean Fresnel zademonstrowali dyfrakcję oraz interferencję światła i od 1850 roku modele falowe zostały powszechnie zaakceptowane. W 1865 roku James Clerk Maxwell wysunął przypuszczenie, że światło jest falą elektromagnetyczną. Hipoteza ta została potwierdzona eksperymentalnie w 1889 roku przez Heinricha Hertza, który odkrył fale radiowe. To ostatecznie zadecydowało o odrzuceniu cząsteczkowego modelu światła.
Teoria falowa Maxwella nie wyjaśnia jednak wszystkich własności światła. Teoria ta przewiduje, że energia fali świetlnej zależy wyłącznie od jej natężenia i nie ma związku z jej częstotliwością. Pomimo to szereg różnych, niezależnych eksperymentów pokazuje, że energia przekazywana atomom przez światło zależy wyłącznie od częstotliwości światła, a nie od jego natężenia. Na przykład niektóre reakcje chemiczne są wyzwalane tylko przez światło o częstotliwości wyższej od pewnej wartości progowej, a światło o częstotliwości niższej od progowej, bez względu na jego natężenie, nie zapoczątkuje reakcji. Podobnie elektrony mogą zostać wybite z metalowej płytki przez oświetlanie jej światłem o wystarczająco wysokiej częstotliwości (efekt fotoelektryczny), a maksymalna energia wybitych elektronów zależy jedynie od częstotliwości światła.
W tym samym czasie badania nad promieniowaniem ciała doskonale czarnego prowadzone przez ponad cztery dekady (1860–1900) przez wielu badaczy zostały uwieńczone hipotezą Maxa Plancka głoszącą, że energia wypromieniowywana przez ciało doskonale czarne ma postać cząstek. Jak wykazał Albert Einstein, pewien rodzaj kwantyzacji energii musi być założony, by wyjaśnić równowagę termiczną zachodzącą pomiędzy materią a promieniowaniem elektromagnetycznym (której nie ma od kilku lub kilkunastu miliardów lat).
Ponieważ teoria światła Maxwella dopuszczała wszystkie możliwe energie promieniowania elektromagnetycznego, większość fizyków przypuszczała początkowo, że energia kwantyzacji jest rezultatem pewnego nieznanego ograniczenia dla materii, która pochłania lub emituje światło. W 1905 roku Einstein jako pierwszy zasugerował, że energia kwantyzacji jest własnością samego promieniowania elektromagnetycznego. Chociaż Einstein uważał teorię Maxwella za słuszną, wskazał, że wiele niewytłumaczalnych zjawisk mogłoby być wyjaśnione, gdyby energia maxwellowskiej fali świetlnej była zlokalizowana w punktowych kwantach, poruszających się niezależnie od siebie, nawet jeżeli sama fala rozprzestrzenia się w przestrzeni w sposób ciągły. W 1909 i 1916 roku Einstein wykazał, że jeśli prawo Plancka opisujące promieniowanie ciała doskonale czarnego jest słuszne, kwanty energii muszą mieć pęd formula_1 co czyni je pełnoprawnymi cząstkami. Pęd fotonu został zaobserwowany eksperymentalnie przez Artura Comptona w rozpraszaniu wysokoenergetycznych fotonów na swobodnych elektronach. Fotony w takim oddziaływaniu zachowują się jak cząstki, a układ foton–elektron w zderzeniu zachowuje pęd i energię. Po kwantowym wyjaśnieniu zjawiska fotoelektrycznego był to kolejny dowód na istnienie fotonów. Arthur Compton za odkrycie tego zjawiska (nazwanego od jego nazwiska efektem Comptona) otrzymał w 1927 roku Nagrodę Nobla. Kluczowe pytanie w tym okresie brzmiało: jak połączyć maxwellowską falową teorię światła z jego cząsteczkową naturą, zaobserwowaną eksperymentalnie? Szukanie odpowiedzi na to pytanie zaprzątało Alberta Einsteina przez resztę jego życia, a odpowiedź została znaleziona w ramach elektrodynamiki kwantowej.

</doc>
<doc id="1589" url="https://pl.wikipedia.org/wiki?curid=1589" title="Fermiony">
Fermiony

Fermiony (ang. "fermion", od nazwiska włoskiego fizyka Enrico Fermiego) – cząstki posiadające niecałkowity spin wyrażony w jednostkach formula_1 (gdzie "h" to stała Plancka). Możliwymi wartościami niecałkowitymi spinu są nieparzyste wielokrotności formula_2 Dla danej wartości spinu formula_3 możliwymi wartościami rzutu spinu na dowolny kierunek są:
Na mocy twierdzenia o związku spinu ze statystyką konsekwencją posiadania niecałkowitego spinu jest to, że fermiony podlegają statystyce Fermiego-Diraca, w tym regule Pauliego.
Każda cząstka jest bozonem lub fermionem, zależnie od posiadanego spinu – twierdzenie statystyki spinowej narzuca wynikającą z niego statystykę kwantową, która odróżnia fermiony od bozonów.
Zgodnie z modelem standardowym fermiony są cząstkami elementarnymi „materii”, natomiast bozony przenoszą oddziaływania. W modelu standardowym oprócz fermionów złożonych (bariony) występują 2 typy cząstek elementarnych, które są fermionami: kwarki i leptony.
Uproszczone rozumowanie pozwalające uzyskać podział cząstek na bozony i fermiony wygląda następująco.
Występowanie spinu jest związane z operacją zamiany cząstek. Załóżmy, że mamy dany stan dwucząstkowy formula_5 Zadziałajmy na niego operatorem zamiany cząstek:
Podwójna zamiana cząstek daje nam stan początkowy, skąd otrzymujemy równanie:
Równanie to ma dwa rozwiązania: +1 i −1. Funkcje falowe symetryczne ze względu na zamianę cząstek (rozwiązania z +1) opisują bozony, natomiast funkcje antysymetryczne (rozwiązania z −1) opisują fermiony.
Rozumowanie przedstawione powyżej w rzeczywistości załamuje się w przestrzeniach o dwóch wymiarach, gdzie możliwe są także inne rodzaje cząstek, tak zwane anyony. Ponieważ w powyższym rozumowaniu wymiar przestrzeni nie został w ogóle uwzględniony, nie jest ono ani ścisłe, ani prawdziwe.
Jeżeli stany jednocząstkowe są opisywane przez funkcje falowe: formula_8 i formula_9 to stan dwucząstkowy jest opisywany przez funkcję falową postaci:
Jest to dwucząstkowa postać tak zwanego wyznacznika Slatera.

</doc>
<doc id="1591" url="https://pl.wikipedia.org/wiki?curid=1591" title="Film aktorski">
Film aktorski



</doc>
<doc id="1592" url="https://pl.wikipedia.org/wiki?curid=1592" title="Film animowany">
Film animowany

Film animowany – utwór filmowy zrealizowany za pomocą metod animacji. Składa się on z serii zdjęć, które przedstawiają następujące po sobie fazy ruchu. Zdjęcia te, zarejestrowane na taśmie, dają w czasie projekcji filmu iluzję ruchomego obrazu. 
Wyświetlenie tak otrzymanych obrazów daje na ekranie wrażenie ruchu – ludzki układ nerwowy interpretuje szybko wyświetlane obrazy jako płynny ruch.
Film animowany od strony artystycznej może poruszać problematykę specyficzną dla filmu aktorskiego, może też, z powodu braku ograniczeń wizualnych otwierać szersze możliwości tematyczne.
Techniki animacji.
Animację można tworzyć przy użyciu różnych technik:
Z powodu wysokich kosztów związanych z obróbką filmu rejestrowanego na taśmie negatywowej i problemów związanych z degradacją jakości materiału podczas długiego przechowywania obecnie dąży się do wprowadzenia procesu digitalizacji obrazu (np. uzyskiwanego w technikach animacji lalkowej) od samego początku produkcji. W tym celu wykorzystuje się urządzenia pozwalające na rejestrację obrazu w postaci cyfrowej, takie jak aparaty i kamery czy inne urządzenia wyposażone w matryce CCD i podobne. Filmy animowane, które produkuje się przy wykorzystaniu specjalistycznego oprogramowania zapisuje się zawsze w tej postaci (2D i 3D).
12 zasad animacji.
Jest to zbiór zasad opracowanych w studiach Walta Disneya w pierwszej połowie XX w.
Historia.
Na świecie.
Najwcześniejsze znane przykłady prób przedstawienia ruchu w sztuce zostały zidentyfikowane w paleolitycznych rysunkach naskalnych. Widoczne tam obrazy zwierząt posiadają zwielokrotnione
kończyny (nałożone na siebie), co wskazuje na intencje artysty i jego zamiar wzbudzenia percepcji ruchu. Innym przykładem pierwszych zarejestrowanych dążeń do przedstawienia ekspresji ruchu są ornamenty umieszczone na ceramicznych naczyniach (artefakty pochodzą sprzed 5200 lat) znalezionych w Shahr-e Sukhteh (Iran). W sztuce starożytnego Egiptu (sprzed 4000 lat) odnajdywane są przykłady umieszczania sekwencji zdarzeń (zapasy), w których układ zdecydowanie sugeruje bezpośrednie następstwo. Prezentacja kolejnych faz ruchu obiektu jest obecna w rysunkach Leonarda da Vinci.
W okresie poprzedzającym powszechną dostępność taśmy filmowej istniały urządzenia pozwalające na odtwarzanie krótkich animowanych sekwencji. Należały do nich fenakistiskop (oparty na wykorzystaniu rotacji dysku z rysunkami), zoetrop (gdzie rysunki były umieszczone na powierzchni obracającego się cylindra) oraz praksinoskop, będący rozbudowaną wersją zoetropu. Innym powszechnie stosowanym rozwiązaniem jest kineograf (ang. flipbook), gdzie do odtworzenia animacji wymagana jest bezpośrednia akcja oglądającego (w postaci czynności szybkiego wertowania książki zawierającej kolejne obrazy).
Początki nowożytnego filmu animowanego w powszechnym znaczeniu sięgają końca XIX i początku XX wieku. Po wynalezieniu metody zdjęć poklatkowych w 1906 roku wyprodukowano pierwszy film animowany zapisany na taśmie filmowej – "Humorous Phases of Funny Faces." Po przełomie dźwiękowym prężnie rozwijała się również amerykańska sztuka animacji, reprezentowana przede wszystkim przez wytwórnię Walta Disneya ("Parowiec WIllie", 1928), filmy Maxa Fleischera, a także pracujących dla Warner Bros. Texa Avery'ego ("Gorący Czerwony Kapturek", 1942) oraz Chucka Jonesa ("Duck Amuck", 1953). Z czasem konkurencją telewizyjną dla Warnera i Disneya stała się wytwórnia Hanna-Barbera, znana między innymi z serialu "Flintstonowie" (1960–1966)"."
Rozwój trójwymiarowej grafiki komputerowej w latach 70. i 80. XX wieku umożliwił pojawienie się filmów animowanych produkowanych z wykorzystaniem tej technologii. Większość najpopularniejszych tego typu filmów pochodzi ze Stanów Zjednoczonych. Pierwszym pełnometrażowym filmem animowanym trójwymiarowym był "Toy Story" (1995, reż. John Lasseter) wyprodukowany przez wytwórnię Pixar. Istotną rolę odgrywa także studio DreamWorks ("Shrek", 2001). Na świecie od lat 70. istnieją stacje telewizyjne dla dzieci emitujące w większości kreskówki (w tym własne produkcje), np. Cartoon Network, Nickelodeon czy Disney Channel, które należą do najpopularniejszych, najstarszych i najbardziej ze sobą rywalizujących.
Poza filmami animowanymi skierowanymi do masowego odbiorcy produkowane są animacje niekonwencjonalne, mogące wprowadzać elementy zaczerpnięte z tzw. sztuki wysokiej, jak np. odwołania do istniejących kierunków w malarstwie – surrealizmu, kubizmu, superrealizmu itp. Przykładem może być powstała w 2003 roku i nagrodzona w Cannes surrealistyczna długometrażowa animacja dla dorosłych "Trio z Belleville" (2003) pomysłu Sylvaina Chometa. Rozgłos poza kręgiem zachodnioeuropejskim zdobyła również japońska sztuka animacji (anime), reprezentowana między innymi przez Hayao Miyazakiego ("Mój sąsiad Totoro", 1987), Isao Takahatę ("Grobowiec świetlików", 1990), Katsuhiro Ōtomo ("Akira", 1990), Masamunego Shirowa ("Ghost in the Shell", 1995), Hideakiego Anno ("Neon Genesis Evangelion", 1995). Animowanym filmem lalkowym zajmują się artyści z Wielkiej Brytanii – reżyser Nick Park i studio filmowe Aardman (film "Uciekające kurczaki", 2000).
W Polsce.
Historia polskiej animacji sięga Władysława Starewicza, który jeszcze pod zaborem rosyjskim nakręcił "Zemstę kinooperatora" (1912), poklatkową parodię ówczesnych melodramatów rosyjskich z udziałem chrząszczy. Do innych animatorów z tamtych czasów można również zaliczyć Feliksa Kuczkowskiego który zaczął tworzyć animacje w 1916 i 1917 roku (najbardziej znanymi przykładami jest "Flirt krzesełek" oraz "Luneta ma dwa końce").
Pierwsze animacje w Polsce zaczęły powstawać już w dwudziestoleciu międzywojennym, jednak z powodu słabych warunków do pracy najczęściej decydowano się na tworzenie animowanych filmów reklamowych. Do najbardziej znanych animatorów z tamtego okresu można zaliczyć Włodzimierza Kowańke, Stanisława Dobrzyńskiego, Zenona Wasilewskiego, Karola i Marte Marczaków oraz Jana Jarosza.
Po II wojnie światowej w polskim studiu Se-ma-for w Łodzi powstawały filmy we wszelkich technikach animacji – lalkowej, rysunkowej, plastelinowej, wycinankowej i kombinowanej – w tym seriale dla dzieci z Misiem Colargolem i Misiem Uszatkiem w rolach głównych. Równie zasłużone polskie studio to Studio Filmów Rysunkowych w Bielsku-Białej, specjalizujące się w produkcji dla dzieci, gdzie powstał najpopularniejszy polski serial animowany "Bolek i Lolek" i "Reksio"; Studio Miniatur Filmowych w Warszawie, gdzie powstały "Dziwne przygody Koziołka Matołka"; wreszcie Studio Filmów Animowanych w Krakowie, z autorską animacją dla dorosłych. Działa też TV Studio Filmów Animowanych Sp. z o.o. w Poznaniu, w którym powstają w większości filmy dla dzieci w różnych technikach animacyjnych, w tym m.in. cykl filmowy "Czternaście bajek z Królestwa Lailonii Leszka Kołakowskiego", na podstawie książki tytułowego filozofa.
Do najważniejszych reżyserów filmów animowanych w Polsce należą: Walerian Borowczyk ("Dom", 1958), Jan Lenica ("Labirynt", 1962), Kazimierz Urbański ("Igraszki", 1962), Władysław Nehrebecki ("Bolek i Lolek", 1963–1978), Witold Giersz ("Czerwone i czarne", 1963), Julian „Antonisz” Antoniszczak ("Jak działa jamniczek", 1973), Jerzy Kucia ("Refleksy", 1977), Daniel Szczechura ("Test pilota Pirxa", 1978), Ryszard Czekała ("Apel", 1978), Maciej Wojtyszko ("Test pilota Pirxa", 1979), Krzysztof Kiwerski ("Awaria", 1979), Zbigniew Rybczyński ("Tango", 1980), Zbigniew Kotecki ("Zdarzenie", 1987), Piotr Dumała ("Franz Kafka", 1991), Marek Skrobecki ("D.I.M.", 1992), Tomasz Bagiński ("Katedra", 2002), Renata Gąsiorowska ("Cipka", 2016), Mariusz Wilczyński ("Zabij to i wyjedź z tego miasta", 2019).
Największym w Polsce festiwalem filmu animowanego jest Animator, odbywający się w Poznaniu.

</doc>
<doc id="1593" url="https://pl.wikipedia.org/wiki?curid=1593" title="Szybkość klatek">
Szybkość klatek



</doc>
<doc id="1594" url="https://pl.wikipedia.org/wiki?curid=1594" title="Falerystyka">
Falerystyka

Falerystyka (z łac. i gr. "phalerae", ozdoba piersi lub czoła) – nauka pomocnicza historii zajmująca się orderami, odznaczeniami i innymi odznakami i znakami honorowymi, nadawanymi osobom zasłużonym dla monarchy, państwa lub innej organizacji posiadającej prawo nadawania odznaczeń. Falerystyka, podobnie jak i weksylologia, wyodrębniła się z heraldyki, zachowując wspólne z nią metody badawcze.
Zainteresowanie falerystyką należy wiązać z ruchem kolekcjonerskim i hobbistycznym, stymulującym próby historycznego opisu i systematyzacji odznaczeń. Wcześniejsze publikacje falerystyczne mające charakter katalogów uzupełniane są opracowaniami naukowymi.

</doc>
<doc id="1595" url="https://pl.wikipedia.org/wiki?curid=1595" title="Falklandy">
Falklandy

Falklandy (inaczej: "Malwiny"; ; ) – brytyjskie terytorium zamorskie na południowym Atlantyku, około 480 km od wybrzeży Argentyny obejmujące wyspy o tej samej nazwie. Wyspy znajdują się pod administracją brytyjską, ale prawa do nich rości sobie także Argentyna.
Ustrój polityczny.
Brytyjskie terytorium zamorskie – samorządna parlamentarna monarchia konstytucyjna. Zgodnie z konstytucją, głową terytorium jest król brytyjski, na czele władzy wykonawczej stoi gubernator mianowany przez rząd brytyjski spośród lokalnych polityków. Władzę ustawodawczą sprawuje jednoizbowy parlament składający się z 10 członków – 8 wybieranych w wyborach powszechnych i dwóch mianowanych.
Historia.
Wyspy zostały odkryte w XVI wieku przez angielskiego kapitana Johna Davisa. Anglia ustanowiła swoje zwierzchnictwo nad nimi w 1761 roku. Następnie należały do Francji. W 1764 roku Louis Antoine de Bougainville założył na wyspach pierwszą kolonię (Port Louis). Potem wyspy należały do Hiszpanii, która przejęła je od Francuzów w 1767 roku. Hiszpanie zmienili nazwę głównego miasta na Puerto Soledad. Następnie Hiszpania w układzie z 22 stycznia 1771 roku scedowała Malwiny na rzecz Wielkiej Brytanii. Argentyna swoje pretensje do wysp zgłosiła w 1820 roku, a w rok później stworzyła w Port Louis garnizon wojskowy i ciężki obóz karny, jednakże w wyniku buntu miasto i wyspy przeszły pod władzę dawnych więźniów. W 1831 roku miasto zostało zburzone, by w roku 1833 powrócić do Wielkiej Brytanii dzięki pomocy marynarki USA. Od 1892 roku uznana za kolonię brytyjską. Do 1985 roku tworzyła, wraz z wyspami Georgia Południowa i Sandwich Południowy, tzw. Dependencję Falklandów. Obecnie te wyspy tworzą odrębne terytorium Georgia Południowa i Sandwich Południowy. Argentyna rości sobie prawa również do tych wysp. Konflikt o Falklandy rozpoczął się w 1982 roku zajęciem ich na okres około dwóch miesięcy przez wojsko argentyńskie (wojna o Falklandy-Malwiny). Reakcją brytyjską było wysłanie potężnej floty, która odzyskała wyspy po krótkiej walce. Od tego czasu pozostają one terytorium brytyjskim i rząd brytyjski nie dyskutuje z rządem argentyńskim na temat tego archipelagu.
Wśród mieszkańców dwukrotnie przeprowadzono referenda dotyczące przynależności politycznej wysp. W pierwszym plebiscycie z 1986 roku za utrzymaniem "status quo" (przynależności do Wielkiej Brytanii) opowiedziało się 96 proc. mieszkańców. W drugim referendum, przeprowadzonym 11 marca 2013 roku, za zachowaniem statusu terytorium zamorskiego Wielkiej Brytanii opowiedziało się 99,8 proc. głosujących przy frekwencji wynoszącej 92 proc.
Geografia.
Terytorium Falklandów obejmuje wyspy Falklandy składające się z dwóch dużych wysp noszących nazwy Falkland Wschodni (ang. "East Falkland", hiszp. "Gran Malvina") i Falkland Zachodni (ang. "West Falkland", hiszp. "Soledad"), stanowiących około 98% powierzchni archipelagu, oraz ok. 700 mniejszych o łącznej powierzchni 12 173 km². Powierzchnia wysp jest pagórkowata – najwyższy szczyt Mount Usborne sięga 705 m n.p.m. Dwie duże wyspy dzieli Cieśnina Falklandzka. Największa odległość ze stolicy archipelagu Stanley do skrajnych zachodnich wysepek (wyspa New Island) Falklandów wynosi 238 km.
Roślinność naturalna wysp to: łąki, torfowiska, wrzosowiska, mchy, porosty. Na wyspie nie występują lasy.
Klimat.
Na wyspach panuje klimat subpolarny morski, obmywają je zimne prądy morskie i smagają silne wiatry (strefa wiatrów zachodnich), roczna suma opadów wynosi około 600 mm (opady równomiernie rozłożone w roku, brak dużych ulew), średnie miesięczne temperatury powietrza wahają się od 4 °C w lipcu (zima) do 13 °C w styczniu (lato), skrajne temperatury to –8 °C zimą i 24 °C latem. Często zdarzają się też nagłe pogorszenia pogody – z ich powodu w czasie wojny o Falklandy na 64 dni trwania konfliktu łącznie przez 27 nie można było używać lotnictwa.
Religia.
Struktura religijna kraju w 2010 roku według "Pew Research Center":
Gospodarka.
Ludność zajmuje się głównie hodowlą owiec i rybołówstwem. Na użytek lokalny kopie się torf. Na znaczeniu zyskuje turystyka.
W 1998 roku wokół wysp odkryto złoża ropy naftowej. W 2012 roku wielkość złóż szacowano na 8,3 mld baryłek, choć niektóre szacunki mówią nawet o 60 mld baryłek. Eksploatacja złóż rozpoczęła się w 2010 roku.
Emisja gazów cieplarnianych.
Emisja równoważnika dwutlenku węgla z Falklandów wyniosła w 1990 roku 0,169 Mt, z czego tylko 0,013 Mt stanowiła emisja dwutlenku węgla. Główne emisje dotyczą metanu, a w nieco mniejszym stopniu podtlenku azotu. W przeliczeniu na mieszkańca emisja wyniosła wówczas 6,553 t dwutlenku węgla. Od tego czasu emisje wahają się, przy czym dość duży wzrost emisji dwutlenku węgla nastąpił w 2012. Emisje pozostałych gazów cieplarnianych nieznacznie maleją, przez co ogólna emisja pozostaje na podobnym poziomie. Głównym źródłem emisji przez cały czas była energetyka. W 2018 emisja dwutlenku węgla pochodzenia kopalnego wyniosła 0,040 Mt, a w przeliczeniu na mieszkańca 13,591 t.

</doc>
<doc id="1596" url="https://pl.wikipedia.org/wiki?curid=1596" title="Falcowanie">
Falcowanie

Falcowanie, złamywanie – jedno- lub wielokrotne składanie arkusza papieru, czystego lub zadrukowanego, na pół (najczęściej) lub w dowolnych innych proporcjach, na różne sposoby (w różnych kierunkach), w celu osiągnięcia docelowego formatu i liczby stron składki, np. trzykrotne złożenie na pół arkusza daje składkę szesnastostronicową. Każde miejsce zaginania arkusza to "falc" ("złam") i po złożeniu tworzy nową krawędź. Złamywania można dokonywać ręcznie, za pomocą kości introligatoskiej, lub maszynowo. Maszyna do falcowania to: "falcerka" czyli "złamywarka." Złamywanie powinno się wykonywać wzdłuż włókien papieru, zmniejsza to ryzyko ich pękania oraz uszkodzenia farby w miejscach zagięcia. Uprzednie przegniecenie arkusza (bigowanie), pozwala na zmniejszenie możliwość uszkodzenia włókien papieru, tyczy się to w szczególności papierów gramaturze n &gt; 150g.
Terminu falcowanie (i pokrewnych) używa się częściej niż złamywanie, ze względu na naleciałości z języka niemieckiego. Ten drugi jest terminem oficjalnym, który występuje w literaturze przedmiotu.
Rodzaje złamywania.
W maszynach drukarskich, w których realizowany jest druk ze wstęgi papieru z roli, na końcu ciągu produkcyjnego ustawiony jest "złamywak" (tzw. "falcaparat"), którego rolą jest odcinanie fragmentów zadrukowanej wstęgi i takie ich składanie oraz klejenie lub szycie (w zależności od wyposażenia agregatów), aby w efekcie otrzymać gotową składkę.

</doc>
<doc id="1597" url="https://pl.wikipedia.org/wiki?curid=1597" title="Fakt">
Fakt

Fakt – zaistniały stan rzeczy, a w rozumieniu potocznym wydarzenie, które miało miejsce w określonym miejscu i czasie. W tym sensie faktem nie może być zdarzenie, które nie miało jeszcze miejsca, można jednak mówić o przewidywaniu przyszłych faktów – czyli zdarzeń, które najprawdopodobniej wydarzą się. Zdarzenia te jednak stają się faktami dopiero wówczas, kiedy już wydarzą się.
Fakt w filozofii Wittgensteina.
Twórcą pierwszego systemu filozoficznego, w którym pojęcie faktu stanowi podstawową kategorię ontologiczną był Ludwig Wittgenstein, który zaprezentował w swym jedynym wydanym za życia dziele Tractatus Logico-Philocopicus. Zwrócił on uwagę na to, że fakty, a nie rzeczy są elementami składowymi świata rzeczywistego. W dyskursie językowym faktom zachodzącym w świecie odpowiadają zdania syntetyczne prawdziwe. Zdaniom syntetycznym fałszywym nie odpowiadają fakty w rzeczywistości. Takie zdania opisują niezaistniałe stany rzeczy. W takim rozumieniu faktu nie jest istotna relacja czasowa pomiędzy zaistnieniem tego faktu a wyartykułowaniem odpowiadającego mu zdania. Zdaniom analitycznym, prawdziwym lub fałszywym na mocy znaczenia zawartych w nich wyrażeń, np. twierdzeniom logiki formalnej czy zaksjomatyzowanych teorii matematycznych nie odpowiadają żadne stany rzeczy w świecie rzeczywistym, pozajęzykowym. Odniesienie tych zdań do rzeczywistości uzyskuje się drogą interpretacji teorii do których należą.

</doc>
<doc id="1599" url="https://pl.wikipedia.org/wiki?curid=1599" title="Franciszkanie">
Franciszkanie



</doc>
<doc id="1600" url="https://pl.wikipedia.org/wiki?curid=1600" title="Forward raytracing">
Forward raytracing

Forward raytracing (proste śledzenie promieni) – metoda śledzenia promieni, która w przeciwieństwie do standardowej, czyli wstecznego śledzenia promieni (), rozpoczyna analizowanie promieni nie od kamery (obserwatora), lecz od źródeł światła. Określenie "forward" oznacza, że symulowane promienie poruszają się w tym samym kierunku i mają ten sam zwrot co w świecie rzeczywistym.
Daje ona lepsze wyniki niż raytracing wsteczny, lecz wymaga nieporównywalnie większej mocy obliczeniowej. Jest więc używana tylko do pewnych specjalnych zastosowań, mających niewiele wspólnego z fotorealistycznym renderingiem, takich jak badanie właściwości sprzętu optycznego oraz w rozwiązaniach hybrydowych w połączeniu ze standardowym raytracingiem.

</doc>
<doc id="1601" url="https://pl.wikipedia.org/wiki?curid=1601" title="Flawiusze">
Flawiusze

 Flawiusze – dynastia w cesarstwie rzymskim panująca w latach 69-96.
Do dynastii tej należeli:

</doc>
<doc id="1602" url="https://pl.wikipedia.org/wiki?curid=1602" title="Florian">
Florian

Florian – imię męskie pochodzenia łacińskiego. Wywodzi się od słowa oznaczającego „kwitnący kwiat”. W Polsce używane bywało w formie Tworzyjan. Od Floriana wywodzi się także czeski neologizm "Květoslav".
Żeński odpowiednik: Florianna, Floriana.
Florian imieniny obchodzi: 4 maja, 7 maja, 5 listopada i 17 grudnia.
Święty Florian jest patronem wykonawców zawodów wiążących się z ogniem: strażaków, hutników, kominiarzy, garncarzy, piekarzy. Jest również patronem Górnej Austrii obok św. Leopolda.
Wybrane osoby noszące imię Florian:

</doc>
<doc id="1603" url="https://pl.wikipedia.org/wiki?curid=1603" title="Fotorealizm (grafika komputerowa)">
Fotorealizm (grafika komputerowa)

Fotorealizm – właściwość obrazów i filmów stworzonych za pomocą technik komputerowych, które wyglądają jak obrazy pochodzące z rzeczywistości i jakby były uchwycone za pomocą kamery. Głównym sposobem uzyskiwania scen fotorealistycznych są techniki oparte na śledzeniu promieni.
Fotorealizm nie zawsze jest jednak pożądany – często istnieje potrzeba stworzenia stylizowanej sceny.

</doc>
<doc id="1604" url="https://pl.wikipedia.org/wiki?curid=1604" title="Filip I Arab">
Filip I Arab

Filip I Arab, (ur. 204, zm. 249) – cesarz rzymski od 244 do 249 roku, panował jako .
Pochodził z Trachonitis, krainy w południowej Syrii położonej na wschód od Jordanu. Miejscem jego urodzenia było Philippopolis w ówczesnej prowincji Arabia Petraea.
Życiorys.
Przejęcie władzy.
Za rządów cesarza Gordiana III był jednym z najważniejszych (razem ze swoim bratem Pryskusem) dowódców na rozkazach Tymezyteusza, prefekta pretorianów i w latach 241–243 najważniejszej osoby w państwie. Tymezyteusz zmarł w drugiej połowie 243 roku w trakcie wojny z Persją i Filip zastąpił go na stanowisku prefekta. W połowie lutego 244 roku rzymska armia została pokonana przez wojska Szapura I na perskim terytorium (niedaleko od Ktezyfonu), a cesarz Gordian zmarł lub zginął wkrótce potem. W trudnej sytuacji należało szybko wybrać nowego cesarza i wybór padł na Filipa. Jego szybka zgoda była traktowana później jako dowód jego udziału w śmierci Gordiana.
Układ z Persją.
Sytuacja Filipa była niebezpieczna. Stał na czele pokonanej armii znajdującej się na terytorium Persji i dodatkowo mógł obawiać się własnych żołnierzy, którzy mogli go zacząć obwiniać o możliwe przyszłe niepowodzenia. Jego najważniejszym celem stało się jak najszybsze wydostanie się z Persji i udanie się do Rzymu, gdzie mógłby ugruntować swoją pozycję jako nowy cesarz. Dlatego też zawarł układ z Szapurem, płacąc ogromny okup i godząc się na to, by Armenia znalazła się w perskiej strefie wpływów. Tereny Mezopotamii, odbite przez Rzymian w latach 243–244, miały pozostać w cesarstwie. Po zawarciu porozumienia wraz z armią wrócił na wschodnie tereny Cesarstwa rzymskiego.
Panowanie.
Filip pozostawił swojego brata, Pryskusa, w Antiochii jako zarządcę Wschodu, a sam udał się do Rzymu, gdzie przybył późnym latem 244 roku. W stolicy nadał swojemu siedmioletniemu synowi Filipowi II tytuł cezara. W 245 wyruszył nad Dunaj, gdzie Karpowie wspierani przez Gotów od końca 243 przekraczali rzekę od strony Dacji i – wykorzystując rzymskie drogi do szybkiego przemieszczania się – spustoszyli bałkańskie prowincje. Filip wypchnął barbarzyńców na północ od Dunaju i wkroczył za nimi do Dacji; latem 246 roku kampania zakończyła się zwycięstwem. Mimo to sytuacja na tym odcinku rzymskiego limesu (Dacja i dolny Dunaj) pozostawała trudna i wymagała sprowadzania dodatkowych garnizonów, co osłabiało inne odcinki granicy.
W 247 Filip przebywał w Rzymie, gdzie zorganizował wielkie obchody – tzw. święta stulecia ("ludi saeculares") z okazji 1000-lecia miasta Rzym (21 kwietnia 247 mijało millenium od tradycyjnej daty założenia miasta), połączone z uczczeniem jego zwycięstw oraz nadaniem synowi tytułu augusta. W 248 nad środkowym Dunajem wybuchła rebelia Pakacjana; została rychło stłumiona, ale destabilizacja w regionie zachęciła Kwadów i Jazygów do splądrowania Panonii. Z kolei Mezję Dolną atakowali Goci, wcześniej opłacani przez Rzym, a teraz pozbawieni tych funduszy decyzją Filipa. Sytuacja zachęciła także Karpów do wznowienia ataków na Mezję i Dację. Filip wysłał nad Dunaj Decjusza z zadaniem opanowania sytuacji. Decjusz działał skutecznie, zyskując uznanie armii, która, rzekomo wbrew jego woli, obwołała go cesarzem w maju lub czerwcu 249.
Niezależnie od tego wydarzenia pozycja Filipa słabła. Po poprzednikach odziedziczył kłopoty finansowe pogłębione jeszcze przez jego wydatki (m.in. wielką rozbudowę rodzinnego Philippopolis w Syrii). Próbował poprawić sytuację psując monetę, ale wstrzymywał się od podnoszenia podatków w Italii i Afryce, pamiętając o kłopotach, jakie przyniosło to cesarzowi Maksyminowi. Jego brat, Pryskus, próbował podnieść podatki na podległym sobie Wschodzie, lecz wywołał tym posunięciem rewoltę Jotapiana w Kapadocji i Uraniusza Antoninusa w Syrii. Być może odpowiedzią na podniesienie podatków były też rozruchy religijne, jakie wtedy wybuchły w Aleksandrii, co ograniczyło dostawy zboża z Egiptu do Rzymu i dodatkowo pogłębiło problemy Filipa.
Upadek i śmierć.
Obwołany cesarzem Decjusz ruszył z armią do Italii. Filip wyruszył na jego spotkanie, pozostawiając syna w stolicy. W sierpniu lub wrześniu 249 doszło do bitwy pod Weroną, gdzie Filip został pokonany i zabity. Na wieść o jego upadku w Rzymie zamordowano Filipa II.

</doc>
<doc id="1605" url="https://pl.wikipedia.org/wiki?curid=1605" title="Flavius Valerius Severus">
Flavius Valerius Severus



</doc>
<doc id="1606" url="https://pl.wikipedia.org/wiki?curid=1606" title="Folusz (maszyna)">
Folusz (maszyna)

Folusz – maszyna stosowana w procesie folowania sukna.
Tkaniny przeznaczone na sukno tkano z grubej, zgrzebnej przędzy wełnianej, a następnie poddawano folowaniu (spilśnianiu). Pierwotnie zabieg ten był wykonywany za pomocą rąk lub nóg. Malowidła ścienne, odnalezione w Pompejach, pokazują jak deptano wówczas sukno w specjalnych korytach. Z czasem zaczęto używać grubych kijów (bijaków). Od drugiej połowy XII w. stosowano do tego procesu drewniane młoty, poruszane kołem wodnym. W takim foluszu stępory (drewniane młoty) uderzały w wielokrotnie poskładane sukno, leżące w stępie (wydrążonej kłodzie). Według zachowanych dokumentów w 1339 r. był we Frankfurcie folusznik, który napędzał swoją pilśniarkę siłą wody. 

</doc>
<doc id="1608" url="https://pl.wikipedia.org/wiki?curid=1608" title="Forum Romanum">
Forum Romanum

Forum Romanum (pol. – rynek rzymski), inna nazwa to Forum Magnum – najstarszy plac miejski w Rzymie, otoczony sześcioma z siedmiu wzgórz: Kapitolem, Palatynem, Celiusem, Eskwilinem, Wiminałem i Kwirynałem. Główny polityczny, religijny i towarzyski ośrodek starożytnego Rzymu, miejsce odbywania się najważniejszych uroczystości publicznych.
Historia.
Powstanie.
Tzw. teoria Gjerstada wypracowana w 1968 zakładała, że pierwotnie (w VII w. p.n.e.) na tym terenie znajdowała się osada składająca się z domów na palach. Po wybudowaniu kanału odwadniającego Cloaca Maxima, osuszeniu terenu, utwardzeniu powierzchni, obszar ten stał się miejscem zgromadzeń obywateli. Inicjatorem wybudowania forum miał być Tarkwiniusz Pyszny. Przy powstałym w VI w. p.n.e. placu miano potem wznieść szereg ważnych budynków: Kurię, mównicę (Rostra), świątynię Westy i dom westalek, świątynie Saturna, Dioskurów, Zgody i siedzibę najwyższego kapłana – budynek Regii, archiwum (Tabularium).
Ostatnie badania archeologiczne terenu forum przeprowadzone w latach osiemdziesiątych i dziewięćdziesiątych XX wieku zaprzeczyły teorii Gjerstada. Wykazano, że do trzeciej ćwierci VIII w. p.n.e. na terenie Forum Romanum znajdowało się zwykłe bagno zasilane potokiem płynącym wzdłuż późniejszej Via Sacra. Stanowiło ono naturalną granicę pomiędzy znajdującymi się na wzgórzach osadami, tzw. palatyńsko-eskwilińską i kapitolińsko-kwirynalską. Bagno osuszono, a powierzchnię utwardzono w latach dwudziestych VIII wieku p.n.e. (729-720 p.n.e.), co zgadza się w zasadzie z przekazanym przez rzymskich historyków czasem zjednoczenia Rzymu Romulusa z okoliczną osadą sabińską Tytusa Tacjusza. Przyjmuje się, że w tym właśnie okresie nastąpiło połączenie osady palatyńskiej z kwirynalską w jeden organizm polityczny, którego centrum stanowiło właśnie nowo osuszone Forum Romanum. Nigdy natomiast nie było tam domów na palach, jak twierdził Gjerstad.
Okres świetności.
W okresie cesarstwa Forum Romanum zostało rozbudowane i wzbogacone o Świątynię Boskiego Juliusza (Templum Divii Iulii), Wenus i Romy, Wespazjana, Antonina i Faustyny oraz łuk triumfalny cesarza Augusta. W tym też czasie rozbudowano Rzym i zbudowano kolejne fora cesarskie, które utworzyły z istniejącym już Forum Romanum zwarty kompleks architektoniczny. Najbliżej Forum Romanum usytuowano Forum Cezara. Oprócz niego powstały Forum Trajana (największe), Forum Augusta, Forum Wespazjana, Forum Nerwy. Inicjatorem tej rozbudowy był Juliusz Cezar. Forum Romanum było centrum życia obywatelskiego i ekonomicznego szczególnie w okresie republikańskim. W okresie cesarstwa fora pełniły raczej funkcje handlowe, rozrywkowe i reprezentacyjne.
Upadek.
Choć Forum zostało zdewastowane w czasie zdobycia Rzymu przez Wizygotów w 410, to jeszcze w 768 zebrał tu się lud rzymski i obwołał papieżem Stefana III(IV). Do zniszczenia mogło też przyczynić się trzęsienie ziemi w 851 roku. W okresie średniowiecza Forum Romanum uległo stopniowemu zniszczeniu. Prawdopodobnie w okresie IX-X wieku Forum, jak i inne pozostałości rzymskie stanowiło kamieniołom - miejsce łatwego pozyskiwania budulca dla nowych inicjatyw architektonicznych. Do XVIII wieku miejsce to służyło na wypas, a później targ bydła. W tym czasie nazywano je Campo Vaccino. 
Współczesność.
Prace wykopaliskowe zapoczątkowano w 1803, a od 1898 prace są prowadzone systematycznie.
Restauracja zabytków została przerwana przez I wojnę światową. W 1980 zlikwidowano ruchliwą ulicę via della Consolazione generującą ruch samochodowy i zanieczyszczenie środowiska między zboczami Kapitolu a Świątynią Saturna aby kontynuować prace wykopaliskowe i konserwację zabytków.

</doc>
<doc id="1609" url="https://pl.wikipedia.org/wiki?curid=1609" title="Figura">
Figura



</doc>
<doc id="1611" url="https://pl.wikipedia.org/wiki?curid=1611" title="Faustyna Młodsza">
Faustyna Młodsza

Faustyna Młodsza, "Faustina, Annia Galeria Faustina" (ok. 122 – 175) – najmłodsze dziecko (druga córka) Antonina Piusa i Faustyny Starszej. Była żoną cesarza Marka Aureliusza, matką co najmniej 14 dzieci; w tym cesarza Kommodusa. Otrzymała tytuł augusty.
Potomstwo z Markiem Aureliuszem.
Dzieci z Markiem Aureliuszem

</doc>
<doc id="1613" url="https://pl.wikipedia.org/wiki?curid=1613" title="Free Pascal">
Free Pascal

Free Pascal (również: FPK Pascal, FPC) jest 32- oraz 64-bitowym kompilatorem języka Pascal, dostępnym na wiele różnych platform sprzętowych i systemów operacyjnych.
Wprowadzenie.
Kompilator Free Pascal jest rozpowszechniany zgodnie z licencją GPL. Biblioteki wykonawcze oraz dodatkowe pakiety rozpowszechniane razem z kompilatorem objęte są jednak zmodyfikowaną licencją LGPL. Modyfikacja owa polega na tym, że autorzy jasno wyrazili zgodę na dołączanie (linkowanie) tych bibliotek do innych programów bez względu na licencję tworzonego programu i sposób łączenia (statyczne lub dynamiczne).
Początkowo projekt nosił nazwę FPK Pascal. FPK stanowiło skrót od niemieckojęzycznej nazwy "Frei Pascal Kompiler", a także oznaczało inicjały twórcy projektu (Florian P. Klaempfl). Podczas wydania wersji 1.0 skrót, jak i nazwę, zmieniono na odpowiednio FPC i Free Pascal Compiler.
Wizualną częścią bibliotek dla FPC, zgodną z VCL znanym z Delphi, a także stworzeniem narzędzia typu RAD, zajmuje się osobny projekt – Lazarus.
FPC posiada własne IDE stworzone w trybie tekstowym, jednak wiele błędów nie zostało naprawionych od wielu lat, mimo stosunkowej popularności środowiska.
FPC rozpoznaje i kompiluje kod Pascala zapisany w poniższych dialektach tego języka:
Możliwości.
Składnia języka jest semantycznie zgodna z TP 7.0, tak samo jak z większością wersji Delphi (klasy, RTTI, wyjątki, ansistrings). Free Pascal udostępnia pełny zakres programowania obiektowego. Możliwe jest przeciążanie funkcji i przeciążanie operatorów oraz zagnieżdżanie komentarzy. Free Pascal obsługuje arytmetykę 64-bitową, SSE. Wbudowana jest optymalizacja kodu maszynowego dla procesorów Pentium, AMD, AMD64. FPC wspiera klasy generyczne (Dodane w serii 2.2). Jednak w związku z problemami w zastosowaniu operatorów w rekordach, które były używane przez klasy generyczne, od wersji 2.6.0 dodano wsparcie dla zaawansowanych rekordów, co umożliwiło ich szersze zastosowanie.
Historia.
Free Pascal powstał w czasie, kiedy stało się jasne, że firma Borland nie wyda kolejnej – ósmej wersji dosowego kompilatora Turbo Pascal. Zamiast niego powstał kompilator Delphi dostępny tylko dla użytkowników Windows. Student Florian Klaempfl zaczął pisać swój własny 16-bitowy kompilator dla DOS-a. Po dwóch latach przepisał go na architekturę 32-bitową przy użyciu rozszerzenia go32v1.
Kompilator został opublikowany w internecie, co spowodowało przyłączenie się do projektu innych programistów. Na pięć lat przed powstaniem Kyliksa Michael van Canneyt stworzył port FPC dla Linuksa. W międzyczasie powstał port dla OS/2, a pierwotna wersja dla DOS-a została znacznie ulepszona. Programiści zdecydowali się wydać wersję 0.99.5, która doczekała się także portu dla procesorów 680x0.
W kolejnej wersji 0.99.8 został dodany port dla platformy Win32 i rozpoczęły się prace nad dodaniem kompatybilności z Delphi. 12 lipca 2000 ukazała się pierwsza stabilna wersja 1.0. Kolejne wydania z serii 1.0.x zawierały głównie poprawki błędów oraz wciąż rozwijany port dla procesorów 680x0. Kłopoty w implementacji tego portu skłoniły zespół programistów FPC do rozpoczęcia prac nad nową serią 1.1.x (przekształconą później w 1.9.x). Miała ona zawierać przepisany od nowa generator kodu ułatwiający portowanie kompilatora do zupełnie nowych architektur.
Na początku 2003 roku powstał port dla PowerPC, a niedługo potem dla AMD64. Rozpoczęto też prace nad portami ARM i SPARC. Wersja FPC dla AMD64 uczyniła go kompilatorem zarówno 32-, jak i 64-bitowym.
15 maja 2005 miała miejsce długo wyczekiwana premiera wersji 2.0. Przyniosła ona zmianę struktury kompilatora na bardziej modularną, by ułatwić jego przenoszenie na nowe platformy. Poprawiono obsługę wątków i optymalizację kodu. Kompilator przeniesiono na systemy Mac OS 9 i X, MorphOS oraz Novell Netware.
Kwiecień 2006 przyniósł pierwszy snapshot kompilatora dla platformy Win64 Vista.
W Sierpniu 2007 wydana została seria 2.2.X która przyniosła wewnętrzny linker dla platform Win32, Win64 oraz WinCE. Pozwolił on na znaczne zredukowanie czasu generowania plików wykonywalnych w porównaniu do linkera GCC. Poza tym w serii tej ulepszono usuwanie martwego kodu oraz smartlinking.
Platformy docelowe.
Wersja "2.6.0" jest dostępna na następujących platformach:

</doc>
<doc id="1614" url="https://pl.wikipedia.org/wiki?curid=1614" title="Festiwal">
Festiwal

Festiwal (łac. "festivus" – radosny, wesoły, świąteczny) – szereg imprez, przeważnie artystycznych jednego typu (np. filmowych, muzycznych, teatralnych), będących przeglądem osiągnięć w danej dziedzinie, zorganizowanych w jednym czasie i pod wspólną nazwą, często ujętych w ramy konkursu. Formę festiwalu mają liczne imprezy dotyczące nauki, technologii i różnych aktualnych problemów interdyscyplinarnych. 
Festiwale kabaretowe w Polsce.
W Polsce odbywa się kilka festiwali kabaretowych, m.in.:
zobacz też: 
Festiwale naukowe w Polsce.
Popularyzacji nauki służą liczne (w tym Festiwale Nauki), organizowane regularnie w różnych miastach Polski, m.in. w:

</doc>
<doc id="1616" url="https://pl.wikipedia.org/wiki?curid=1616" title="Flavius Theodosius">
Flavius Theodosius



</doc>
<doc id="1617" url="https://pl.wikipedia.org/wiki?curid=1617" title="Filip Arab">
Filip Arab



</doc>
<doc id="1618" url="https://pl.wikipedia.org/wiki?curid=1618" title="Femto">
Femto

Femto (f) (duńskie "femten" – piętnaście) – przedrostek jednostki miary oznaczający mnożnik 0,000 000 000 000 001 = 10-15 (jedna biliardowa).

</doc>
<doc id="1619" url="https://pl.wikipedia.org/wiki?curid=1619" title="F">
F

F (minuskuła: f) – szósta litera alfabetu łacińskiego, dziewiąta litera alfabetu polskiego. Oznacza zwykle w danym języku spółgłoskę szczelinową wargowo-zębową bezdźwięczną, np. [].

</doc>
<doc id="1621" url="https://pl.wikipedia.org/wiki?curid=1621" title="Fe">
Fe



</doc>
<doc id="1622" url="https://pl.wikipedia.org/wiki?curid=1622" title="Flawiusz Wiktor">
Flawiusz Wiktor

Flawiusz Wiktor (ur. ?, zm. sierpień 388) – cesarz rzymski od 384 lub 387 do 388 roku n.e. Syn Magnusa Maksymusa i jego pierwszej żony.

</doc>
<doc id="1623" url="https://pl.wikipedia.org/wiki?curid=1623" title="Flaga państwowa">
Flaga państwowa

Flaga państwowa, inaczej flaga narodowa – jeden z symboli państwowych, oficjalny znak symbolizujący suwerenność państwa. Flaga jest płatem tkaniny, najczęściej prostokątnym (inne kształty spotyka się tylko wyjątkowo: kwadratowy – Watykan, Szwajcaria czy trójspiczasty – Nepal). Jest wywieszana na urzędach państwowych oraz w trakcie świąt i uroczystości państwowych, bądź spotkań międzynarodowych. Flaga państwowa podlega ochronie prawnej i jej zniszczenie lub obelżywe traktowanie traktowane jest jako obraza narodu. W niektórych państwach obchodzone jest święto flagi państwowej, np. w Szwecji czy USA lub w Polsce (2 maja). Wygląd flagi państwowej jest w ogromnej większości państw określony przez ustawę zasadniczą i ustawy szczegółowe.
Terminologia, zakres znaczeniowy, warianty.
Sposób stosowania terminu flaga narodowa lub flaga państwowa jest różny w różnych państwach świata i wynika z uwarunkowań historycznych, kulturowych i prawnych. W krajach Europy Zachodniej, zwłaszcza anglosaskich i francuskojęzycznych, na ogół utożsamia się naród (ang. "Nation", fr. "la Nation") z ogółem obywateli państwa. Dlatego używa się tam raczej pojęcia flaga narodowa. Używanie terminu flaga państwowa jest bardziej oczywiste w państwach wielonarodowych i wielonarodowościowych, oraz w tych, gdzie naród i państwo postrzega się jako dwie zasadniczo odmienne jakości.
Niejednoznaczność wynika też z tego, że w niektórych państwach istnieje odrębna flaga przeznaczona dla instytucji państwowych nazywana właśnie flagą państwową (na język polski termin ten bywa tłumaczony jako flaga urzędowa), oraz odrębna flaga przeznaczona dla prywatnych obywateli, zwana na ogół flagą cywilną. Obie zaś są określane łącznie jako flaga narodowa. Na ogół flaga urzędowa posiada herb państwowy, a flaga cywilna go nie posiada. Przykładem takiego rozwiązania są Niemcy. Sytuacja w Polsce, gdzie istnieje tzw. flaga z godłem i flaga bez godła, nie jest identyczna z powyższym podziałem, gdyż oficjalny zakres użycia flagi z godłem jest bardzo ograniczony.
Trzecim wariantem flagi narodowej może być flaga wojenna – jest to odrębny wariant flagi państwowej lub narodowej przeznaczony dla sił zbrojnych. Występuje on lub występował tylko w nielicznych państwach. W Polsce zasadniczo flaga wojenna nie występuje, choć w pewnym zakresie jej rolę spełnia "flaga lotnisk i lądowisk wojskowych".
Specyficzną kategorią flag narodowych i państwowych są bandery. Pojęciem tym określa się w języku polskim flagi podnoszone na jednostkach pływających (statkach i okrętach). Rozróżnienie między flagą lądową a banderą występuje jednak tylko w niektórych językach, np. w języku angielskim rozróżnia się flagę ("a flag") od bandery ("an ensign"). Natomiast np. w niemieckim istnieje tylko jeden wyraz "die Flagge". W niektórych krajach bandery różnią się od flag narodowych używanych na lądzie (przykładem może być flaga Wielkiej Brytanii). W innych natomiast jest tylko jeden wariant używany zarówno na lądzie jak i na morzu, np. we Francji i Stanach Zjednoczonych.
W przypadku bander na ogół mamy do czynienia z bardziej klarownym niż na lądzie podziałem na różniące się wizualnie banderę wojenną podnoszoną na okrętach wojennych, banderę handlową oraz niekiedy występującą banderę urzędową podnoszoną na jednostkach pływających w państwowej służbie cywilnej. Wariantem bandery urzędowej bywa niekiedy odrębna bandera statków straży granicznej.
Specyficznym rodzajem bandery, a zarazem też flagi narodowej, jest proporzec.
Pojęciem "flaga narodowa" określa się też często potocznie flagi narodów, narodowości i grup etnicznych nie posiadających własnego państwa, np. flagę Bretończyków, Kurdów itp. Niektóre z nich mają oficjalny status flag jednostek administracyjnych, np. "Senyera", flaga postrzegana jako flaga narodowa Katalończyków, jest jednocześnie oficjalną flagą regionu autonomicznego Katalonii. Niektóre zostały proklamowane flagami narodowymi przez organizacje, kongresy i komitety polityczne pretendujące do reprezentowania tych narodów, jeszcze inne mają status całkiem nieoficjalny. W weksylologii są one na ogół określane mianem flag mniejszości narodowych.
Historia flagi narodowej.
Niektóre obecnie używane flagi narodowe i państwowe mają kilkusetletnią tradycję, np. flaga Danii używana jest od średniowiecza. Jednak w zasadzie w odniesieniu do okresu przed XVIII wiekiem nie można mówić o flagach narodowych i państwowych w pełnym tego słowa znaczeniu. Flagi, o których mowa, były jedynie banderami wojennymi i handlowymi, kojarzonymi raczej z monarchą i jego siłami zbrojnymi, nie zaś z państwem, a tym bardziej z ogółem jego mieszkańców. Niektóre statki handlowe używały zresztą odrębnych bander miejskich i bander kompanii handlowych. Na lądzie natomiast używano chorągwi i sztandarów, które były raczej wytworami rzemiosła artystycznego a nie masowej produkcji, przedstawiały głównie godła heraldyczne i nie były ustandaryzowane, a jedynie używane zwyczajowo.
Nowoczesna flaga państwowa i narodowa – a więc ustandaryzowana, postrzegana jako symbol państwa i narodu, produkowana fabrycznie lub na tyle prosta by mogła być wykonana samodzielnie w domu przez zwykłego obywatela, zaczęła występować w końcu XVIII w. i była wynikiem demokratyzacji polityki, związanej między innymi z amerykańską wojną o niepodległość i rewolucją francuską. W XIX w. europejska kultura używania flagi narodowej przyjęła się w tych krajach pozaeuropejskich, które były uznawane za podmioty prawa międzynarodowego i członków tzw. rodziny narodów, np. Chiny, Japonię, Syjam.
Do pojawienia się kolejnych flag narodowych przyczyniały się każdorazowo kolejne fale pojawiania się nowych państw na mapie politycznej świata: dekolonizacja Ameryki Łacińskiej w pierwszej połowie XIX w., upadek Imperium Osmańskiego, Austro-Węgier i carskiej Rosji po I wojnie światowej, dekolonizacja Azji i Afryki po II wojnie światowej, wreszcie rozpad ZSRR i Jugosławii.
Rodziny flag.
Flagi państw i narodów bliskich kulturowo, politycznie i geograficznie są często podobne do siebie nawzajem, i tak np.:

</doc>
<doc id="1624" url="https://pl.wikipedia.org/wiki?curid=1624" title="Flaga Arabii Saudyjskiej">
Flaga Arabii Saudyjskiej

Flaga Arabii Saudyjskiej jest zielonym prostokątem (zieleń – symbol islamu) z białym napisem w języku arabskim „لا إله إلا الله محمد رسول الله” (Nie ma Boga prócz Allaha, a Mahomet jest jego prorokiem) – jest to "szahada", czyli wyznanie wiary – jeden z pięciu filarów wiary muzułmańskiej. Pod napisem jest wizerunek miecza.
Konstrukcja i proporcje.
Zielony prostokąt o proporcjach 2:3 z białym napisem ("szahadą") i białym mieczem pod napisem. "Szahada" po obu stronach płata flagi jest zapisana zgodnie z regułami pisma arabskiego (od prawej ku lewej), natomiast rękojeść miecza zwrócona jest z obu stron flagi ku drzewcowi, w wyniku czego wygląd przedniej i tylnej strony flagi jest inny.
Historia.
Flagę przyjęto 15 marca 1973. Flaga jest ta bardzo podobna do używanej od 1901 flagi Wahhabitów – zwolenników odnowy religijnej, którzy w 1932 utworzyli Arabię Saudyjską.

</doc>
<doc id="1625" url="https://pl.wikipedia.org/wiki?curid=1625" title="Flaga Libii">
Flaga Libii

Flaga Libii − prostokątna flaga z białym półksiężycem i gwiazdą na czarnym poziomym pasie, znajdującym się pomiędzy o połowę węższymi pasami: czerwonym i zielonym. Została ona określona jako flaga państwowa przez Narodową Radę Tymczasową w deklaracji konstytucyjnej z 3 sierpnia 2011 roku; jest to flaga używana przez rebeliantów w 2011 roku i wcześniejsza flaga Królestwa Libii. Kolory flagi symbolizują historyczne krainy wchodzące w skład dzisiejszej Libii: Fazzan (czerwony), Cyrenajkę (czarny pas z półksiężycem) i Trypolitanię (zielony); ponadto są to barwy panarabskie.
Historia.
1951–1969
Flaga Królestwa Libii została wprowadzona po uzyskaniu niepodległości w 1951 roku. Flaga ta bazowała na fladze Senussich. Za rządów Kaddafiego flaga ta była używana przez monarchistów libijskich oraz szeroko rozumianą opozycję.
1969–1972
Po rewolucji libijskiej władzę w kraju przejął Muammar al-Kaddafi, który ogłosił powstanie Libijskiej Republiki Arabskiej. Wprowadzono także nową flagę, o barwach stosowanych przez inne państwa arabskie począwszy od rewolucji egipskiej z 1952 roku.
1972–1977
W 1972 roku Libia wraz z Egiptem i Syrią utworzyły Federację Republik Arabskich. Państwa członkowskie przyjęły wspólną flagę, także czarno-biało-czerwoną, z centralnie umieszczonym wizerunkiem jastrzębia – herbem Kurajszytów.
1977–2011
11 listopada 1977 roku oficjalna nazwa państwa została zmieniona na "Wielka Arabska Libijska Dżamahirijja Ludowo-Socjalistyczna". Ustanowiono także nową flagę: prostokątną, o proporcjach 1:2, w całości zieloną. Była to jedyna na świecie flaga państwowa o jednolitej barwie. Kolor zielony symbolizował przede wszystkim islam oraz "zieloną rewolucję" w rolnictwie. Jeden kolor to także wyraz równości wszystkich obywateli.
od 2011
Flaga używana wcześniej, w latach 1951–1969. Wprowadzona po obaleniu Kaddafiego w wyniku wojny w Libii w 2011.

</doc>
<doc id="1626" url="https://pl.wikipedia.org/wiki?curid=1626" title="Flaga Polski">
Flaga Polski

Flaga Polski (Flaga Państwowa Rzeczypospolitej Polskiej) – jeden z symboli państwowych Rzeczypospolitej Polskiej.
Według ustawy z dnia 31 stycznia 1980 r. o godle, barwach i hymnie Rzeczypospolitej Polskiej oraz o pieczęciach państwowych, jest nią prostokątny płat tkaniny o barwach Rzeczypospolitej Polskiej i proporcji 5:8, umieszczony na maszcie. Zgodnie z art. 6 ust. 2 ustawy, za flagę Polski uważany jest także wariant z godłem Polski, umieszczonym pośrodku białego pasa.
Barwy Rzeczypospolitej Polskiej stanowią składniki flagi państwowej Rzeczypospolitej Polskiej. Ustawa stanowi, że barwami Rzeczypospolitej Polskiej są kolory biały i czerwony, ułożone w dwóch poziomych, równoległych pasach tej samej szerokości, z których górny jest koloru białego, a dolny koloru czerwonego.
Od 2004 roku 2 maja jest w Polsce oficjalnie obchodzony jako Dzień Flagi Rzeczypospolitej Polskiej.
Znaczenie heraldyczne barw.
Barwy flagi złożone z dwóch poziomych pasów: białego oraz czerwonego są odwzorowaniem kolorystyki godła państwowego, który stanowi orzeł biały na czerwonym polu. Zgodnie z zasadami heraldyki pas górny reprezentuje białego orła, a dolny czerwone pole tarczy herbowej. Kolory te według symboliki używanej w heraldyce mają następujące znaczenie:
Historia.
Pierwotnie polską barwą narodową był karmazyn, stanowiący symbol dostojeństwa i bogactwa, a zarazem uważany za najszlachetniejszy z kolorów. Z uwagi na cenę barwnika – koszenili uzyskiwanej z larw czerwca polskiego, mało kto mógł sobie na niego pozwolić, dlatego też był on wykorzystywany jedynie przez najbogatszą szlachtę i dostojników państwowych.
Na pierwszych flagach i sztandarach reprezentujących Królestwo Polskie widniał biały orzeł w koronie na czerwonym tle. Jan Długosz opisując przygotowania do bitwy pod Grunwaldem pisze o „chorągwi wielkiej, na której wyszyty był misternie orzeł biały z rozciągnionemi skrzydły, dziobem rozwartym i z koroną na głowie, jako herb i godło całego Królestwa Polskiego”.
Barwami królewskimi Rzeczypospolitej Obojga Narodów był sztandar złożony z trzech pasów: dwóch czerwonych umieszczonych w dole i na górze oraz oddzielającym je pasie białym, na których umieszczano zwykle czterodzielny Herb Rzeczypospolitej Obojga Narodów o czerwonym tle zawierający dwa pola przedstawiające białego Orła w koronie, czyli symbol Korony i dwa z wizerunkiem Pogoni – herbu Litwy. Na tarczy sercowej znajdował się najczęściej herb rodowy aktualnie panującego monarchy.
W okresie przedrozbiorowym silna była także tradycja uznająca za barwy narodowe trzy kolory: biały, karmazynowy i granatowy. Tradycja barw karmazynowej i granatowej wywodzi się od strojów żołnierzy kawalerii, w których to formacjach służyli w większości szlachcice (tzw. wojska narodowego autoramentu). Chociaż już w XVI wieku próbowano unifikować stroje poszczególnych oddziałów, jezdni w Polsce aż do XVIII wieku nie nosili mundurów. W początkach XVIII wieku jednolite stroje pojawiły się w pierwszych oddziałach prywatnych, a w połowie wieku w oddziałach królewskich. W roku 1746 hetman wielki litewski Michał Kazimierz Radziwiłł wprowadził regulaminowy ubiór dla wszystkich husarzy i pancernych litewskich. Husaria nosić miała kontusze koloru karmazynowego i żupany granatowego, a pancerni kontusze koloru granatowego i żupany koloru karmazynowego. Wkrótce potem żołnierze koronni zaczęli z własnej inicjatywy nosić mundury, które dodawały im prestiżu wśród obywateli i ułatwiały rozpoznanie w tłumie. Usankcjonowaniem zwyczaju noszenia munduru w wojsku koronnym (husarii) był uniwersał hetmana Jana Klemensa Branickiego z 1763 roku. Pochodzenie bieli jako jednego z kolorów narodowych pochodzi zaś od kopii i proporców jazdy, które zdobiono chorągiewkami czerwono-białymi lub karmazynowo-granatowymi. W roku 1775 powstała Kawaleria Narodowa, której mundury określono na granatowo-karmazynowo-białe. Po upadku państwa tradycja kolorystyczna nie zanikła. W 1807 roku sformowano pierwszy pułk lekkokonny dla gwardii Napoleona Bonaparte i utrzymano w nim mundury w barwach narodowych. Trójkolorowe kokardy pojawiły się także w pierwszych dniach powstania listopadowego.
Barwy biała i czerwona zostały uznane za narodowe po raz pierwszy 3 maja 1792. Podczas obchodów pierwszej rocznicy uchwalenia "Ustawy Rządowej" damy wystąpiły wówczas w białych sukniach przepasanych czerwoną wstęgą, a panowie nałożyli na siebie szarfy biało-czerwone. Nawiązano tą manifestacją do heraldyki Królestwa Polskiego – białego orła na czerwonej tarczy herbowej.
Po raz pierwszy polskie barwy zostały uregulowane w uchwale Sejmu Królestwa Polskiego z 7 lutego 1831 na wniosek posła Walentego Zwierkowskiego, wiceprezesa Towarzystwa Patriotycznego, jako propozycja kompromisowa pomiędzy barwą białą – nadaną przez Augusta II Mocnego i proponowaną przez konserwatystów i trójbarwną – biało-czerwono-szafirową – barwami konfederacji barskiej proponowanymi przez Towarzystwo Patriotyczne:
Po odzyskaniu niepodległości barwy i kształt flagi uchwalił Sejm Ustawodawczy odrodzonej Polski 1 sierpnia 1919. W ustawie podano: „Za barwy Rzeczypospolitej Polskiej uznaje się kolor biały i czerwony w podłużnych pasach równoległych, z których górny – biały, dolny zaś – czerwony.” Dwa lata później Ministerstwo Spraw Wojskowych wydało broszurę, w której sprecyzowano odcień czerwieni definiując ją jako karmazyn, ale w 1927 odcień koloru czerwonego został zmieniony na cynober, ten sam kolor został użyty w definicji flagi w ustawie z 1955.
Kolory flagi zostały ponownie zmienione ustawą z 31 stycznia 1980, ustawę opracował zespół ekspertów w skład którego wchodzili Szymon Kobyliński, Maria Szypowska, Kazimierz Sikorski oraz Nikodem Sobczak.
Konstrukcja, wymiary i barwy.
Obowiązująca flaga Rzeczypospolitej Polskiej jest prostokątem o proporcjach 5:8 podzielonym na dwa poziome pasy: biały (u góry) i czerwony. Dopuszcza się także wieszanie flagi w formie pionowej wstęgi: wówczas barwa biała powinna znajdować się po lewej stronie (widok z przodu).
Odwzorowanie graficzne barw ustawowych.
Zgodnie z załącznikiem do ustawy kolor biały na fladze polskiej przypomina bardziej kolor „srebrnobiały” („szary”) niż „biały” („śnieżnobiały”), który dla niektórych osób wydaje się bardziej „naturalny” czy „intuicyjny”.
Według raportu „Informacja o wynikach kontroli używania symboli państwowych przez organy administracji publicznej” Najwyższej Izby Kontroli dostępne w sprzedaży, a także używane oficjalnie symbole narodowe (godło i flaga), w wielu przypadkach rażąco różnią się od wzorców opisanych w Konstytucji i ustawie.
Różnice i kontrowersje związane z jasnością bieli wynikają z tego, iż ustawodawca określając w załączniku do ustawy barwy flagi brał pod uwagę możliwość odwzorowania bieli na materiale, z którego flagi są szyte (jasność materiału). Monitory CRT i LCD potrafią wyświetlić znacznie jaśniejszą biel, więc po przeliczeniu barw flagi z xyY na sRGB objawiło się to szarawym odcieniem bieli. Ponieważ nie wiadomo, czy ustawodawca podając parametry barw narodowych podawał je wyłącznie dla tkanin, czy też dla wszystkich możliwych materiałów, w tym monitorów, nie można stwierdzić, czy barwy wyliczone z ustawy są poprawnym odwzorowaniem barw Rzeczypospolitej Polskiej na ekranie, czy też nie.
Komisja Heraldyczna do tej pory nie ustaliła, w jaki sposób zdefiniować barwy flagi mające być prezentowane w Internecie. W 2005 zespół ekspertów wstępnie określił barwy flagi w poligraficznym systemie CMYK, jednak po wyborach prezydenckich prace zespołu nie zostały wznowione. Projekt nowej ustawy, sformułowany w sposób odpowiedni dla ery cyfrowej, Ministerstwo Kultury, Dziedzictwa Narodowego i Sportu przedstawiło na początku października 2021.
Odcień bieli.
Biel wchodząca w skład barw Polski do 1980 była określana słownie, bez podawania jej parametrów. Stan ten zmieniła ustawa z 1980: biel ustalona w załączniku do ustawy z 31 stycznia 1980 nie jest idealnie biała. Jest to skutkiem faktu, że ustalono dla bieli parametr jasności Y=82; idealna biel powinna mieć ten parametr równy około 100. Po przeliczeniu ustawowej bieli na jedną z przestrzeni barw RGB, sRGB, otrzymuje się szesnastkowo:   #E9E8E7  .
Oceniając biel flagi na monitorze, należy zwrócić uwagę na jego ustawienia: załączony tu rysunek najpoprawniej oddaje intencje ustawodawcy przy ustawieniu punktu bieli na temperaturę barwową 6500 K. Odcień szarości może też zależeć od kąta patrzenia, zwłaszcza na monitorach typu LCD. Ponadto należy pamiętać, że względna szarość ustawowej bieli jest pogłębiona faktem jej wyświetlania tutaj na idealnie białym tle (współczesne monitory potrafią wyświetlić biel jaśniejszą niż kartka). Ten sam kolor na ciemnym tle naturalnie uznaje się za biały.
Odcień czerwieni.
Z ustawy z 1919 nie wynikało, jaki ma być odcień czerwieni. Dopiero dwa lata później ukazała się broszura „Godła i barwy Rzeczypospolitej Polskiej” z barwnymi wizerunkami znaków państwowych, wydana przez Ministerstwo Spraw Wojskowych, a opracowana przez Stanisława Łozę. Czerwień na fladze narodowej miała odcień karmazynu. Jednakże w rozporządzeniu Prezydenta Rzeczypospolitej z 13 grudnia 1927 odcień czerwieni zmieniono na cynober. Nowe rozporządzenie weszło w życie 28 marca 1928; zezwolono w nim jednak na używanie dotychczasowych flag do 28 marca 1930 r. (ów „czas przejściowy” wydłużono następnie do 31 grudnia 1936 roku). Cynober – potwierdzony w ustawie z 1955 sankcjonującej godło bez korony – pozostał na polskich flagach do 1980. Obydwa odcienie czerwieni, karmazyn i cynober, nie były dokładnie zdefiniowane. Zarówno opis słowny, jak i wzór graficzny odnosiły się do potocznego rozumienia tych barw, bez podawania ich konkretnych parametrów.
W ustawie z dnia 31 stycznia 1980 r. o godle, barwach i hymnie Rzeczypospolitej Polskiej oraz o pieczęciach państwowych (), w załączniku nr 2, barwy flagi określono za pomocą współrzędnych trójchromatycznych, a dopuszczalne różnice barw za pomocą parametru ΔE*u*v*, w przestrzeni CIELUV:
Dobrym odwzorowaniem barwy czerwonej na fladze mogą być wzorce stosowane w komputerowych mieszalniach farb, które są zdefiniowane przez trzy parametry liczbowe, dające się jednoznacznie odnieść do podanych w Ustawie teoretycznych wartości CIELUV z uwzględnieniem dopuszczalnej odchyłki delta E. Kolorami farb, które spełniają podane kryteria są trzy odcienie ze wzornika Natural Color System: S1080-R, S1080-Y90R oraz 1085-Y90R, a w notacji standardu RAL, kolorem takim jest RAL3018 (truskawkowy).
Po przeliczeniach na system RGB (sRGB) otrzymuje się czerwień, której parametry RGB w zapisie szesnastkowym to   #D4213D  .
Czerwień wynikająca z wyliczeń parametrów podanych w ustawie oraz zamieszczona na kolorowym rysunku stanowiącym załącznik 3 ustawy jest w odcieniu karmazynu, czym nawiązuje do barw stosowanych do roku 1930 (ze względu na słowny opis barw obowiązujących w latach 1919–1930 nie można stwierdzić, czy jest to ten sam odcień czerwieni).
Flaga państwowa z godłem.
Od 1955 dwa rodzaje flag są w Polsce nazywane „flagą państwową”. Oprócz opisanej powyżej flagi biało-czerwonej istnieje także flaga z godłem Polski na białym prostokącie nazywana „flagą państwową z godłem”.
Flagę tę po raz pierwszy ustanowiono 1 sierpnia 1919 i początkowo przeznaczono dla polskich poselstw (przedstawicielstw dyplomatycznych) i konsulatów oraz statków handlowych. W latach 1928–1930 była tylko banderą handlową. Wynikało to z faktu, że flaga biało-czerwona była i jest do dziś jedną z flag Międzynarodowego Kodu Sygnałowego używanego do sygnalizacji na morzu (flaga biało-czerwona to flaga „H”, oznaczająca (jako sygnał jednoliterowy): „Mam pilota na statku”), zatem aby odróżnić od niej polską banderę, dodano godło.
Od 1930 ponownie używają jej polskie instytucje państwowe za granicą. Dekret z 7 grudnia 1955 potwierdził ten zakres używania flagi, rozszerzając go na lotniska cywilne, porty lotnicze oraz polskie samoloty komunikacyjne za granicą. Ustawa z 31 stycznia 1980 rozszerzyła zakres używania flagi także na kapitanaty i bosmanaty portów. Rysunek orła w czerwonym polu na białym pasie flagi zmieniał się wraz z urzędową zmianą godła państwowego, a obecny wzór herbu pochodzi z 9 lutego 1990 roku.
Aktualnie flagę państwową z godłem Rzeczypospolitej Polskiej podnoszą:
Bandera wojenna.
Modyfikacją flagi państwowej z godłem jest bandera wojenna ustanowiona po raz pierwszy ustawą z 1 sierpnia 1919 roku. Stanowiła ona, że „banderą morską wojenną” (ale także „flagą wojenną lądową”) będzie „chorągiew z wycięciem o barwach narodowych […] z herbem Rzeczypospolitej pośrodku białego pasa”. Wobec zmiany widniejącego na banderze godła państwowego, wraz z nim uległa zmianie także i sama bandera wojenna.
Obecny wzór pochodzi z ustawy z dnia 19 lutego 1993 r. o znakach Sił Zbrojnych Rzeczypospolitej Polskiej, która określiła, że „banderą wojenną jest prostokątny płat tkaniny o barwach Rzeczypospolitej Polskiej, zakończony dwoma trójkątnymi językami na wolnym liku. Pośrodku długości białego pasa, mierzonej od liku przydrzewcowego do wierzchołka wcięcia między językami, jest umieszczone godło Rzeczypospolitej Polskiej. Stosunek szerokości płata do jego długości wynosi 1:2,1. Głębokość wcięcia pomiędzy językami jest równa połowie szerokości płata. Stosunek wysokości godła do szerokości płata wynosi 2:5”.
W Rzeczypospolitej przedrozbiorowej polska bandera wojenna przedstawiała Orła Białego na czerwonym tle, statki handlowe nosiły natomiast czerwoną banderę z wizerunkiem ramienia trzymającego pałasz.
Znieważenie flagi i naruszenie przepisów o jej używaniu.
Znieważenie, niszczenie, uszkadzanie lub usuwanie flagi Polski to występek zagrożony karą grzywny, karą ograniczenia wolności albo karą pozbawienia wolności do roku. Art. 137 § 1 Kodeksu karnego w tym samym zakresie obejmuje ochroną godło, sztandar, chorągiew, banderę, flagę lub inny polski znak państwowy.
Art. 137 § 2 k.k reguluje kwestię znieważenia, niszczenia, uszkadzania lub usuwania flag lub innych znaków państwowych obcego państwa na terenie Polski. Ochrona prawna z tytułu art. 137 § 2 k.k ogranicza się do znaków wystawionych publicznie przez przedstawicielstwo tego państwa lub na zarządzenie polskiego organu władzy. Zgodnie z tak zwaną klauzulą wzajemności z art. 138 § 1 k.k znaki państwa obcego na terenie Polski są objęte ochroną jedynie w przypadku gdy państwo obce zapewnia wzajemność.
Naruszenie przepisów o używaniu flagi Polski stanowi wykroczenie określone w art. 49 § 2 Kodeksu wykroczeń zagrożone karą aresztu lub grzywny.
W poezji.
Pieśń o fladze napisał m.in. Konstanty Ildefons Gałczyński. Wiersz został napisany 1 października 1944 roku w stalagu (). Zaczyna się od słów:
Na Festiwalu Piosenki Żołnierskiej w 1969 roku za piosenkę „Biało-czerwona” został wyróżniony Krzysztof Cwynar.

</doc>
<doc id="1627" url="https://pl.wikipedia.org/wiki?curid=1627" title="Flaga Irlandii">
Flaga Irlandii

Flaga Irlandii (irl. "Bratach na hÉireann", pot. "Irish Tricolour") – jeden z symboli państwowych Republiki Irlandii.
Wygląd i symbolika.
Flaga Irlandii – prostokątna, trzy pasy o układzie pionowym: od lewej zielony, biały i pomarańczowy. Przyjmuje się, że kolor zielony oznacza katolików, pomarańczowy – protestantów, a biały – ciągłe dążenie do pokoju między nimi. Inna interpretacja kolorów: pomarańczowy oznacza wierność, zielony symbolizuje republikę a biały to pokój. Flaga potocznie nazywana jest "Irish Tricolour".
Historia.
Zestaw barw używany obecnie na fladze Irlandii po raz pierwszy pojawił się w formie kokard w 1830 podczas obchodów rocznicy rewolucji francuskiej i był inspirowany flagą Francji. Flagi w tym zestawieniu kolorów faktycznie zaistniały w czasie powstania w 1848 wywołanego przez ugrupowanie Młoda Irlandia, a za pomysłodawcę flagi uznawany jest jego przywódca, Thomas Francis Meagher.
W tym czasie przez irlandzki ruch narodowy używane były też różne inne flagi, głównie w dominującym kolorze zielonym, często z wizerunkiem harfy i z różnymi napisami.
W drugiej połowie XIX i w początkach XX wieku "Irish Tricolour" która dzięki swej trójbarwnej kompozycji kojarzyła się z flagą Francji, a przez to z republikanizmem, była używana głównie przez kręgi bezkompromisowo niepodległościowe i antybrytyjskie: Fenian, Ligę Gaelicką, a przede wszystkim przez Sinn Féin. „Konkurencyjna” i łatwiejsza do akceptacji przez władze brytyjskie flaga zielona z harfą (tak zwana "Green Flag"), była natomiast preferowana głównie przez zwolenników autonomii Irlandii (home rule), tak zwanych nacjonalistów westminsterskich.
Oficjalną flagą Irlandii w latach 1800–1922 pozostawała jednak skonstruowana przez władze brytyjskie biała flaga z czerwonym ukośnym krzyżem, tak zwanym krzyżem Świętego Patryka ("Saint Patrick Cross"), który jest także do dziś wpisany we flagę brytyjską.
W 1916 podczas powstania wielkanocnego "Irish Tricolour" została wywieszona na budynku Poczty Głównej w Dublinie wraz z zieloną flagą z napisem „Irish Republic”. Uznaje się na ogół, że w ten sposób została uznana „oficjalnie” za flagę narodową Irlandii (jakkolwiek powstanie wielkanocne było tylko jednostronną i z prawnego punktu widzenia samozwańczą proklamacją niepodległości).
Od 1922 "Irish Tricolour" nieprzerwanie pełni funkcję flagi narodowej i państwowej Irlandii: od 1922 Wolnego Państwa Irlandzkiego, kolejno od 1937 Éire i od 1949 Republiki Irlandii.
Konstrukcja i wymiary.
Prostokąt o proporcjach 1:2 podzielony na trzy równe sobie pionowe pasy: zielony, biały, pomarańczowy.
Inne flagi Irlandii.
Rolę flagi państwowej i narodowej Irlandii spełniało w przeszłości (lub pod pewnymi względami nadal spełnia) także kilka innych flag:

</doc>
<doc id="1628" url="https://pl.wikipedia.org/wiki?curid=1628" title="Flaga Nauru">
Flaga Nauru

Flaga Nauru – prostokątna, niebieska z poziomym, żółtym pasem przechodzącym przez środek flagi, pod nim 12-ramienna, biała gwiazda. Została przyjęta 31 stycznia 1968. Proporcje flagi wynoszą 1:2.
Symbolika.
Błękit symbolizuje bezkresne przestrzenie Oceanu Spokojnego, pas pośrodku – równik (żółty), biały – kolor fosforytów, największego bogactwa wyspy, gwiazda – położenie wyspy tuż na południe od równika. Dwunastoramienna gwiazda symbolizuje dwanaście pierwotnych plemion zamieszkujących Nauru. Umiejscowienie gwiazdy odnosi się do położenia samej wyspy – jeden stopień geograficzny od równika.

</doc>
<doc id="1629" url="https://pl.wikipedia.org/wiki?curid=1629" title="Flaga Brazylii">
Flaga Brazylii

Flaga Brazylii – jeden z symboli Brazylii.
Wygląd.
Flaga Brazylii ma postać prostokąta, na którym, na zielonym tle, znajduje się duży żółty romb, a w nim niebieska kula z gwiazdami (m.in. z Krzyżem Południa), przecięta białym pasem z napisem "Ordem e Progresso" (port. „ład i postęp”). Na niebie flagi odwzorowano układ gwiazd widziany w Rio de Janeiro w dniu 15 listopada 1889, czyli dniu ogłoszenia kraju republiką. Proporcje wymiarów flagi to 7:10.
Historia.
Projekt flagi Republiki Brazylii został przyjęty 19 listopada 1889. Koncepcję flagi opracował Raimundo Teixeira Mendesa we współpracy z Miguelem Lemosem i Manuelem Pereirą Reisem. W ostatecznej formie wykonał ją Décio Vilares. Na obecnie używanej fladze jest kilka nieznacznych zmian w stosunku do oryginału. Ostatnie zmiany zostały zatwierdzone 11 maja 1992.
Wcześniejszy projekt flagi dla nowej republiki inspirowany był flagą Stanów Zjednoczonych. Ta wersja obowiązywała tylko cztery dni (między 15 a 19 listopada 1889).
Symbolika.
Powszechnie uważa się, że barwy narodowe Brazylii – zieleń i żółcień – symbolizują bogactwa kraju. Zieleń oznacza równikowe lasy Amazonii, żółcień – złoto, które przyciągnęło tam europejskich kolonistów, a którego to Brazylia miała znaczne zasoby.
W rzeczywistości kolory te pochodzą z poprzedniej flagi Cesarstwa Brazylii i wywodzą się z barw rodu panującego. Kolor zielony był symbolem królewskiej rodziny Bragança i Piotra I, pierwszego cesarza Brazylii, a kolor żółty – rodziny Castela e Lorena – żony Piotra, Leopoldiny.
Na starej fladze cesarstwa widniał herb rodziny cesarskiej. Na sztandarze republiki został on zastąpiony niebieską kulą. Przedstawiony na niej układ gwiazd odzwierciedla niebo nad Rio de Janeiro rankiem 15 listopada 1889; w dniu, w którym Brazylia została ogłoszona republiką. Ukazany on jest spoza sfery niebieskiej (obraz jest lustrzanym odbiciem). Każda z 27 gwiazd reprezentuje jedną jednostkę podziału administracyjnego Brazylii (26 stanów oraz dystrykt federalny). Liczba gwiazd, początkowo 21, wzrastała wraz z wydzielaniem nowych regionów administracyjnych.
Gwiazda symbolizująca Dystrykt Federalny to sigma Octantis, która wskazuje biegun południowy nieba. Taki wybór był także symboliczny, gdyż gwiazda ta jest widoczna przez cały rok na obszarze całego kraju, a pozostałe gwiazdy na nieboskłonie (czyli stany Brazylii) wydają się obracać wokół niej.
Motto "Ordem e Progresso" („Ład i Postęp”) inspirowane było mottem pozytywizmu autorstwa Augusta Comte’a: "L’amour pour principe et l’ordre pour base; le progrès pour but" („Miłość jako zasada, porządek jako podstawa, a postęp jako cel”).
Gwiazdy na fladze.
Poszczególne gwiazdy symbolizują jednostki podziału administracyjnego kraju:

</doc>
<doc id="1630" url="https://pl.wikipedia.org/wiki?curid=1630" title="Flaga">
Flaga

Flaga – płat tkaniny określonego kształtu i barwy (barw) przymocowywany do drzewca; może zawierać godła, symbole, wizerunki. Flaga jest znakiem rozpoznawczym, symbolem państwa (flaga państwowa) lub organizacji, czasem również miast (flaga miejska) czy jednostek podziału administracyjnego, oddziału wojsk (sztandar), organizacji: politycznych, społecznych, kościelnych, sportowych itd. lub jednostki.
Flagi są również używane jako środek przekazywania sygnałów wizualnych (np. flagi sygnałowe ułatwiające komunikację z wykorzystaniem alfabetu semaforowego), w tym również umownych (np. biała flaga parlamentariusza i kapitulacji) i kodowych (np. flagi sygnalizujące dostępność kąpieliska, flagi wyścigowe w sportach samochodowych, flagi Międzynarodowego Kodu Sygnałowego w żegludze).

</doc>
<doc id="1631" url="https://pl.wikipedia.org/wiki?curid=1631" title="Flaga olimpijska">
Flaga olimpijska

Flaga olimpijska – symbol igrzysk olimpijskich w formie prostokątnej flagi przedstawiającej na białym tle pięć złączonych kół. Symbol olimpijski (koła) zaprojektował inicjator wskrzeszenia idei olimpijskiej, baron Pierre de Coubertin w 1912 a flagę w 1913. Wykonana została w zakładzie krawieckim „Bon Marche” w Paryżu.
Jej wymiary to: trzy metry długości i dwa metry szerokości. Emblemat pięciu kół olimpijskich umieszczony jest centralnie, na białym polu bez obramowania. Koła są splecione ze sobą w dwóch szeregach (od lewej do prawej):
Pierwszy raz została wyeksponowana publicznie w amfiteatrze Uniwersytetu w Paryżu w 1914, podczas uroczystości z okazji 20-lecia podjęcia decyzji o wznowieniu igrzysk olimpijskich. W ramach tych uroczystości odbył się V Kongres Olimpijski, na którym oficjalnie zatwierdzono flagę olimpijską.
Inspiracją dla barona były V Letnie Igrzyska Olimpijskie, które odbyły się w 1912 w Sztokholmie, gdzie po raz pierwszy uczestniczyły reprezentacje państw z pięciu kontynentów.
Flaga olimpijska jest jednym z najważniejszych symboli olimpijskich. Wciągana jest na maszt na stadionie olimpijskim podczas ceremonii otwarcia i pozostaje tam przez całe igrzyska olimpijskie aż do ceremonii zamknięcia. Pierwszy sztandar wciągnięto na maszt podczas VII Letnich Igrzysk Olimpijskich w Antwerpii w 1920 – pierwszy egzemplarz znajduje się obecnie w Muzeum Olimpijskim w Lozannie.
Okręgi w rzeczywistości symbolizują pięć kontynentów. Sześć barw (pięć powyższych plus biała) ma oznaczać kolory obecne w roku 1913 na wszystkich flagach państw świata, na przykład Grecji, Szwecji, Hiszpanii, Francji, Wielkiej Brytanii, Stanów Zjednoczonych, Niemiec, Włoch, Belgii i Węgier.
Kolory kół na fladze symbolizują różnorodność i jedność ludzi. Poszczególne kolory symbolizują kontynenty:
Pierre de Coubertin nawiązywał również do nowatorskiej flagi Brazylii i Australii, oraz starożytnych symboli Japonii i jemu współczesnych Chin.
Flaga olimpijska jest chroniona prawnie jako znak towarowy notoryjny (powszechnie znany). Oprócz tego 41 krajów (między innymi Polska) jest sygnatariuszami międzynarodowej konwencji, tak zwanego Traktatu z Nairobi, mówiącej o ochronie idei symbolizowanych przez flagę olimpijską i ściganiu nadużyć związanych z jej wykorzystywaniem.

</doc>
<doc id="1632" url="https://pl.wikipedia.org/wiki?curid=1632" title="Flaga Czerwonego Krzyża">
Flaga Czerwonego Krzyża



</doc>
<doc id="1633" url="https://pl.wikipedia.org/wiki?curid=1633" title="Floren">
Floren

Floren – złota moneta o masie ok. 3,5 grama, bita przez Florencję od 1252 roku. Jej stabilna wartość pozwoliła stać się główną złotą monetą w Europie. Od XIV w. była często naśladowana, m.in. przez Włochy, Czechy, Węgry, Francję oraz Niderlandy. Jej weneckim odpowiednikiem był początkowo cekin, potem dukat.
Floren szybko stał się standardem i przez stulecia był naśladowany w wielu krajach europejskich (podobnie jak grosz i srebrny talar). Własne jego naśladownictwa bili m.in. , także pod względem ikonograficznym (lilia na awersie, postać świętego Jana Chrzciciela na rewersie), co miało ułatwić ich odbiór na szerszym rynku.
Najważniejszą ich wschodnioeuropejską odmianą (z królem w majestacie na awersie i herbem na rewersie) są floreny bite od XIV wieku przez władców królestwa Węgier, gdzie wówczas odkryto największe w Europie złoża złota (jego roczne wydobycie na początku XIV wieku przekraczało pięciokrotnie łączne wydobycie na Śląsku i w Czechach, drugiego wtedy na kontynencie obszaru złotonośnego). Ze zniekształcenia oryginalnej nazwy pochodzi określenie forinta.
Inną obcokrajową odmianę stanowi bardzo nieliczna emisja florenów polskich Władysława Łokietka z inskrypcją WLADISLAVS D(e)I G(ratia) REX – S(anctus) STANISLAVS POL(oni)E, której wygląd został częściowo zapożyczony z typu węgierskiego (władca w majestacie na awersie) i florenckiego (za pośrednictwem florenów węgierskich typ ten naśladujących – gdyż i tam bito kopie florenów florenckich). Świadczy o tym umieszczona na rewersie postać świętego Stanisława, którą zastąpiono wizerunek św. Jana Chrzciciela. W zbiorach polskich znajduje się obecnie tylko jeden egzemplarz tej monety (drugi domniemany jest własnością obywatela Litwy).
W Polsce florenem nazywano będące w obiegu monety złote. Z czasem określenie to zostało wyparte przez nazwę dukat (także czerwony złoty lub czerwoniec).

</doc>
<doc id="1635" url="https://pl.wikipedia.org/wiki?curid=1635" title="Fenig">
Fenig

Fenig (niem. "Pfennig, Pfenning") – potoczna nazwa denara używana w krajach germańskich od VIII do X wieku. Później nazwa bitej z miedzi drobnej monety stanowiącej część marki. 
Przejęty z wczesnośredniowiecznego systemu monetarnego, jako moneta zdawkowa wszedł do systemów walutowych krajów niemieckich. W okresie poprzedzającym reformę walutową z lat 1871-73 w II Rzeszy fenig odpowiadał wartości 1/300 talara, a w następstwie wprowadzenia marki równej trzeciej jego części, w systemie decymalnym stał się jej setną (1/100) częścią.
Do 1 stycznia 2002 był monetą zdawkową używaną w Niemczech, a wcześniej w obu powojennych państwach niemieckich (RFN i NRD).
Na ziemiach polskich feniga używano w państwie krzyżackim, później w pruskim oraz na Śląsku i na Pomorzu Zachodnim. Żelazne monety fenigowe (w nominałach 1, 5, 10 i 20) jako rozmienność marki polskiej emitowano w latach 1916-1917 w okupowanym Królestwie Polskim, a do kwietnia 1924 były monetami obiegowymi w odrodzonej Rzeczypospolitej. 

</doc>
<doc id="1636" url="https://pl.wikipedia.org/wiki?curid=1636" title="Fm">
Fm



</doc>
<doc id="1637" url="https://pl.wikipedia.org/wiki?curid=1637" title="Fr">
Fr



</doc>
<doc id="1639" url="https://pl.wikipedia.org/wiki?curid=1639" title="Modulacja częstotliwości">
Modulacja częstotliwości

Modulacja częstotliwości, FM () – kodowanie informacji w fali nośnej przez zmiany jej chwilowej częstotliwości w zależności od sygnału wejściowego, tj. jej modulację.
Modulacja częstotliwości jest systemem transmisji sygnału analogowego stosowanym do przesyłania sygnału radiowego radia rozsiewczego w zakresie fal ultrakrótkich, stąd zakres ten w mowie potocznej często określa się jako „FM”. Modulację częstotliwości stosowało się też w transmisji sygnału w analogowej telewizji satelitarnej oraz dźwięku w większości analogowych systemach telewizji naziemnej oraz informacji o kolorze (chrominancji) w systemie telewizji kolorowej SECAM. System ten umożliwia odfiltrowanie po stronie odbiornika znacznie więcej zakłóceń niż w systemie AM. Sygnał po odebraniu i wzmocnieniu może być ograniczony do takiej samej amplitudy, dzięki czemu udaje się wyeliminować większość zakłóceń.
Częstotliwość sygnału nośnego o częstotliwości formula_1 zmienia się w zakresie od formula_2 do formula_3 gdzie formula_4 to tzw. "dewiacja częstotliwości"; stosunek formula_5 nazywa się "wskaźnikiem dewiacji częstotliwości" lub "współczynnikiem modulacji częstotliwości".
Realizacja modulatora FM.
Jako modulator FM najczęściej stosuje się diodę pojemnościową w obwodzie rezonansowym generatora, która zmieniając swoją pojemność w takt zmian napięcia sygnału modulującego zmienia częstotliwość generowanej fali nośnej.
FM cyfrowe.
Oznaczenie FM stosuje się też do oznaczania niektórych technik kodowania informacji cyfrowej. Najbardziej znany jako system zapisu informacji na nośnikach magnetycznych. Przy odczycie sygnału z takiego nośnika dość trudny jest sam odczyt namagnesowania, znacznie łatwiejszy jest odczyt zmian namagnesowania (pola magnetycznego), gdyż wywołuje ono powstanie prądu indukcyjnego w głowicy magnetycznej.
Przy zapisie FM kierunek pola magnetycznego zmieniany jest na początku komórki bitowej oraz w jej środku, ale tylko wtedy, gdy kodowana jest jedynka (binarna).
Algorytm kodowania 1 bitu metodą FM:
Zapis liczby binarnej 010011 wywoła następujący przepływ prądu:
Nadawaniu ciągu jedynek odpowiada sygnał o częstotliwości dwa razy większej niż przy nadawaniu ciągu zer, dlatego ten sposób zapisu nazwano FM. Sposób ten został wyparty przez system MFM, a następnie przez RLL.

</doc>
<doc id="1640" url="https://pl.wikipedia.org/wiki?curid=1640" title="Faktoria">
Faktoria

Faktoria ( „robić, czynić”) – placówka handlowa położona w krajach kolonialnych, które stanowiły obszary eksploatacji przez europejskie potęgi kolonialne, głównie przez Brytyjczyków, Francuzów czy Holendrów. Lokalizowane zazwyczaj na wybrzeżu, tak by wywóz surowców z nowych obszarów był jak najbardziej ułatwiony.
Opis.
Początkowo faktorie stanowiły punkty handlowe, gdzie europejscy kupcy spotykali się z autochtonami, sprzedającymi produkty rolne oraz swoje wyroby. Później faktorie stanowiły przyczółki głębszej penetracji nieznanych bądź jeszcze nie podbitych obszarów. Faktorie stanowiły główne punkty, przez które przechodziły towary ze skolonizowanych krajów. Najbardziej znane faktorie handlowe powstawały w Kanadzie, na wybrzeżu afrykańskim i indyjskim, a także na Dalekim Wschodzie.
Dzisiaj słowa „faktoria” używa się do określenia fabryki, zakładu lub skupiska placówek handlowych, oraz zamiennie do słowa „manufaktura”.
Nazwę faktoria można spotkać w literaturze młodzieżowej o Indianach autorów: Karl May, James Fenimore Cooper, James Curwood czy Longin Jan Okoń, Krystyna i Alfred Szklarscy, Adam Bahdaj i Arkady Fiedler.

</doc>
<doc id="1641" url="https://pl.wikipedia.org/wiki?curid=1641" title="Fale dekametrowe">
Fale dekametrowe



</doc>
<doc id="1643" url="https://pl.wikipedia.org/wiki?curid=1643" title="Fale średnie">
Fale średnie

Fale średnie, fale hektometrowe, ŚR, Ś, MF (od ang. "medium frequency") – zakres fal radiowych (pasmo radiowe) o częstotliwości: 300–3000 kHz i długości 100–1000 m.
Obecnie radiofonie europejskie nadają programy na falach średnich w zakresie 522–1611 kHz z odstępem między kanałami 9 kHz. W Ameryce Północnej zakres fal średnich to 515–1715 kHz, a odstęp międzykanałowy wynosi 10 kHz.
W Polsce, poza nielicznymi stacjami projektu „Twoje Radio AM”, których właścicielami są gminy, na falach średnich nie są obecnie nadawane szerzej słuchane programy. Polskie Radio zrezygnowało z emisji na falach średnich pod koniec lat 90. XX w. Dzięki właściwościom propagacyjnym fal średnich kilkaset stacji zagranicznych jest słyszalnych w Polsce w porze wieczornej i nocnej.
W zakresie średniofalowym działają radiolatarnie bezkierunkowe (NDB) wykorzystywanie w radionawigacji lotniczej. Na częstotliwościach pośrednich nadają stacje referencyjne DGPS (283,5–325 kHz). W tym zakresie nadawane są również komunikaty meteorologiczne systemu Navtex (490 i 518 kHz). Częstotliwości z przedziału 1810–2000 kHz (pasmo 160 m) oraz (w niektórych krajach) niewielki zakres częstotliwości w okolicach 500 kHz (pasmo 500 kHz) przyznane zostały krótkofalowcom.
Propagacja fal średnich.
Fale średnie rozchodzące się w jonosferze ulegają silnej absorpcji. W ciągu dnia tłumienie jest tak duże, że fala jonosferyczna praktycznie nie występuje i o zasięgu decyduje jedynie fala przyziemna. Po zmroku absorpcja fali jonosferycznej ulega zmniejszeniu i zasięg zwiększa się do kilkuset kilometrów.

</doc>
<doc id="1644" url="https://pl.wikipedia.org/wiki?curid=1644" title="Fale hektometrowe">
Fale hektometrowe



</doc>
<doc id="1646" url="https://pl.wikipedia.org/wiki?curid=1646" title="Fale długie">
Fale długie

Fale długie ("fale kilometrowe") (ang. LF – "Low Frequency" lub LW – "Long Waves"), zakres fal radiowych (pasmo radiowe) o częstotliwości: 30-300 kHz co odpowiada długości fali 10-1 km. Zakres ten jest przeznaczony głównie dla rozgłośni radiowych w I regionie ITU (Europa, Afryka i Bliski Wschód).
Obecnie w programowej radiofonii europejskiej używany jest zakres 144–288 kHz z odstępami pomiędzy kanałami wynoszącymi 9 kHz. W paśmie tym nadawane są też na zasadzie rozgłaszania dane cyfrowe np. wzorce czasu, jednym z nich jest europejski radiowy sygnał zegarowy DCF77 nadawany spod Frankfurtu nad Menem na częstotliwości 77,5 kHz.
W paśmie fal długich na częstotliwości 225 kHz nadaje Polskie Radio Program I (za pośrednictwem Radiowego Centrum Nadawczego w Solcu Kujawskim).
Częstotliwości powyżej 190 kHz wykorzystywane są również przez radiolatarnie bezkierunkowe (NDB).
Propagacja fal długich.
Fale długie w wyniku bardzo małego tłumienia i dużej dyfrakcji (łatwo uginają się na przeszkodach) rozchodzą się na duże odległości za pomocą fali powierzchniowej. Ulegają również odbiciu od najniższych warstw jonosfery o bardzo niskiej gęstości elektronowej i praktycznie nie wnikają dalej. W ciągu dnia odbijają się od dolnej części warstwy D, a nocą, gdy warstwa D zanika, od warstwy E. W odległości 1000-2000 km do nadajnika natężenie pola fali jonosferycznej przewyższa natężenie pola fali powierzchniowej. Zasięg fal długich jest duży i wynosi, zarówno w dzień i w nocy, kilka tysięcy kilometrów.
Do oszacowania natężenie pola elektrycznego na falach długich służy półempiryczny wzór Austina:
gdzie
Przy czym średnie wahania roczne natężenia pola w danym punkcie mogą wynosić 20-50%. Największe natężenie występuje latem w dzień, a zimą w nocy.

</doc>
<doc id="1647" url="https://pl.wikipedia.org/wiki?curid=1647" title="Fale kilometrowe">
Fale kilometrowe



</doc>
<doc id="1649" url="https://pl.wikipedia.org/wiki?curid=1649" title="Funt (masa)">
Funt (masa)

Funt ( lub , od staroangielskiego "pund", które wywodzi się z germańskiej adaptacji łacińskiego "(libra) pondo" – "Pfund"; lb, lb"m", lbm, ℔) – pozaukładowa jednostka masy wywodząca się od rzymskiej libry. Miara funta była różna na przestrzeni wieków w różnych państwach, zwykle wynosiła pomiędzy 0,4 a 0,5 kilograma. Obecnie w państwach anglosaskich jest przyjęty międzynarodowy funt równy 0,453 592 37 kg oraz stosowany jest skrót lb (od "libra", liczba mnoga w jęz. angielskim: lbs).
Współczesne rodzaje funta.
Masa funta "avoirdupois" została określona przez kupców w Londynie w 1303. Jego dokładna miara i sposób ustalania zmieniały się. W 1878 zdefiniowano wzorzec funta jako odpowiadający ok. 0,453 592 338 kg, w 1883 – ok. 0,453 592 427 kg, wreszcie od 1889 w zaokrągleniu jako 0,453 592 43 kg. Od 1894 funt równy 0,453 592 427 kg przyjęto również w Stanach Zjednoczonych. Współczesny funt międzynarodowy o mierze 0,453 592 37 kg przyjęto w 1958 w porozumieniu między USA a krajami Brytyjskiej Wspólnoty Narodów.
W niektórych regionach krajów niemieckojęzycznych, przeważnie jednak w Niemczech słowa „funt” ("Pfund") używa się w potocznym języku zamiast „pół kilograma”.
Funt jest także jednostką używaną w oznaczaniu siły naciągu łuku oraz krzywej ugięcia wędzisk (charakterystyka pracy/sztywność).

</doc>
<doc id="1650" url="https://pl.wikipedia.org/wiki?curid=1650" title="Fale bardzo długie">
Fale bardzo długie

Fale bardzo długie (fale myriametrowe, VLF – z ang. "very low frequency") – zakres fal radiowych o częstotliwości: 3–30 kHz (długości 10–100 km).
Propagacja fal bardzo długich.
Fale bardzo długie w wyniku bardzo małego tłumienia i dużej dyfrakcji rozchodzą się na duże odległości za pomocą fali powierzchniowej. Ulegają również odbiciu od najniższych warstw jonosfery o bardzo niskiej gęstości elektronowej i praktycznie nie wnikają dalej. W ciągu dnia odbijają się od dolnej części warstwy D, a nocą, gdy warstwa D zanika, od warstwy E. W odległości 1000–2000 km do nadajnika natężenie pola fali jonosferycznej przewyższa natężenie pola fali powierzchniowej. Zasięg fal bardzo długich jest duży i wynosi, zarówno w dzień i w nocy, kilka tysięcy kilometrów.
Wykorzystanie.
Ponieważ fale VLF mogą przenikać przez wodę morską, są wykorzystywane do komunikacji z okrętami podwodnymi zanurzonymi na niewielkich głębokościach. W Oceanie Atlantyckim, gdzie zasolenie wynosi 3,2%, fale VLF mogą przenikać na głębokość 10-20 metrów; przy mniejszym zasoleniu, np. w Morzu Śródziemnym, czy Bałtyckim ten sam sygnał może przeniknąć na głębokość ponad 40 metrów. Fale VLF stosowane są również w elektromagnetycznych badaniach geofizycznych, jak również w systemach nawigacyjnych dalekiego zasięgu, np. RSDN-20.

</doc>
<doc id="1651" url="https://pl.wikipedia.org/wiki?curid=1651" title="Fale myriametrowe">
Fale myriametrowe



</doc>
<doc id="1652" url="https://pl.wikipedia.org/wiki?curid=1652" title="Fale decymetrowe">
Fale decymetrowe

Fale decymetrowe, UHF (z ang. "ultra high frequency" – fale ultra wielkiej częstotliwości) – zakres fal radiowych o częstotliwości 300–3000 MHz i długości 10–100 cm.
Zastosowanie.
Pasmo zakresu UHF znajduje powszechne zastosowanie w radiodyfuzji telewizji na świecie.
Pasmo fal decymetrowych wykorzystywane jest też przez sieci telefonii komórkowej. W Polsce używany jest standard GSM 900/1800, używający częstotliwości z zakresu 880–960 MHz i 1710–1880 MHz. Technologia UMTS wykorzystuje częstotliwości ok. 1,9 GHz i 2,1 GHz.
Bezprzewodowe sieci komputerowe Wi-Fi (2,4 GHz) pracują w tym paśmie, na częstotliwościach od 2,400 GHz do 2,483 GHz. Częstotliwość ta wykorzystywana jest też w łączności Bluetooth.
W Polsce na częstotliwościach 430–440 MHz znajduje się pasmo przeznaczone dla krótkofalowców, pasmo PMR 446 MHz oraz dwa pasma ISM 433 i 868 MHz.
Pasmo IV i V.
Pasmo telewizyjne UHF obejmuje kanały telewizyjne 21–37 o częstotliwości 470–606 MHz dla zakresu IV i kanały telewizyjne 38–69 o częstotliwości 606–862 MHz dla zakresu V. Szerokość kanału wynosi 8 MHz. Dla odróżnienia z pasmem hyperband telewizji kablowej, numery kanałów telewizji naziemnej są oznaczane literą C.
Kanały 61–69 zostały zajęte przez sieć komórkową 4G (LTE). Kanały 49–60 zostały zwolnione z nadawania sygnału telewizyjnego w ramach Refarmingu 700 MHz i zostaną zajęte przez sieć komórkową 5G.
Nadawanie telewizji analogowej w IV i V paśmie w Polsce rozpoczęto w latach siedemdziesiątych.

</doc>
<doc id="1653" url="https://pl.wikipedia.org/wiki?curid=1653" title="Fale centymetrowe">
Fale centymetrowe

Fale centymetrowe (, SHF) – zakres fal radiowych (pasmo radiowe) o częstotliwości: 3-30 GHz i długości 1-10 cm.
Stosowane w łączności satelitarnej up-link, down-link, międzysatelitarnej, radioliniach łączności naziemnej, urządzeniach radiolokacyjnych (radar), radioastronomii, łączności z sondami kosmicznymi (również międzyplanetarnymi i głębokiego kosmosu - sondy Pioneer i Voyager, sonda Deep Space).
W tym zakresie pracują też niektóre bezprzewodowe sieci komputerowe Wi-Fi (5 GHz) i WiMax.
Pasmo SHF występuje również w połączeniu konwerter - dekoder poprzez kabel koncentryczny.

</doc>
<doc id="1654" url="https://pl.wikipedia.org/wiki?curid=1654" title="Fale milimetrowe">
Fale milimetrowe

Fale milimetrowe (ang. EHF - "Extremely high frequency") – zakres fal radiowych (pasmo radiowe) obejmujący częstotliwości: 30-300 GHz i długości 1-10 mm.
Stosowane w łączności satelitarnej, radiolokacji (radar), radioastronomii, ostatnio - w badaniach nad bronią elektromagnetyczną. 

</doc>
<doc id="1655" url="https://pl.wikipedia.org/wiki?curid=1655" title="Fale submilimetrowe">
Fale submilimetrowe

Fale submilimetrowe – zakres fal elektromagnetycznych zaliczany do dalekiej podczerwieni o częstotliwości od 300 GHz do 3 THz, czyli o długości fali w przedziale 1 mm - 100 μm. Promieniowanie zaliczane także do fal radiowych i określane jako fale terahercowe (THF).
W badaniach naukowych.
W zakresie tych fal prowadzone są obserwacje radioastronomiczne za pomocą teleskopów:

</doc>
<doc id="1656" url="https://pl.wikipedia.org/wiki?curid=1656" title="Finowie">
Finowie

Finowie (fiń. "suomalaiset") – naród ugrofiński zamieszkujący Finlandię, ok. 7 mln osób. Posługują się językiem fińskim, sporadycznie także szwedzkim i rosyjskim.
Są oni potomkami ugrofińskich plemion, które przybyły nad Zatokę Fińską. Miało miejsce to – według różnych szacunków – już w IV tysiącleciu p.n.e. lub dopiero w I tysiącleciu n.e. Od samego początku Finowie znajdowali się pod wpływami zarówno kultury zachodu, jak i wschodu. Dotyczy to zarówno chrystianizacji kraju i wpływu na nie dwóch wyznań chrześcijańskich, jak i wpływu kultury szwedzkiej i wschodniosłowiańskiej na fińską kulturę ludową, w której widoczne są odrębności pomiędzy wschodnimi a zachodnimi Finami.

</doc>
<doc id="1657" url="https://pl.wikipedia.org/wiki?curid=1657" title="Kinematografia amerykańska">
Kinematografia amerykańska

Początek kina.
Tworzą go prototypy urządzeń kinematograficznych takich jak camera obscura i laterna magica zwana też latarnią czarodziejską. Istniały też różnego rodzaju aparaty optyczne do pokazywania ruchomych obrazków – zoetrop, stroboskop, praksinoskop, fenakistiskop. Właściwa ewolucja kina rozpoczęła się jednak z chwilą wynalezienia fotografii i projekcji kinematograficznej.
W 1891 Thomas Edison opatentował kamerę kinetoskop opracowaną według pomysłu Williama Kennedy Laurie Dicksona. Wkrótce pojawiły się miejsca, gdzie ustawiono pudła, w których można było oglądać ruchome obrazki przez wizjer, obrazki wyświetlane z taśmy filmowej. Na początku nazywano je „arcade peepshows”, a później nickelodeonami (nazwa od drobnych niklowych monet stanowiących zapłatę za pokaz). Pierwszy materiał do kinetoskopu powstał w 1894, był to "Fred Ott’s sneeze" ("Kichanie Freda Ott") znany też pod tytułem "Edison Kinetoscopic Record of a Sneeze" ("Kamera Edisona rejestruje kichnięcie"). Miał 5 sekund i był chroniony patentem Edisona.
W 1893 Edison Laboratories zbudowało pierwsze studio filmowe w West Orange w New Jersey.
W 1894 we Francji skonstruowano urządzenie znacznie lepsze niż kinetoskop – Bracia Auguste i Louis Lumière skonstruowali kinematograf. Pozwalał on na wyświetlanie filmu na ekranie, w związku z czym jednocześnie mogło oglądać film wielu widzów. Upowszechnił się on na początku XX w. na całym świecie.
William Dickson odszedł od Edisona i z kilkoma inwestorami założył firmę American Mutoscope Company. Wytwarzał w niej mutoskop – urządzenie konkurencyjne wobec kinetoskopu i jak Edison, produkował do niego filmy. Rozwijając ideę braci Lumière, American Mutoscope opracował „biograph” – projektor, który pozwalał na pokazywanie filmów na ekranie dla wielkiej publiczności, co zaczęło stanowić potężną konkurencję dla nickelodeonów. Edison odpowiedział wielkim projektorem nazwanym Vitascope. Jednocześnie uważając, że to on ma wyłączność patentową na film w Ameryce, rozpoczął ostrą walkę z wszelką konkurencją.
Kino nieme.
Kino nieme to masowa produkcja krótkich jedno-, dwuaktowych komedii i melodramatów na początku XX w. Tanie kina, do których bilet kosztował 5 centów ("nickel") zwano "nickelodeonami". Pokaz odbywał się zwykle z podkładem muzyki granej na żywo na pianinie. Rozgrywki między rosnącymi jak grzyby po deszczu wytwórniami sprawiły, że część z nich przeniosła się na wybrzeże zachodnie, gdzie zresztą warunki do filmowania (a więc np. oświetlenie i pogoda) były lepsze.
Głównie była to ucieczka przed agentami Motion Picture Patents Company Edisona, który miał patenty filmowe na USA i bezlitośnie ścigał wszystkich, którzy chcieli złamać jego monopol na kręcenie filmów. Odległość od New Jersey, gdzie była siedziba The Edison Manufacturing Co., do Kalifornii była tak duża, że niezależni filmowcy czuli się tu bezpiecznie.
Pierwsze studio filmowe w Hollywood to Nestor Studio założone w 1911 przez producentów Ala Christie i Davida Horsley. W tym samym roku pojawili się tu inni niezależni producenci.
Wkrótce najważniejszym ośrodkiem amerykańskiej sztuki filmowej stała się dzielnica Los Angeles – Hollywood. W latach 20. powstał tu system pięciu wielkich wytwórni: Fox (później 20th Century Fox), Loew’s Incorporated (później Metro-Goldwyn-Mayer), Paramount Pictures, RKO (Radio-Keith-Orpheum) i Warner Bros., działające zarówno jako producenci filmów, jak i dystrybutorzy i właściciele kin. Zatrudniały też własne gwiazdy aktorskie. Znaczenie trzech następnych wytwórni Universal Studios, Columbia Pictures i United Artists było też duże, ale nie posiadały one własnych kin i własnych gwiazd aktorskich. W opozycji do „wielkiej piątki” było ośmiu niezależnych producentów wśród których byli: Samuel Goldwyn, David O. Selznick, Walt Disney i Walter Wanger.
Stany Zjednoczone szybko uległy magii kinematografii. Kino pozwalało oderwać się od niełatwej codzienności, a dużej części społeczeństwa umożliwiało obcowanie z dotychczas dość hermetyczną sztuką. Olbrzymie studia filmowe tworzyły dekoracje, na tle których rozgrywały się wielkie sceny historyczne, ekranizacje wielkich powieści. Rozwijała się technika filmowania, operatorzy zaczęli wykorzystywać grę światła i cienia, a także różne ujęcia kamery.
W 1919 powstała wytwórnia United Artists Corporation założona przez filmowców Charliego Chaplina, D.W. Griffitha, Douglasa Fairbanksa, i Mary Pickford, którzy chcieli tworzyć filmy artystyczne.
Ważne filmy kina niemego.
Filmy powstałe przed epoką kina dźwiękowego, zaliczane obecnie do wielkich osiągnięć kinematografii amerykańskiej:
Od samego początku kino amerykańskie nastawione było na lansowanie gwiazd aktorskich. Oprócz ww. komików wielkimi gwiazdami amerykańskiego kina niemego byli także: Mary Pickford, Lillian Gish, Pola Negri, Douglas Fairbanks, Theda Bara, Barbara La Marr, Rudolph Valentino, Gloria Swanson, Ramón Novarro, John Barrymore i in.
Kino dźwiękowe.
Nowy etap w historii kinematografii – nie tylko amerykańskiej, ale i światowej – rozpoczął się w 1927, gdy na ekranach ukazał się pierwszy film dźwiękowy "Śpiewak jazzbandu" Alana Croslanda z gwiazdą Alem Jolsonem. Wynalazek został przyjęty entuzjastycznie przez widzów, a z konsternacją przez samych twórców kina niemego. Charlie Chaplin twierdził, że jest to sezonowa nowinka techniczna i sam swoje filmy "Światła wielkiego miasta" (1931) i "Dzisiejsze czasy" (1936) nakręcił bez dialogów, chociaż była w tych filmach muzyka i efekty dźwiękowe. Główny zarzut dotyczył problemów technicznych z nagrywaniem dźwięku na planie i związanymi z tym wielkimi ograniczeniami w ruchu kamery. Problemy te znakomicie zostały pokazane w dużo późniejszym filmie "Deszczowa piosenka" (1952).
W 1929 King Vidor zrealizował "Dusze czarnych", obraz uważany za pierwsze wybitne dzieło kina dźwiękowego.
Lata 30. i 40. („złote lata Hollywood”).
To okres produkcji ponad 400 filmów rocznie, a 90 milionów widzów co tydzień było w amerykańskich kinach. Wtedy też ukształtowały się najbardziej typowe gatunki kina amerykańskiego: western, komedie slapstickowe, film noir (czarny film gangsterski), musical, rysunkowy film animowany (Walt Disney), film biograficzny, melodramat itp.
Produkcja filmów stała się biznesem skoncentrowanym w wytwórniach zatrudniających setki producentów, reżyserów, scenarzystów, aktorów, kaskaderów, techników kina przybyłych po I wojnie światowej z Europy do Stanów Zjednoczonych, zachęconych lepszymi warunkami pracy i wyższymi zarobkami, którzy przyczynili się do powstania wielkiej gałęzi przemysłu filmowego. Oprócz dużej ilości filmów miałkich powstało w tym okresie wiele znakomitych dzieł z "Obywatelem Kane" w reż. Orsona Wellesa na czele (premiera 1 maja 1941). Najbardziej znani z nich:
Filmy „wielkiego kryzysu”.
Na ekrany weszły klasyczne dziś horrory z Draculą i Frankenstein (1931). W 1933 miał premierę "King Kong." Howard Hughes zrealizował film wojenny "Aniołowie piekieł" ("Hell’s Angels") (1930). Walt Disney wyprodukował swoje pierwsze krótkie filmy animowane w na taśmie kolorowej (Technicolor). Karierę rozpoczęli bracia Marx ze swoimi zwariowanymi komediami – "Małpi interes", "Końskie pióra", i "Kacza zupa". Powstały znakomite filmy gangsterskie, prekursorskie dla gatunku noir – "Mały Cezar" w reż. Mervyna LeRoya z Edwardem G. Robinsonem, "Wróg publiczny" Williama A. Wellmana z Jamesem Cagneyem i "Człowiek z blizną" Howarda Hawksa.
Gwiazdy lat 30. i 40..
Umocniły swe pozycje w kinematografii po wielkim kryzysie gospodarczym. W największych wytwórniach zawierano długoletnie kontrakty z reżyserami i aktorami:
Aktorzy byli czasami „wypożyczani” do innych wytwórni. Grali jednocześnie w czterech czy nawet pięciu filmach. Humphrey Bogart wystąpił w 36 filmach pomiędzy 1934 a 1942. "Casablanca" była jednym z pięciu jego filmów realizowanych w 1943.
Film noir.
Film noir jest uważany przez jednych za nurt w kinie amerykańskim, przez drugich za gatunek filmowy. Zaliczane są do niego zarówno filmy detektywistyczne, jak i mroczne romanse (np. "Rebeka" ("Rebecca") Hitchcocka (1940) czy "Casablanca" Michaela Curtiza (1942). Największą gwiazdą, grającą role zarówno detektywów, jak i gangsterów był Humphrey Bogart.
Europejscy krytycy bardzo lubią klasyfikowanie filmów nie według gatunków, lecz według kierunków, nurtów i szkół filmowych. Termin film noir (czarny film) wymyślony został po II wojnie światowej przez francuskiego krytyka filmowego Nino Franka. W 1946 we Francji można było jednocześnie obejrzeć "Sokoła maltańskiego" ("The Maltese Falcon"), Johna Hustona (1941), "High Sierra" Raoula Walsha (1941), "Laurę" Otto Premingera (1944), "Żegnaj laleczko" ("Murder, My Sweet") Edwarda Dmytryka (1944), "Podwójne ubezpieczenie" ("Double Indemnity") Billy Wildera (1944) i "Kobietę w oknie" ("Woman in the Window"), Fritza Langa (1945). Filmy powstałe w pierwszej połowie lat 40., niezależnie od siebie, niewyświetlane w Europie ze względu na wojnę, oglądane w jednym momencie ukazały wiele wspólnych, nowych jakości formalnych i treściowych, pozwalających na wyodrębnienie nowego zjawiska, które w równym stopniu jak neorealizm włoski wywarło wpływ na powojenne kino. Nino Frank – pisał, że obrazy te określane zwykle mianem „filmu detektywistycznego” lub „gangsterskiego”, bardziej zasługują na określenie „przygoda kryminalna” lub „psychologia zbrodni”.
Wielki wpływ na twórców filmowych wywarli pisarze tzw. czarnych kryminałów, których powieści i opowiadania były często tworzywem literackim filmów: Raymond Chandler (Żegnaj laleczko, Wielki sen, Dama z jeziora), James Cain (Podwójne ubezpieczenie, Listonosz dzwoni zawsze dwa razy) i Dashiell Hammett (Sokół maltański).
Filmy tego nurtu realizowane były w latach 40. i w I połowie lat 50. Niektórzy krytycy uznający „czarny film” nie za nurt, lecz za gatunek filmu amerykańskiego zaliczają do niego także późniejsze filmy detektywistyczno-gangsterskie.
System wielkich wytwórni Hollywood zaczął się chwiać pod koniec lat 40. Złożyły się na to dwie przyczyny – rządowa – federalna akcja antytrustowa wymuszająca oddzielenie produkcji filmów od rozpowszechniania oraz gwałtowny rozwój telewizji po II wojnie światowej.
Lata 50. – epoka telewizji („srebrne lata Hollywood”).
Okres, kiedy masowo pojawiały się telewizory i jednocześnie pustoszały kina. Rozpoczęła się dramatyczna walka o widza, wytwórnie postanowiły wprowadzić do kin szeroki ekran.
W 1952 opracowano technikę cineramy (projekcja z trzech taśm).
W 1953 przypomniano francuski wynalazek sprzed lat CinemaScope (tzw. „panorama” z zastosowaniem obiektywu anamorficznego).
W 1954 opracowano format VistaVision z klatką umieszczoną poziomo na taśmie filmowej. Później zaczęto stosować taśmę 70 mm. Takich filmów nie można było obejrzeć w telewizji. W USA powstawały też nowe kina pod gołym niebem – kina samochodowe.
To również szczyt zimnej wojny pomiędzy światem zachodnim a Związkiem Radzieckim. W Hollywood działała kongresowa komisja do spraw działalności antyamerykańskiej tzw. komisja McCarthy’ego szukająca szpiegów sowieckich i sympatyków partii komunistycznej. Część filmowców straciła pracę, niektórzy jak Charlie Chaplin emigrują do Europy.
Reżyserzy, którzy zdobyli sławę w latach 40., tacy jak Billy Wilder, John Huston, Elia Kazan, Alfred Hitchcock czy John Ford dalej tworzyli dobre filmy. Pojawiały się także nowe twarze. Do grona wielkich gwiazd dołączyły m.in. Marilyn Monroe, Marlon Brando, Natalie Wood, James Dean, Burt Lancaster, Debbie Reynolds, Frank Sinatra, Kirk Douglas, Gene Kelly, Steve McQueen, Grace Kelly, Shirley MacLaine.
Lata 60. – czas dorastającego pokolenia powojennego („brązowe lata Hollywood”).
To okres, kiedy do kin zaczyna przychodzić młodzież wychowana po wojnie, mająca w domach telewizory, szukająca w kinach nie tylko rozrywki. To właśnie młodzi ludzie stanowią większość widowni kinowej. W filmie amerykańskim, podobnie jak to się dzieje w Europie, pojawia się kino autorskie – reżyserów takich jak Mike Nichols, Sidney Lumet, John Cassavetes, Blake Edwards, Stanley Kubrick, John Schlesinger, Paul Mazursky, Sam Peckinpah, Woody Allen, Dennis Hopper czy Roman Polański.
Lata 70..
To okres zdominowany przez nowe pokolenie filmowców, zwanych Movie Brats. Wielu krytyków jest zdania, że wówczas kręcono najlepsze filmy w dziejach amerykańskiego kina. Wysokobudżetowe produkcje przyciągały do kina zarówno młodzież, jak i starszych widzów. Pełne żywej akcji, a także o bardzo wysokich walorach artystycznych filmy trafiały do wszystkich krajów świata, a reklama zapewniała i wielki sukces kasowy. Najsłynniejszymi przedstawicielami grupy są Stanley Kubrick, Francis Ford Coppola, Martin Scorsese i dwaj emigranci z Europy Wschodniej – Miloš Forman i Roman Polański. Nową epokę wielkich kinowych hitów otworzył Ojciec chrzestny (The Godfather) Coppoli czy Chinatown Polańskiego.
Najważniejsze filmy Movie Brats.
W dalszej kolejności swoją pozycję ugruntowywali tacy reżyserzy jak George Lucas czy Steven Spielberg. Do największych sukcesów należał cykl Gwiezdnych wojen (Star Wars) Lucasa i trylogia o Indiana Jonesie Spielberga. Zastosowano w nich na szeroką skale efekty techniczne, ale były to wersje seriali filmowych z lat trzydziestych i czterdziestych. „Nie wymagały od widzów wysiłku intelektualnego, nie zawierały pogłębionego wizerunku bohaterów, bazując jedynie na spektakularnej scenerii. Sam Spielberg kwestionował jakość takich produkcji”.
Lata najnowsze.
Spowodowała, że światowa publiczność lepiej niż kiedykolwiek przedtem zna filmy i nazwiska i reżyserów takich jak Oliver Stone, Jonathan Demme, John Hughes, Ivan Reitman, Robert Zemeckis, James Cameron, Ridley Scott czy Quentin Tarantino.
W każdym okresie rozwoju filmu pojawiały się gwiazdy ekranu. W dzisiejszych czasach można wymienić takie nazwiska aktorów jak: Daniel Day-Lewis, Sean Penn, Clint Eastwood, Arnold Schwarzenegger, Sylvester Stallone, Harrison Ford, Mel Gibson, Robert De Niro, Al Pacino, Michael Douglas, Jack Nicholson, Gene Hackman, Dustin Hoffman, Morgan Freeman, Tom Hanks, Kevin Costner, Brad Pitt, Tom Cruise, Robert Downey Jr. Spośród aktorek: Julia Roberts, Sharon Stone, Demi Moore, Jodie Foster, Michelle Pfeiffer, Meryl Streep, Jessica Lange, Winona Ryder, Angelina Jolie czy Susan Sarandon.
Najważniejsze amerykańskie filmy lat 90..
Pod koniec XX wieku, wielkie wytwórnie filmowe w Stanach Zjednoczonych nastawione są głównie na produkcję ekranizacji, adaptacji, sequeli, prequeli i remake’ów. „W pogoni za sukcesem kasowym Hollywood woli zachować ostrożność, dostarczając widzom kolejne odcinki popularnych filmów, takich jak Batman czy Zabójcza broń (Lethal Weapon). Czerpie także z własnej przeszłości, produkując nowe wersje dawnych hitów kinowych, na przykład „Przylądek strachu (Cape Fear”) lub „Cud na 34. ulicy (Miracle on 34th Street)”. Powstają również adaptacje słynnych filmów nakręconych w innych krajach, takie jak „Trzech mężczyzn i dziecko (Three Man and a Baby)” czy „Sommersby”, a także coraz więcej filmów pełnometrażowych, tworzonych na podstawie popularnych seriali telewizyjnych, na przykład „Ścigany (The Fugitive)” i „Flintstonowie (The Flintstones)”.

</doc>
<doc id="1658" url="https://pl.wikipedia.org/wiki?curid=1658" title="Fruktany">
Fruktany

Fruktany – niskocząsteczkowe, często rozgałęzione polimery -fruktozy (β--fruktofuranozy). Większość fruktanów zawiera cząsteczkę glukozy zlokalizowaną terminalnie, na końcu redukującym. Są rozpuszczalne w wodzie.
Są związkami pochodzenia roślinnego – produkowane są jako materiał zapasowy, oraz mikrobiologicznego – jako egzopolisacharydy. Rzadko występują w liściach (cebula), przede wszystkim magazynowane są w bulwach, kłączach oraz dolnych częściach łodyg. Można je spotkać również w niedojrzałych owocach. Drobnoustroje produkują fruktany w obecności sacharozy w podłożu. Mikrobiologiczny fruktan to lewan, a jego synteza ujawnia się w postaci małych kropelek w pobliżu kolonii.
Występowanie.
Można je zaobserwować u szczepów:
Fruktany występują pospolicie w wakuolach roślin należących do rodzin:
Podział fruktanów.
Fruktany różnią się między sobą masą cząsteczkową i budową wewnętrzną, podstawę rozróżnienia stanowi rodzaj wiązań pomiędzy resztami fruktozy. Badania fruktanów izolowanych z liści spichrzowych cebuli ("Allium cepa") wykazały, że są one mieszaniną substancji składających się z 3–12 reszt monosacharydowych Najlepiej poznanym fruktanem roślinnym jest inulina, której masa cząsteczkowa wynosi ok. 5 kDa, co odpowiada 30–35 resztom cukrowym. Inulina jest białym proszkiem bez smaku, przypominającym wyglądem zewnętrznym skrobię, nie daje jednak zabarwienia z jodem, rozpuszcza się łatwo w gorącej wodzie, wypadając z roztworu w niskich temperaturach (ta właściwość ułatwia jej otrzymywanie), pod działaniem kwasów lub enzymu – inulinazy zostaje rozłożona całkowicie do fruktozy.
I grupa.
Należą tu związki, w których występuje wiązanie glikozydowe β-2,1:
II grupa.
Wiązanie między fruktozami to β-2,6 glikozydowe. Fruktany tej grupy to przede wszystkim polimery występujące w roślinach z rodziny "Gramineae" oraz lewany produkowane przez bakterie. Paciorkowce odpowiedzialne za próchnicę zębów – "Streptococcus salivarius" oraz "Streptococcus mutans" produkują zewnątrzkomórkowy enzym, który przekształca sacharozę w lewan, który następnie przyczepia się do powierzchni zębów. Na nim właśnie gromadzą się kwaśne produkty fermentacji mlekowej i tworzy się płytka nazębna. Niektóre szczepy wykorzystują produkowany lewan, gdy w podłożu braknie sacharozy.
Zastosowanie.
Fruktany są związkami, które wywierają korzystny wpływ na zdrowie człowieka – są prebiotykami – nie są trawione ani wchłaniane w przewodzie pokarmowym gospodarza, organizm człowieka nie posiada enzymów hydrolizujących wiązanie β-2-1 glikozydowe, natomiast ulegają selektywnej fermentacji bakteryjnej w jelicie grubym, stymulując wzrost pałeczek kwasu mlekowego – "Lactobacillus" oraz "Bifidobacterium". Mechanizm działania jest związany z selektywną fermentacją fruktanów przez bifidobakterie, które syntetyzują enzym rozkładający wiązania β-1,2 glikozydowe – beta fruktozydazę.
Wynikiem procesu rozkładu fruktanów jest zmiana składu mikroflory bakteryjnej jelit, polegająca na zahamowaniu wzrostu bakterii patogennych. Wzrost "złych bakterii" jest hamowany przez bakterie probiotyczne, które poprzez produkcję krótkołańcuchowych kwasów tłuszczowych obniżają pH środowiska i w ten sposób tworzą niekorzystne środowisko dla rozwoju patogenów tj: "Escherichia coli", "Salmonella", "Shigella", "Clostridium". Produkowane z fruktanów w jelicie krótkołańcuchowe kwasy tłuszczowe to przede wszystkim:
Kwasy te wpływaja na metabolizm ogólnoustrojowy:
Fruktany w organizmie gospodarza:
W celu uzyskania efektu prebiotycznego dzienna dawka fruktanów wynosi 4–10 g/dobę/osobę.

</doc>
<doc id="1659" url="https://pl.wikipedia.org/wiki?curid=1659" title="Fale radiowe">
Fale radiowe

Fale radiowe, promieniowanie radiowe – fale elektromagnetyczne o częstotliwości od 3 kHz do 3 THz (3·103–3·1012 Hz). Według literatury zachodniej zakres częstotliwości obejmuje fale od 3 Hz. Zależnie od długości dzielą się na pasma radiowe.
Źródła fal radiowych dzieli się na:
Ze względu na środowisko propagacji wyróżnia się:
Propagacja fali radiowej zależy od różnorodnych zjawisk falowych, na przykład dyfrakcji i interferencji, refrakcji, odbicia (w tym od jonosfery), a zjawiska te są uwarunkowane własnościami ośrodka w którym rozchodzi się fala, jego granicami, a także od długości fali radiowej.

</doc>
<doc id="1660" url="https://pl.wikipedia.org/wiki?curid=1660" title="Flavius Julius Constans">
Flavius Julius Constans



</doc>
<doc id="1661" url="https://pl.wikipedia.org/wiki?curid=1661" title="Flavius Gratianus">
Flavius Gratianus



</doc>
<doc id="1662" url="https://pl.wikipedia.org/wiki?curid=1662" title="Florianus">
Florianus



</doc>
<doc id="1663" url="https://pl.wikipedia.org/wiki?curid=1663" title="Franz Boas">
Franz Boas

Franz Uri Boas (ur. 9 lipca 1858 w Minden, zm. 21 grudnia 1942 w Nowym Jorku) – amerykański prekursor antropologii kulturowej i językoznawca niemieckiego pochodzenia, twórca historyzmu boasowskiego, nazywany „ojcem antropologii amerykańskiej”.
Studiował fizykę i geografię w Niemczech. W 1881 roku uzyskał stopień doktora fizyki. Wziął udział w wyprawie badawczej do północnej Kanady, gdzie zainteresował się kulturą oraz językiem Inuitów zamieszkujących Ziemię Baffina. Od tego czasu prowadził badania terenowe nad kulturami i językami ludów z rejonu północno-zachodniego wybrzeża Pacyfiku.
W 1887 roku wyemigrował do USA, gdzie pracował jako kurator muzealny w Instytucie Smithsona. W 1899 roku został profesorem antropologii na Uniwersytecie Columbia w Nowym Jorku. Pozostał tam aż do końca swojej kariery. Wielu z jego studentów stało się założycielami ośrodków antropologicznych w Stanach Zjednoczonych. Edukując młodych ludzi, miał ogromny wpływ na rozwój amerykańskiej antropologii. Do jego uczniów należą m.in. Manuel Gamio, Alfred Kroeber, Ruth Benedict, Edward Sapir, Margaret Mead i Zora Neale Hurston.
Był zagorzałym przeciwnikiem popularnej pod koniec XIX w. ideologii zwanej rasizmem naukowym ("scientific racism"), którego podstawą było założenie, że rasa człowieka jest uwarunkowana biologicznie, a zachowania ludzkie od niej zależne. W cyklu studiów nad szkieletem człowieka Boas udowodnił, że kształt i wielkość czaszki są w dużym stopniu zmienne, uzależnione od czynników otoczenia, np. zdrowia czy odżywienia organizmu. Zwolennicy teorii naukowego rasizmu uważali kształt głowy za niezmienną cechę rasową.
Uważał, że różnice w zachowaniu ludzkim nie są determinowane wrodzonymi czynnikami biologicznymi, ale są wynikiem różnic kulturowych, nabytych w ciągu wychowania w danym społeczeństwie. Boas uznawał kulturę za najważniejszy czynnik kierujący zachowaniem różnych grup ludzkich. Stała się ona zatem głównym przedmiotem jego analizy antropologicznej.
Był również przeciwnikiem popularnego wówczas ewolucjonizmu w naukach społecznych. Zwolennicy tej teorii zakładali, że wszystkie społeczeństwa przechodzą przez kolejne fazy rozwoju techniki i kultury, którego szczytem jest cywilizacja zachodnioeuropejska. Boas uważał, że kultura rozwija się poprzez interakcje z innymi grupami ludzkimi i rozprzestrzenianie się idei (dyfuzjonizm), a jednolity dla wszystkich form kulturowych proces osiągania coraz to wyższych stadiów rozwoju nie istnieje. Na mocy tego poglądu Boas zrezygnował z organizacji wystaw etnograficznych w oparciu o ewolucjonistyczne fazy rozwoju. Preferował wystawianie obiektów połączonych ze sobą poprzez związki z konkretnymi grupami.
Boas wprowadził również pojęcie relatywizmu kulturowego. Zakłada ono, że kultury nie mogą być obiektywnie oceniane jako lepsze lub gorsze, a wszyscy ludzie widzą świat przez własne, przyswojone normy kulturowe. Według niego, zadaniem antropologii było zrozumienie w jaki sposób kultura uwarunkowuje postrzeganie i rozumienie świata przez ludzi.
Łącząc archeologię (studia nad kulturą materialną), antropologię fizyczną (studia nad różnicami w ludzkiej anatomii) oraz etnologię (studia nad kulturowymi różnicami, językoznawstwo opisowe i badania nad językami lokalnymi), Boas uznał, iż antropologia powinna być połączeniem antropologii fizycznej, lingwistyki, archeologii i antropologii kulturowej. Te cztery pola badawcze stały się podwaliną amerykańskiej antropologii w XX w.
Życiorys.
Dzieciństwo i młodość.
Urodził się w Minden w Westfalii. Jego dziadkowie byli praktykującymi Żydami, natomiast rodzice przyjęli założenia haskali – ruchu intelektualnego opowiadającego się m.in. za integracją ze społecznościami nieżydowskimi. Byli wykształceni i liberalni, a Boas został wychowany w atmosferze sprzyjającej wolności przekonań. Już we wczesnym okresie życia wykazywał zainteresowanie przyrodą oraz naukami biologicznymi.
W nocie autobiograficznej "An Anthropologist’s Credo" Boas napisał:
"Podłożem mojego wczesnego myślenia był niemiecki dom, w którym żywe były ideały okresu Wiosny Ludów. Mój ojciec, liberał pozostawał bierny w sprawach publicznych; moja matka idealistka, żywo się nimi interesowała. Około 1854 była założycielką przedszkola w moim rodzinnym mieście, poświęcała się nauce. Moi rodzice zerwali z dogmatami. Ojciec zachował sentymentalne przywiązanie do obrzędowości swoich rodziców, ale nie pozwolił, by wpływała ona na jego wolność intelektualną"
Studia.
Studia rozpoczął na Uniwersytecie w Heidelbergu, gdzie spędził jednak tylko jeden semestr. Następne cztery lata studiował fizykę, geografię i matematykę na Uniwersytecie w Bonn. W 1879 roku chciał przenieść się na Uniwersytet w Berlinie, by studiować fizykę u Hermanna von Helmholtza, ale – z powodów rodzinnych – przeniósł się na Uniwersytet w Kilonii. Tam studiował u Theobalda Fischera i otrzymał doktorat z fizyki w 1881 roku, za pracę "Beiträge zur Erkenntnis der Farbe des Wassers" ("Przyczynek do zrozumienia koloru wody"), w której opisywał absorpcję, refleksję i polaryzację światła w wodzie morskiej. Chociaż Boas uzyskał doktorat z fizyki, jego mentor Fischer (uczeń Carla Rittera) był geografem. Z tego powodu niektórzy biografowie uważają Boasa za bardziej geografa niż fizyka.
Badania, które prowadził na potrzeby dysertacji doktorskiej miały na celu sprawdzenie, w jaki sposób różne częstotliwości światła tworzą odmienne kolory podczas interakcji z poszczególnymi typami wody. Podczas eksperymentów Boas miał trudność z obiektywnym rozróżnieniem niewielkich różnic kolorystycznych wody, w wyniku czego zaczął zastanawiać się nad percepcją i jej wpływem na wyniki badań. Dzięki kursowi estetyki u Kuno Fischera w Heidelbergu, Boas zainteresował się również filozofią Immanuela Kanta. To z kolei zaowocowało chęcią przeprowadzenia badań z dziedziny psychofizyki, które dotyczyłyby związku między tym, co psychiczne, a tym co fizyczne. Choć nie miał odpowiedniego wykształcenia z dziedziny psychologii, w okresie odbywania rocznej służby wojskowej opublikował sześć artykułów na temat psychofizyki (1882–1883). Ostatecznie jednak zdecydował skupić się na geografii, głównie po to, by uzyskać finansowanie dla planowanej wyprawy na Ziemię Baffina.
Początek badań.
Geografia stanowiła dla niego sposób odkrywania związków między subiektywnym doświadczeniem a światem obiektywnym. W tym czasie wśród geografów niemieckich panowała rozbieżność zdań na temat różnic kulturowych: wielu uważało, że środowisko fizyczne jest najważniejszym czynnikiem determinującym różnice kulturowe, inni (m.in. Friedrich Ratzel) zaś twierdzili, że rozprzestrzenianie idei poprzez migracje ludności jest ważniejsze. W 1883 roku, zachęcony przez Theobalda Fischera, Boas odbył podróż na Ziemię Baffina by przeprowadzić badania dotyczące wpływu środowiska na migracje tamtejszych ludów inuickich. Podczas tych pierwszych etnograficznych badań terenowych, Boas zebrał notatki, na podstawie których napisał swoją pierwszą monografię "The Central Eskimo". Została ona opublikowana w 6th Annual Report from Bureau of American Ethnology w 1888 roku.
Boas żył i pracował w bliskich relacjach z Inuitami zamieszkującymi Ziemię Baffina. Napisał w swoim dzienniku: "Często zadaję sobie pytanie o to, jakie zalety posiada nasze „dobre społeczeństwo”, które sprawiają, że jest lepsze niż „ludzie prymitywni”. Dochodzę do wniosku, im więcej ich zwyczajów poznaję, że nie mamy prawa nimi pogardzać. Nie mamy prawa oskarżać ich o formy zwyczajów, przesądy, które mogą się nam wydawać śmieszne. My „wysoko wykształceni ludzie” jesteśmy dużo gorsi, mówiąc ogólnie..." – Franz Boas do Marie Krackowizer, 23 grudnia 1883.
Zainteresowanie Boasa lokalnymi społecznościami wzrosło wraz z otrzymaniem pracy w Królewskim Muzeum Etnologicznym w Berlinie, gdzie poznał członków plemienia Nuxalk z Kolumbii Brytyjskiej. To zapoczątkowało jego dożywotni związek z Indianami kanadyjskimi zamieszkującymi rejon północno-zachodniego wybrzeża Pacyfiku.
Po wyprawie na Ziemię Baffina powrócił do Berlina, by dokończyć studia. W 1886 roku z pomocą Hermanna von Helmholtza obronił habilitację "Baffin-Land".
Podczas pobytu na Ziemi Baffina zaczął interesować się kulturami niezachodnimi, co zaowocowało pracą "The Central Eskimo" opublikowaną w 1888 roku. Wcześniej, w 1885 roku, Boas rozpoczął współpracę z antropologiem fizycznym Rudolfem Virchowem oraz etnologiem Adolfem Bastianem w Królewskim Muzeum Etnologicznym w Berlinie. Bastian znany był z negatywnego stosunku do teorii determinizmu środowiskowego. Był zwolennikiem teorii „fizycznej jedności ludzkości”, która głosiła, że wszyscy ludzie mają taki sam potencjał intelektualny, a wszystkie kultury oparte są na takiej samej podstawie mentalnej. Różnice w zwyczajach i wierzeniach wynikają z historii. Pogląd ten miał duży wpływ na Boasa, zwłaszcza w okresie jego pierwszego pobytu na Ziemi Baffina oraz skierował jego zainteresowania na antropologię.
Już w trakcie pracy w Królewskim Muzeum Etnologicznym w Berlinie, Boas zaczął interesować się Indianami kanadyjskimi z rejonu północno-zachodniego wybrzeża Pacyfiku. Po obronie pracy habilitacyjnej udał się przez Nowy Jork do Kolumbii Brytyjskiej na trzymiesięczny wyjazd. W styczniu 1887 roku otrzymał propozycję pracy jako asystent redaktora w interdyscyplinarnym czasopiśmie naukowym „Science”, publikowanym w Stanach Zjednoczonych. Rosnące w Niemczech antysemityzm, nacjonalizm oraz ograniczenia w karierze akademickiej sprawiły, że Boas pozostał w USA. Możliwe, że do tej decyzji przyczynił się również romans z Marie Krackowizer, którą poślubił w tym samym roku.
Wystawa światowa w 1893.
Antropolog Frederic Ward Putnam, dyrektor i kurator Peabody Museum przy Uniwersytecie Harvarda, wybrał Boasa na asystenta przygotowanej wystawy światowej w Chicago, która miała się odbyć w 1893 roku, w 400 rocznicę przybycia Krzysztofa Kolumba do Ameryki. Dało to Boasowi okazję do praktycznego zastosowania jego koncepcji wystawienniczych. Zarządzał grupą około 100 asystentów, których zadaniem była organizacja kilku ekspozycji etnograficznych o Indianach Ameryki Północnej i Południowej. Putnam chciał, aby wystawa była upamiętnieniem podróży Kolumba. Uważał, że pokazanie plemion Inuitów i Indian kanadyjskich z końca XIX wieku, w „ich naturalnych warunkach życia” będzie stanowiło doskonały kontrast i ukaże cztery wieki osiągnięć Zachodu (od 1493 roku).
Boas wyruszył w podróż na północ Kanady, by zdobyć materiały etnograficzne na wystawę. Zorganizował przyjazd 14 ludzi z plemienia Kwakiutl z Kolumbii Brytyjskiej do Chicago. Zamieszkali oni w specjalnie zbudowanej tam wiosce, dzięki czemu przedstawiane przez nich życie codzienne miało nabrać właściwego sobie kontekstu. Boas pragnął, by zwiedzający mogli wynieść z wystawy konkretną wiedzę.
Po wystawie, zebrane materiały etnograficzne posłużyły jako podstawy kolekcji nowo utworzonego Muzeum Historii Naturalnej w Chicago, w którym Boas został kuratorem sekcji antropologii. Pracował tam do 1894 roku, kiedy zastąpił go archeolog William Henry Holmes.
Praca w muzeum.
W 1896 roku został asystentem kuratora sekcji etnologii w Amerykańskim Muzeum Historii Naturalnej w Nowym Jorku. W 1897 roku zorganizował wyprawę w rejon północnego Pacyfiku. Były to pięcioletnie badania terenowe wśród Indian kanadyjskich. Boas starał się tworzyć wystawy połączone bardziej kontekstem, niż modnym wówczas pojęciem ewolucji. Opracował również program badawczy współgrający z jego celami kuratorskimi. Spisując instrukcje dla swoich studentów, dotyczące poszerzania kontekstów interpretacyjnych w społeczeństwie tłumaczył, że ludzie dostają okazy oraz ich wyjaśnienia, które nie tylko odnoszą się do okazów, lecz także do spraw abstrakcyjnych dotyczących człowieka. Koncepcja poszerzonych kontekstów interpretacji skupiała się na jednym aspekcie, w którym okazy czy ich grupy byłyby wystawione. Boas pragnął kolekcji odnoszących się do konkretnych plemion, by pokazać unikalny styl każdej z grup. Jego podejście doprowadziło do konfliktu z dyrektorami muzeum Morrisem Jesupem oraz Hermonem Bumpusem. Do 1900 roku, Boas zaczął się wycofywać z pomysłu wykorzystania amerykańskiej antropologii muzealnej jako narzędzia edukacji. Całkowicie zrezygnował z niego w 1905 roku i do końca życia nie pracował już w żadnym muzeum.
Niektórzy uczeni, jak student Boasa. Alfred Kroeber, uważali, że używał on w swoich badaniach fizyki jako modelu dla antropologii. Jednak wielu innych, w tym Alexander Lesser, czy Matti Bunzl zwracali uwagę na to, że sam Boas jednoznacznie odrzucał fizykę na rzecz historii jako modelu dla badań antropologicznych.
Koncepcje wystawiennicze i początki teorii.
Pod koniec XIX wieku, antropologia w USA była zdominowana była przez Bureau of American Ethnology. Jego dyrektorem był John Wesley Powell – geolog i zwolennik teorii ewolucji kulturowej Lewisa H. Morgana. BAE miało siedzibę w Instytucie Smithsona w Waszyngtonie, a kurator do spraw etnologii z Instytutu Smithsona, Otis T. Mason miał takie same poglądy na ewolucję kulturową jak Powell.
Podczas pracy nad kolekcją muzealną i wystawami Boas sformułował podstawy swojego podejścia do kultury, które doprowadziły go do porzucenia muzeów oraz próby stworzenia antropologii jako naukowej dyscypliny akademickiej.
Podczas tego okresu wyruszył na 5 kolejnych wypraw do ludów północno-zachodniego Pacyfiku. Prowadzone przez niego badania terenowe sprawiły, że uważał kulturę za lokalny kontekst działań ludzkich. Duży nacisk kładł na pojęcie lokalności i sprzeciwiał się popularnej w tamtym okresie teorii ewolucji kulturowej.
Ostatecznie zerwał z teorią ewolucji kulturowej w swojej analizie pokrewieństwa. Ewolucjonista Lewis Morgan uważał, że wszystkie społeczności ludzkie przechodzą z podstawowej formy organizacji matrylinearnej do patrylinearnej. Indianie kanadyjscy z północnego wybrzeża Kolumbii Brytyjskiej, na przykład Tsimshian i Tlingit byli zorganizowani w klanach matrylinearnych. Indianie kanadyjscy z południowego wybrzeża, na przykład Nootka czy Saliszowie byli zorganizowani patrylinearnie. Boas skupił się na Kwakiutlach żyjących pomiędzy tymi dwoma grupami. W plemieniu Kwakiutlów można było spotkać mieszankę dwóch lineaży. Na początku, Boas, podobnie jak Morgan, uważał, że Kwakiutlowie używają struktury matrylinearnej, tak jak ich sąsiedzi z północy, lecz powoli ewoluują w grupy patrylinearne. Jednak w 1897 roku zmienił podejście i uważał, że Kwakiutlowie zmieniają pierwotną organizację patrylinearną na matrylinearną, w miarę poznawania jej zasad.
Odrzucenie teorii Morgana doprowadziło go do napisania w 1887 roku artykułu rzucającego wyzwanie dotychczasowym zasadom wystaw muzealnych. Stawką były sprawy bardziej jeszcze podstawowe niż klasyfikacja obiektów muzealnych i organizacja wystaw. Podejście ewolucjonistyczne do kultury materialnej doprowadziło kuratorów muzealnych do układania obiektów na wystawach według pełnionej przez nie funkcji lub stopnia rozwoju technologicznego. Kuratorzy zakładali, że zmiany z formach obiektów odzwierciedlają naturalne procesy ewolucji. Jednak Boas uważał, że forma obiektu odzwierciedla sytuację, w której był wyprodukowany i używany. Twierdził on, że nawet te obiekty, które mają podobne formy mogły powstać w zupełnie różnym kontekście kulturowym i z innych powodów. Wystawy muzealne zorganizowane zgodnie teorią ewolucjonizmu, błędnie zestawiają ze sobą efekty zamiast przyczyny działań.
Minic Wallace.
Podczas pracy na stanowisku asystenta kuratora w Amerykańskim Muzeum Historii Naturalnej w Nowym Jorku, Franz Boas nakłonił Roberta E. Peary’ego badacza Arktyki by przywiózł przedstawiciela tubylców z Grenlandii do Nowego Jorku. Ten przywiózł w 1897 roku sześciu Inuitów, którzy zamieszkali w piwnicy Amerykańskiego Muzeum Historii Naturalnej. Czterech z nich zmarło na gruźlicę w przeciągu roku od przyjazdu. Przeżył tylko chłopiec zwany Minik Wallace. Boas zaaranżował pogrzeb jego ojca, ale zamiast pochować ciało poddał je sekcji zwłok i wystawił w muzeum. Kiedy Minik Wallace dowiedział się, że kości jego ojca są częścią muzealnej wystawy, zażądał ich zwrotu. W tym czasie Boas nie był już pracownikiem muzeum, ale instytucja nie chciała zwrócić kości. W końcu Minik wrócił na Grenlandię, a Boas w żaden sposób mu nie pomógł. Po tym incydencie Boas był ostro krytykowany za sprowadzenie Inuitów do muzeum i brak zainteresowania ich dalszym losem.
Działalność dydaktyczna i organizacyjna.
Franz Boas zmarł na zawał w Columbia University Faculty Club 21 grudnia 1942 roku, przebywając w towarzystwie Claude’a Levi-Straussa.
Boas był jednym z najbardziej wpływowych i szanowanych naukowców swojej generacji. W latach 1901–1911 na Uniwersytecie Columbia był promotorem 7 prac doktorskich z dziedziny antropologii, co w ówczesnych warunkach wystarczało do uczynienia Instytutu Antropologii na Uniwersytecie Columbia wiodącym ośrodkiem antropologicznym w kraju. Wielu z jego studentów stało się założycielami ośrodków studiów antropologicznych na najbardziej znaczących uczelniach kraju.
Pierwszym doktorantem Boasa był Alfred L. Kroeber (1901), który razem z Robertem Lowie otworzył instytut antropologiczny na Uniwersytecie California w Berkeley. Uczniem Boasa był również William Jones, jeden z pierwszych antropologów o pochodzeniu Indiańskim, z plemienia Fox, który został zabity podczas prowadzenia badań na Filipinach w 1909 roku.
Boas kształcił również wielu innych studentów, którzy przyczynili się do rozwoju antropologii jako dziedziny akademickiej: Frank Speck (1908) – uzyskał stopień doktora na Uniwersytecie Pennsylvania w Filadelfii i założył tam instytut antropologii; Edward Sapir (1909) oraz Fay Cooper Cole (1914), który rozwinął ośrodek studiów antropologicznych na Uniwersytecie Chicago; Alexander Goldenweiser (1910), który wspólnie z Elsie Clews Parsons założył ośrodek antropologiczny w New School for Social Research w Nowym Jorku; Leslie Spier (1920), który razem z żoną Erną Gunter (również studentką Boasa) założył ośrodek studiów antropologicznych na Uniwersytecie w Waszyngtonie; Melville Herskovits (1923), który założył ośrodek antropologiczny na Uniwersytecie Northwestern w Evanston. Uczniami Boasa byli również John R. Swanton, Paul Radin, Ruth Benedict, Alexander Lesser, Margaret Mead, Gene Weltfish, E. Adamson Hoebel, Jules Henry oraz Ashley Montagu.
Wśród jego studentów znalazł się meksykański antropolog Manuel Gamio, który obronił pracę magisterską w latach 1909–1911 i został założycielem Mexico’s Bureau of Anthropology w 1917 roku. Clark Weisler, który obronił doktorat z psychologii na Uniwersytecie Columbia w 1901 roku, kontynuował następnie studia antropologiczne z Boasem, potem zaś zajął się badaniem Indian amerykańskich. Esther Schiff, potem Goldfrank, która pracowała latem 1920, 1921 i 1922 roku prowadząc badania wśród Indian w Nowym Meksyku. Gilberto Freyre, który dał podstawy demokracji rasowej w Brazylii. Viola Garfield, która kontynuowała pracę Boasa nad plemionami Tsimshian. Frederica de Laguna, która zajmowała się plemionami Tlingit oraz Inuitami. Antropolożka, folklorystka i pisarka Zora Neale Hurston, która była absolwentką Barnard College w Nowym Jorku, żeńskiej szkoły związanej z Uniwersytetem Columbia.
Boas i jego studenci mieli również duży wpływ na Claude’a Levi-Straussa, który spotykał się z nimi podczas swojego pobytu w Nowym Jorku latach 40. XX wieku. Wielu ze studentów Boasa podzielało jego zainteresowanie ostrożną, historyczną rekonstrukcją oraz jego sprzeciw wobec spekulatywnych, ewolucyjnych modeli. Co więcej, Boas zachęcał swoich studentów, by biorąc przykład z niego samego, krytykowali siebie tak samo i często jak innych.
Poglądy i wpływ na naukę.
Antropologia akademicka.
W 1896 roku Boas został wykładowcą antropologii fizycznej na Uniwersytecie Columbia w Nowym Jorku. Na profesora awansował w 1899 roku. Kiedy Boas porzucił pracę w muzeum wynegocjował z władzami uczelni skupienie profesorów antropologii w jednym ośrodku, którego został dyrektorem. Akademicki program Boasa na Uniwersytecie Columbia był pierwszym programem studiów doktoranckich z zakresu antropologii w USA.
W tym czasie Boas odgrywał kluczową rolę w organizowaniu American Anthropological Association (AAA), organizacji mającej pełnić pieczę nad rodzącą się dyscypliną akademicką. Z początku Boas chciał, aby AAA zrzeszało jedynie profesjonalnych antropologów, ale William John McGee uważał, że stowarzyszenie powinno mieć charakter otwarty. Jego argumenty przekonały większość członków i w 1902 roku został on wybrany na przewodniczącego organizacji. Wiceprzewodniczącymi zostali Franz Boas, Frederic Ward Putnam, John Wesley Powell oraz William Henry Holmes.
W AAA oraz na Uniwersytecie Columbia Boas wprowadzał swoje podejście „czterech pól badawczych” (ang. "4 field approach"). Miał duży wkład w antropologię fizyczną, lingwistykę, archeologię oraz antropologię kulturową. Jego prace w tych dziedzinach były pionierskie: w antropologii fizycznej odwodził uczonych od klasycznej klasyfikacji rasowej, a kładł nacisk na biologię i ewolucję człowieka. W lingwistyce przekraczał granice klasycznej filologii i ustalił jedne z najważniejszych zagadnień współczesnej lingwistyki oraz antropologii kognitywnej. W antropologii kulturowej był autorem kontekstualnego podejścia do kultury, relatywizmu kulturowego oraz metody obserwacji uczestniczącej.
Podejście "4 field" widziano jako przewartościowanie antropologii poprzez integrację różnych przedmiotów badań w jedną dyscyplinę. Koncepcja ta była największą zasługą Boasa oraz stała się najbardziej charakterystyczną cechą amerykańskiej antropologii.
W eseju "Anthropology" (1907) Boas sformułował dwa podstawowe pytania:
Przedmiot badań antropologii wyjaśniał w następujący sposób: "Nie omawiamy anatomicznej, fizjologicznej i mentalnej charakterystyki człowieka jako jednostki, ale interesuje nas różnorodność cech u różnych grup żyjących w różnych zakątkach świata, w różnych klasach społecznych. Naszym zadaniem jest zastanawiać się nad przyczynami widocznych różnic oraz badać kolejność zdarzeń, które doprowadziły do powstania różnorodnych form ludzkiego życia. Mówiąc inaczej, interesuje nas anatomiczna i mentalna charakterystyka ludzi żyjących w tym samym środowisku biologicznym, geograficznym i społecznym"
Koncepcje Boasa wiązały się z odchodzeniem od popularnego na początku XX w. podejścia do zróżnicowania kulturowego, w którym zakładano, iż społeczności niepiśmienne lub którym brak źródeł pisanych, nie mają historii. Dla niektórych badaczy rozróżnienie między społeczeństwami piśmiennymi i nie, wyjaśniało jednocześnie różnicę między historią, socjologią, ekonomią a antropologią mającą się skupiać na ludach niepiśmiennych. Boas odrzucał taki podział zarówno społeczeństw, jak i dyscyplin akademickich. Uważał, że wszystkie społeczeństwa mają swoją historię i wszystkie mogą być obiektami badań antropologicznych. Aby umożliwić pracę w społeczeństwach zarówno piśmiennych, jak i niepiśmiennych podkreślał wagę studiowania historii ludzkości poprzez analizę nie tylko tekstów pisanych. W artykule z 1904 roku „The History of Anthropology” pisał: "historyczny rozwój pracy antropologów wydaje się jasno wyodrębniać dziedzinę wiedzy, którą nie zajmowała się dotąd żadna inna nauka. To jest biologiczna historia ludzkości w całej swej różnorodności; lingwistyka stosowana do ludów bez języków pisanych; etnologia ludzi bez źródeł historycznych oraz prehistoryczna archeologia"
Jedna z najważniejszych prac Boasa, "Umysł człowieka pierwotnego" (ang. "The Mind of Primitive Man") z 1911 roku, połączyła teorie historyczne i rozwoju kulturowego oraz wprowadziła sposób myślenia, który zdominowało amerykańską antropologię na następne 15 lat. W publikacji tej Boas udowadniał, że w każdej społeczności biologia, język, kultura symboliczna oraz materialna są całkowicie odrębne, a każdy z tych aspektów jest równie ważny dla natury człowieka. Mówiąc inaczej, Boas uważał, że kultura nie jest determinowana czynnikami zewnętrznymi. Podkreślał, że cechy biologiczne, lingwistyczne oraz kulturowe dowolnej grupy społecznej są wynikiem rozwoju historycznego, na który składają się zarówno czynniki kulturowe, jak i pozakulturowe. Uważał, że różnorodność kulturowa jest podstawową cechą ludzkości, a konkretne środowisko kulturowe kształtuje zachowanie jednostki.
Boas widział siebie jako przykład do naśladowania, obywatela-naukowca, który rozumiał, że każda wiedza, nawet ta najbardziej „prawdziwa”, ma swoje konsekwencje moralne. "Umysł człowieka pierwotnego" zakończony jest odezwą: "Nasze osądy staną się wolne tylko wtedy, gdy nauczymy się oceniać jednostki na podstawie ich własnych zdolności i charakteru. Wtedy odkryjemy, że gdybyśmy mieli wybrać najlepszych reprezentantów ludzkości, to znaleźliby się tam przedstawiciele wszystkich ras i narodowości. Wtedy też zaczniemy doceniać i kultywować różnorodność spotykanych form ludzkiej myśli i działalności, brzydząc się wszelkimi próbami narzucenia całemu narodowi lub wręcz całemu światu, jednego wzoru myślowego, widząc w tym coś, co prowadzi do całkowitej stagnacji"
Antropologia fizyczna.
Prace Boasa z zakresu antropologii fizycznej łączyły jego zainteresowanie teorią ewolucji Darwina oraz migracjami jako przyczynami zmian. Jego najważniejsze badania na tym polu skupiały się na zmianach w parametrach ciała wśród dzieci imigrantów w Nowym Jorku. Już wcześniej inni naukowcy zaobserwowali różnice we wzroście, wymiarach szkieletu oraz innych cechach fizycznych między Amerykanami a ludźmi z różnych części Europy. Wielu wykorzystywało te różnice do poparcia tezy, że istnieją wrodzone różnice biologiczne między rasami. Głównym zainteresowaniem Boasa, w obrębie kultury symbolicznej i materialnej oraz języka, były studia nad procesami zmiany. Dlatego też próbował ustalić, czy formy cielesne również są im poddane. Boas przeprowadził badania na 17 821 osobach, które pozdzielił na 7 grup według kryteriów etnicznych. Odkrył, że przeciętne wymiary kręgosłupa imigrantów znacznie różniły się od wymiarów osób urodzonych w Stanach Zjednoczonych oraz że średnie wymiary kręgosłupa dzieci urodzonych w przeciągu 10 lat od przybycia ich matek do USA znacznie się różniły od właściwych dzieciom urodzonym po upływie więcej niż 10 lat od przybycia ich matek do USA.
Nie zaprzeczał, że cechy fizyczne, takie jak wzrost czy rozmiary kręgosłupa są dziedziczne, ale uważał, że środowisko również ma na nie wpływ, co staje się widoczne z czasem. Badania te stały się jego głównym argumentem za poglądem, że różnice między rasami nie są niezmienne.
Debata nad wynikami jego badań trwa do dzisiaj. W 2002 roku antropologowie Corey Sparks i Richard Jantz stwierdzili, że różnice między rodzeństwem urodzonym w Europie i Ameryce są bardzo małe, wręcz nieznaczące. Widzieli również brak widocznych efektów wpływu środowiska amerykańskiego na rozmiar szkieletu dzieci. Uważali, że ich wyniki są zaprzeczeniem odkryć Boasa i pokazują, że nie można ich dłużej używać w dyskusji o morfologicznej plastyczności szkieletu. Wartość tez Boasa podtrzymuje Jonathan Marks. W 2003 roku antropologowie Clarence C. Grevlee, H. Russell Bernard oraz William R. Leonard przeanalizowali dane Boasa i potwierdzili, że większość z jego wniosków była prawdziwa. Użyli oni nowej metody komputerowej analizy statystycznej danych zebranych przez Boasa i odkryli jeszcze więcej dowodów na plastyczność kręgosłupa. W późniejszej publikacji Grevlee, Bernard i Leonard ocenili analizę Sparksa i Jantza. Uważali, że Sparks i Jantz źle zinterpretowali wnioski Boasa, a ich wyniki właściwie potwierdzają jego tezę. Na przykład pokazują oni, że Sparks i Jantz badają zmiany w kształcie kręgosłupa w odniesieniu do długości pobytu badanego w USA. Boas natomiast badał zmiany w wymiarach kręgosłupa w odniesieniu do długości pobytu matki badanego w USA. Grevlee, Bernard i Leonard uważali, że metoda Boasa była bardziej praktyczna, ponieważ środowisko prenatalne to kluczowy czynnik rozwoju jednostki.
Dalsze publikacje Jantza, opierały się na teoriach Gravlee’go, według których Boas specjalnie wybrał dwie grupy imigrantów (sycylijskich i hebrajskich), którzy najbardziej różnili się w tych samych aspektach, a odrzucił inne grupy, które różniły się w innych. Skomentował to następująś
co: "Używając najnowszej analizy Gravelee’go (2003) możemy zaobserwować, że w aspekcie drugim maksymalna różnica w wymiarach szkieletu spowodowana imigracją (u Hebrajczyków) jest dużo mniejsza niż największa różnica etniczna między Sycylijczykami i Czechami. To pokazuje, że mądrzy rodzice mogą spłodzić mądre potomstwo i na odwrót. Aby udowodnić, że dzieci imigrantów stają się „typu amerykańskiego” Boas musiał użyć dwóch najbardziej zmieniających się grup”".
Niektórzy socjobiolodzy i zwolennicy psychologii ewolucyjnej sugerowali, że Boas był przeciwnikiem teorii ewolucji Darwina. W rzeczywistości był jednak zagorzałym jej zwolennikiem. W 1888 roku zadeklarował, że rozwój etnologii jest w dużym stopniu skutkiem generalnego rozpowszechnienia teorii biologicznej ewolucji. Od czasów Boasa antropolodzy fizyczni zakładają, że ludzka zdolność tworzenia kultury jest wynikiem ludzkiej ewolucji. Badania Boasa nad zmianami form ciała odegrały istotną rolę w popularyzacji teorii Darwina. Należy jednak pamiętać, że Boas kształcił się w czasach, kiedy biolodzy nie posiadali jeszcze wiedzy z zakresu genetyki. Odkrycie Mendla stało się powszechnie znane dopiero w 1900 roku. Przed nim, biolodzy polegali na mierzeniu cech fizycznych jako danych empirycznych popierających teorię ewolucji. Badania biometryczne Boasa, doprowadziły go do kwestionowania użytkowania tej metody i uzyskanych dzięki niej danych.
Lingwistyka.
Boas miał również duży wkład w podstawy lingwistyki jako dziedziny nauki w Stanach Zjednoczonych. Opublikował wiele artykułów o językach autochtonicznych Ameryki i opisywał trudności teoretyczne z klasyfikacją języków. Był również autorem programu studiów nad związkami języka z kulturą, którego rozwój kontynuowali później jego studenci, m.in. Edward Sapir, Paul Rivet, Alfred Kroeber.
Artykuł Boasa z 1889 roku "On alternating sounds" ("O zmienności dźwięków") miał wpływ a metodologię tak lingwistyki, jak i antropologii kulturowej. Był on odpowiedzią na pracę Daniela Garrisona Brintona z 1888 roku, który w tym czasie był profesorem językoznawstwa oraz archeologii na Uniwersytecie Pennsylvania w Filadelfii. Brinton zaobserwował, że w wielu językach Indian niektóre głoski ulegały regularnym wymianom. Z pewnością przyczyną nie była wymowa indywidualna. Brinton nie sugerował, że niektórzy wymawiali poszczególne słowa inaczej niż inni, ale uważał, że istniało wiele słów które, nawet powtarzane przez tę samą osobę, różniły się od siebie. Używając teorii ewolucji Brinton przekonywał, że ta niespójność była znakiem niższości badanych języków i dowodem na to, że Indianie amerykańscy byli na niskim szczeblu rozwoju ewolucyjnego.
Boas dobrze znał argumenty Brintona, sam doświadczył czegoś podobnego podczas swoich badań na Ziemi Baffina oraz wśród ludów z regionu północno-zachodniego wybrzeża Pacyfiku. Mimo to twierdził, że „zmienne dźwięki” nie są znakiem szczególnym charakterystycznym dla języków amerykańskich Indian, a nawet, że zmienne dźwięki w ogóle nie istnieją. Uważał raczej, że są one obiektywnym dowodem na różne stadia rozwoju ewolucji kulturowej. Boas badał je pod kątem swojego długotrwałego zainteresowania subiektywnym postrzeganiem obiektywnych zjawisk fizycznych. Nawiązywał tu również do wcześniejszej krytyki ewolucjonistycznych wystaw muzealnych. Wyszczególniał w niej, że dwie rzeczy (obiekty kultury materialnej), które wydają się podobne, mogą się różnić. W tym artykule zakłada również możliwość, że dwie rzeczy (dźwięki), które wydają się różne, mogą być podobne.
Boas skupił swoją uwagę na percepcji dźwięków. Rozpoczął od postawienia pytania empirycznego: kiedy ludzie opisują jeden dźwięk na różne sposoby to czy przyczyną tego jest fakt, że nie postrzegają oni różnicy? Od razu również założył, że nie zajmuje się przypadkami związanymi ze stałym zaburzeniem – dźwiękowym odpowiednikiem daltonizmu. Wyróżnia, że owo pytanie o ludzi opisujących jeden dźwięk na różne sposoby jest porównywalne do tego, o ludzi opisujących różne dźwięki w jeden sposób. Jest to podstawą badań lingwistyki opisowej: podczas nauki nowego języka, jak należy zapisać wymowę poszczególnych słów? (Boas buduje tutaj podstawy dla rozróżnienia fonetyki i fonologii). Ludzie mogą wymawiać słowo na wiele sposobów, ale nadal rozpoznawać, że używają tego samego słowa. Problemem zatem nie jest to, że takie wrażenia nie są rozpoznawane jako jednostkowe (to znaczy, że ludzie rozpoznają różnice w wymowie), ale raczej, że dźwięki są klasyfikowane według swojego podobieństwa (czyli, ludzie klasyfikują wszystkie dźwięki w konkretne kategorie). Podobnie mogłoby być w przypadku słów lub kolorów. Angielskie słowo "green" ("zielony") może być używane w odniesieniu do całej gamy odcieni oraz barw. Ale istnieją języki, w których nie ma słowa określającego „zielony”. W takich przypadkach, ludzie mogą zaklasyfikować to, co użytkownicy języka angielskiego nazywają „zielonym” jako „żółte” lub „niebieskie”. Nie jest to przykład daltonizmu, ludzie mogą bowiem dostrzegać różnice w kolorach, ale klasyfikować podobne kolory inaczej niż użytkownicy języka angielskiego.
Boas stosował te pojęcia w swoich studiach nad językami Inuitów. Uczeni stosowali wiele wariantów zapisu danego słowa. W przeszłości,
dane takie interpretowano na różne sposoby – mogły oznaczać lokalne różnice w wymowie słowa lub różne dialekty. Boas uważał, że istniało alternatywne wyjaśnienie: różnica nie leży w tym jak Inuici wymawiają dane słowo, ale raczej w tym, jak anglojęzyczni uczeni są fizycznie niezdolni do przyswojenia rzeczonego dźwięku, czy też system fonetyczny języka angielskiego nie może dostosować tego dźwięku.
Boas miał swój wkład w metody lingwistyki opisowej, uważał, że stronniczość obserwatora nie musi być osobista, może być kulturowa. Kategorie pojęciowe zachodnich badaczy mogą powodować, że błędnie lub w ogóle nie odbierają oni znaczących elementów innej kultury. Tak jak w swojej krytyce wystaw muzealnych Otisa Masona, Boas pokazuje, że to, co wydawało się dowodem na ewolucję kulturową, było tak naprawdę konsekwencją wykorzystywania nienaukowych metod i odzwierciedleniem przekonań ludzi z zachodu o ich własnej wyższości kulturowej. Ten argument stanowi metodologiczną podstawę dla relatywizmu kulturowego Boasa, który zakłada, że poszczególne elementy kultury nabierają znaczenia dopiero w kontekście danej kultury. W innej, mogą nabrać innego znaczenia lub zostać go zupełnie pozbawione.
Antropologia kulturowa.
Podstawy podejścia Boasa do etnografii zawiera jeden z jego pierwszych esejów "The Study of Geography" ("Studia geograficzne"). Sądził, że należy uważać każde zjawisko za warte zbadania. Już sam fakt, że istnieje daje mu prawo by przyciągnąć uwagę, a wiedza o jego istnieniu i ewolucji w czasie oraz przestrzeni w pełni satysfakcjonuje badacza.
Takie podejście doprowadziło Boasa do promowania antropologii kulturowej, charakteryzującej się mocnym przywiązaniem do:
Boas uważał, że aby zrozumieć, konkretne cechy kultury (zachowania, wierzenia, symbole) należy badać je w kontekście lokalnym. Uważał również, że podczas migracji ludzi z jednego miejsca do innego, a także wskutek zmian wynikających z upływu czasu, elementy kultury oraz ich znaczenia ulegają przekształceniom. Doprowadziło go to do podkreślania wagi lokalnej historii w analizie kultury.
Boas zwracał uwagę na historię, która ukazuje jak pewne cechy rozprzestrzeniały się z jednego miejsca do innych. To sprawiło, że Boas uważał granice kulturowe za niejednoznaczne, przenikające się wzajemnie i wysoce przepuszczalne. Jego student, Robert Lowie opisał kulturę jako „zbiór strzępków i łat”.
Nawet obecnie, wielu zachodnich badaczy widzi podstawową różnicę między współczesnymi społeczeństwami, które charakteryzują się dynamizmem oraz indywidualizmem a społeczeństwami tradycyjnymi, stabilnymi i homogenicznymi (jednorodnymi). Empiryczne badania terenowe Boasa sprawiły, że stał się on przeciwnikiem tego rozróżnienia. Esej z 1903 roku "Decorative Designs of Alaskan Needlecases. A History of Conventional Designs, Based on Materials in a U.S. Museum" jest kolejnym przykładem teorii Boasa, które bazowały na danych empirycznych. Po wyróżnieniu podobieństw między igielnikami Boas pokazuje, że pewne cechy stanowią kanon, na podstawie którego kolejni artyści mogli tworzyć wariacje. Nacisk kładziony na kulturę jako kontekst nadający znaczenie zachowaniu sprawił, że Boas był wrażliwy na różnice w społeczeństwie.
W eseju programowym z 1920 roku "The Methods of Ethnology" ("Metody etnologii") Boas pisał, że zamiast „systematycznej numeracji standaryzowanych wierzeń i zwyczajów plemienia”, antropologia powinna dokumentować „sposób, w jaki jednostka reaguje na swoje otoczenie oraz modele zachowań w społeczeństwach prymitywnych”
Boas uważał, że dzięki skupieniu na jednostce można odkryć, że „zachowania jednostki są w dużym stopniu zdeterminowane przez jej otoczenie, ale również jej zachowania mają wpływ na społeczeństwo, w którym żyje i mogą wprowadzać modyfikacje w przyjętych formach”. Boas widział kulturę jako dynamiczną w swej istocie.
Uważał on, że społeczeństwa piśmienne i niepiśmienne powinny być analizowane w ten sam sposób. Historycy z XIX wieku stosowali techniki filologiczne by odtworzyć historię oraz związki historyczne między społeczeństwami piśmiennymi. Zdaniem Boasa, by używać tych metod w społeczeństwach niepiśmiennych badacze powinni samodzielnie zbierać i tworzyć teksty na temat badanych grup. Teksty te powinny przybierać nie tylko formy słowników czy gramatyk lokalnych języków, ale również nagrań przekazów mitów, opowieści ludowych, wierzeń, przepisów lokalnej kuchni. Aby to zrobić, Boas polegał na współpracy z piśmiennymi etnografami wywodzącymi się z lokalnych społeczności (np. w społeczności Kwakiutlów na Georgu Huncie) i wymagał od swoich studentów by widzieli w takich ludziach cennych partnerów – co prawda gorszych pod względem usytuowania w społeczeństwie zachodnim, ale lepszych w rozumieniu ich własnej kultury.
Posiłkując się takimi metodami, Boas opublikował w 1920 roku kolejny artykuł. Powrócił w nim do kwestii pokrewieństwa u Kwakiutlów. Pod koniec lat 90. XIX wieku, Boas próbował zrekonstruować transformację w organizacji klanów przez porównywanie jej do organizacji klanów u plemion sąsiadujących z Kwakiutlami od północy i południa. Tym razem jednak opowiedział się przeciwko tłumaczeniu zasad pokrewieństwa grup w Kwakiutlów jakimikolwiek angielskimi słowami. Zamiast prób wpasowania tej grupy w konkretny model, Boas pragnął zrozumieć jej wierzenia oraz działania na jej własnych zasadach. Na przykład wcześniej Boas tłumaczył słowo "numaym" jako „klan”. Teraz jednak zauważył, że słowo to oddaje związek przywilejów, który nie może być oddany żadnym angielskim słowem. Tak jak w swojej pracy nad "On alternating sounds", Boas zdał sobie sprawę, że różne interpretacje etnologiczne zasad pokrewieństwa Kwakiutlów były wynikiem ograniczeń narzucanych przez zachodnie kategorie pojęciowe. Jak w swojej pracy o igielnikach z Alaski dostrzegał, że różnice w zwyczajach Kwakiutlów są wynikiem związku między normami społecznymi a kreatywnością jednostki. Przed śmiercią w 1942 roku, Boas wyznaczył Helen Codere, aby zredagowała i opublikowała jego manuskrypty o kulturze ludu Kwakiutlów.
Folklorystyka.
Działalność Franza Boasa miała wpływ także na rozwój folklorystyki jako dyscypliny naukowej. Boas uważał, że powinna ona stać się integralną częścią antropologii, gdyż w przypadku usamodzielnienia jej standardy obniżą się. Boas wprowadzał do folklorystyki rygorystyczne, naukowe metody. Był on mistrzem w wykorzystywaniu obszernych badań terenowych oraz naukowych wytycznych w badaniach folkloru. Uważał, że prawdziwa teoria może być stworzona tylko poprzez dogłębną analizę; ale nawet tak stworzona teoria powinna być traktowana jako niedokończona aż do czasu, gdy zostanie udowodniona. Tego rodzaju sztywna, naukowa metoda została szeroko zaakceptowana jako jedna z podstawowych zasad folklorystyki: metody Boasa używane są aż do dzisiaj. Wielu z jego studentów zaliczanych jest dzisiaj do najważniejszych badaczy folkloru.
Boas pasjonował się kolekcjonowaniem folkloru i uważał, że podobieństwa w opowieściach ludowych wśród różnych grup ludności są wynikiem rozpowszechniania idei (dyfuzjonizm). Starał się udowodnić tę teorię, a jego wysiłki zaowocowały utworzeniem metody analizy opowieści ludowych, poprzez rozkładanie jej na poszczególne części i kolejną ich analizę. Jego pojęcie słów kluczowych (ang. "catch words") pozwoliło klasyfikować poszczególne części i analizować je w związku z innymi, podobnymi w różnych opowieściach. Boas chciał również dowieść, że nie wszystkie kultury rozwijały się tak samo, a te pozaeuropejskie nie były prymitywne, lecz po prostu inne.
Boas pozostawał aktywnym działaczem na rzecz rozwoju folkloru przez całe życie. W 1908 roku Został redaktorem „Journal of American Folklore”. Regularnie pisał i publikował artykuły z dziedziny folkloru (często właśnie w tym czasopiśmie). Pomagał również w 1925 roku w wyborze Louise Pound na przewodniczą American Folklore Society.
Dziedzictwo.
Koncepcje i badania Boasa miały długotrwały wpływ na antropologię. Prawie wszyscy współcześni antropolodzy akceptują dzisiaj jego zaangażowanie na rzecz empiryzmu oraz metodologię relatywizmu kulturowego. Większość współczesnych antropologów kulturowych dzisiaj podziela jego poglądy na temat badań terenowych, obejmujących potrzebę przedłużonego czasu pobytu, naukę miejscowego języka oraz rozwój relacji z informatorami. Jego badania stały się inspiracją dla późniejszych badań amerykańskich strukturalistów w językoznawstwie (hipoteza Sapira-Whorfa o determinującej roli języka w postrzeganiu rzeczywistości). Boas był zwolennikiem poglądu, że badanie kultury jest niemożliwe bez uwzględnienia kontekstów historycznych. Doceniana jest również jego krytyka ideologii rasistowskich. Thomas Gossett w dziele z 1963 roku "Race. The History of an Idea in America", napisał: „Prawdopodobne jest, że Boas zrobił więcej, by pokonać uprzedzenia rasowe, niż ktokolwiek inny w historii”

</doc>
<doc id="1664" url="https://pl.wikipedia.org/wiki?curid=1664" title="Francis Crick">
Francis Crick

Francis Harry Compton Crick (ur. 8 czerwca 1916 w Northampton, zm. 28 lipca 2004 w San Diego) – angielski biochemik, genetyk i biolog molekularny, laureat Nagrody Nobla w dziedzinie fizjologii lub medycyny w roku 1962. Wraz z Jamesem D. Watsonem, Maurice'em Wilkinsem i Rosalindą Franklin odkrył strukturę molekularną DNA. Pracownik naukowy Laboratorium Biologii Molekularnej na Uniwersytecie Cambridge, członek Towarzystwa Królewskiego w Londynie. Laureat Medalu Copleya.
Pomimo swoich ateistycznych przekonań, był jednym z uczonych negujących możliwość samoistnego powstania życia z materii nieożywionej. Odrzucając możliwość ewolucji chemicznej w warunkach ziemskich, Crick starał się wytłumaczyć pochodzenie życia na Ziemi za pomocą teorii panspermii.

</doc>
<doc id="1665" url="https://pl.wikipedia.org/wiki?curid=1665" title="Flamewar">
Flamewar



</doc>
<doc id="1666" url="https://pl.wikipedia.org/wiki?curid=1666" title="Fourier">
Fourier



</doc>
<doc id="1668" url="https://pl.wikipedia.org/wiki?curid=1668" title="Faust (imię)">
Faust (imię)

Faust – imię męskie pochodzenia łacińskiego (, „błogi, szczęśliwy”), należące do niedużej grupy najstarszych imion rzymskich (imion właściwych, "praenomen", por. Marek, Tyberiusz, Tytus), i oznaczające „szczęśliwie rosnący”. Łacińska grupa, od której imię to pochodzi ( – "błogi, pomyślny", ale także "faveo, favi, fautum" – "sprzyjać, być przychylnym") nie ma pochodzenia indoeuropejskiego oraz pewnych odpowiedników w żadnym języku indoeuropejskim.
"Faustus" już w epoce klasycznej zaczął wychodzić z roli imienia i występować w funkcji przydomka ("cognomen"). W Polsce po raz pierwszy zapisano to imię w 1488 roku. Jego patronem jest m.in. św. Faust z Riez.
Żeńskim odpowiednikiem jest Fausta.
Faust imieniny obchodzi: 15 lutego, 16 lipca, 3 października, 4 października, 5 października, 13 października, 19 listopada i 26 listopada.
Znane osoby o imieniu Faust:

</doc>
<doc id="1670" url="https://pl.wikipedia.org/wiki?curid=1670" title="Friedrich August Kekulé von Stradonitz">
Friedrich August Kekulé von Stradonitz

Friedrich August Kekulé von Stradonitz (ur. 7 września 1829 w Darmstadt, zm. 13 lipca 1896 w Bonn) – chemik niemiecki, profesor na uniwersytecie w Gandawie od 1858 oraz w Heidelbergu i Bonn od 1867, współtwórca teorii budowy związków organicznych. Laureat Medalu Copleya.
Udowodnił (1858), że węgiel jest czterowartościowy i że jego atomy w związkach organicznych mogą tworzyć łańcuchy.
Odkrywca (1866) pierścieniowej budowy benzenu, co zapoczątkowało nowy dział chemii organicznej – chemię związków aromatycznych. Legenda głosi, jakoby rozwiązanie dręczącego chemika problemu nadeszło we śnie, podczas drzemki przy kominku, w postaci tańczących w kręgu diabłów. W innej anegdocie, podczas snu zobaczył węża, zjadającego własny ogon niczym starożytny Uroboros. Sam uczony jednak tak opisywał odkrycie: 
I kolejny opis snu: 

</doc>
<doc id="1671" url="https://pl.wikipedia.org/wiki?curid=1671" title="Farad">
Farad

&lt;/math&gt;
 |CGS-jedn = 
 |Imp-jedn = 
 |Nazwisko = Michael Faraday
 |Etymologia = 
 |JednPodstawowa = 
Farad (F) – jednostka pojemności elektrycznej w układzie SI (jednostka pochodna układu SI).
Przewodnik elektryczny ma pojemność elektryczną jednego farada, gdy zwiększa potencjał o 1 wolt po dostarczeniu ładunku 1 kulomba. W uproszczeniu pojemność jednego farada oznacza, że w przewodniku o potencjale jednego wolta można „umieścić” ładunek o wartości jednego kulomba.
Jest to zarazem pojemność kondensatora, w którym naładowanie okładek przeciwnymi ładunkami o wartości 1 kulomba wytwarza napięcie 1 wolta.
formula_1
Jednostka została nazwana na cześć dziewiętnastowiecznego fizyka i chemika angielskiego Michaela Faradaya.
Farad jest bardzo dużą jednostką. Najczęściej używane są podwielokrotności farada:
Historia.
Pojęcie „farad” zostało początkowo wprowadzone przez Josiaha Latimera Clarka i Charlesa Brighta w roku 1861 jako jednostka ładunku. W roku 1881, na międzynarodowym kongresie elektryków w Paryżu, termin ten był już oficjalnie używany jako określenie pojemności elektrycznej.

</doc>
<doc id="1672" url="https://pl.wikipedia.org/wiki?curid=1672" title="Francois Villon">
Francois Villon



</doc>
<doc id="1674" url="https://pl.wikipedia.org/wiki?curid=1674" title="François Duquesnoy">
François Duquesnoy

François Duquesnoy lub "Du Quesnoy", zwany "Il Fiammingo" (ochrz. 12 stycznia 1597 w Brukseli, zm. 19 lipca 1643 w Livorno) – flamandzki rzeźbiarz.
Życiorys.
Syn flamandzkiego rzeźbiarza barokowego Jérôme'a Duquesnoy, którego dziełem jest słynna rzeźba-fontanna Manneken pis.
W 1618 François Duquesnoy wyjechał do Rzymu, gdzie m.in. wykonywał rzeźby dla bazyliki św. Piotra. Przyjaźnił się z malarzem francuskim Nicolasem Poussinem, którego teorie artystyczne miały duży wpływ na jego sztukę. Jako jedynemu w tym czasie udało mu się stworzyć indywidualny klasycyzujący styl, przeciwstawny stylowi Berniniego (któremu wpływowi uległo większość rzeźbiarzy włoskich tego czasu). Arcydziełem Duquesnoy jest rzeźba świętej Zuzanny w rzymskim kościele Santa Maria di Loreto (1629–1633).

</doc>
<doc id="1675" url="https://pl.wikipedia.org/wiki?curid=1675" title="Fulwia Plautilla">
Fulwia Plautilla



</doc>
<doc id="1676" url="https://pl.wikipedia.org/wiki?curid=1676" title="Fiszer">
Fiszer



</doc>
<doc id="1677" url="https://pl.wikipedia.org/wiki?curid=1677" title="Ferdynand Magellan">
Ferdynand Magellan

Ferdynand Magellan, , (ur. wiosną 1480, zm. 27 kwietnia 1521) – żeglarz portugalski w służbie hiszpańskiej, odkrywca i morski podróżnik.
Jedna z najwybitniejszych postaci w dziejach żeglugi i odkryć geograficznych. 20 września 1519 roku wyruszył z Hiszpanii drogą zachodnią, czyli przez Ocean Atlantycki, do Wysp Korzennych w Archipelagu Malajskim. Wyprawa, która wypłynęła pod jego dowództwem, jako pierwsza opłynęła Ziemię. Zginął 27 kwietnia 1521 w walce z mieszkańcami wyspy Mactan w Archipelagu Filipińskim, opierającymi się kolonizacji.
Życiorys.
Ferdynand Magellan urodził się w Ponte da Barca, na terenie współczesnego dystryktu Viana do Castelo, w położonym na północnym zachodzie Portugalii regionie Minho. Jego rodzicami byli Pedro Rui de Magalhães. Magellan miał dwoje rodzeństwa: brata Diega de Sousę i siostrę Isabel.
Rodzice Magellana zmarli, gdy miał 10 lat. W wieku 12 lat Magellan został paziem króla Jana II i królowej Eleonory na dworze królewskim w Lizbonie, dokąd jego brat przyjechał dwa lata wcześniej. Tutaj, razem z kuzynem Francisco Serrano, Magellan kontynuował edukację, interesując się szczególnie geografią i historią. Niektórzy spekulują, że jego nauczycielem mógł być Martin Behaim. W 1495 zmarł jego protektor, król Jan II, a jego następcą został Manuel I, przeciwnik swego poprzednika i jego ludzi.
W wieku 25 lat Ferdynand wypłynął po raz pierwszy na morze. W 1505 roku popłynął do Indii, aby wprowadzić tam Francisca de Almeidę na urząd portugalskiego wicekróla i założyć po drodze bazy wojskowe i morskie. To właśnie tam uzyskał chrzest bojowy: gdy miejscowy król odmówił płacenia daniny, oddziały Almeidy zdobyły muzułmańskie miasto Kilwa w dzisiejszej Tanzanii.
W 1513 roku Magellan został wysłany do Maroka, gdzie 28 sierpnia wziął udział w bitwie pod Azamorem i został poważnie ranny w kolano w trakcie oblężenia mauretańsko-marokańskiej twierdzy. Mimo awansu na kwatermistrza Magellanowi nie wiodło się dobrze: został oskarżony o nielegalny handel z muzułmańskimi Mauretanami. Wdał się też w konflikt z Almeidą: gdy opuścił bez zezwolenia armię, Almeida posłał negatywny raport o żeglarzu na portugalski dwór. Kilka oskarżeń wycofano, ale Magellan popadł w niełaskę króla Manuela I, który wprawdzie podniósł go do godności "fidalgo escudeiro", ale odmówił podwyższenia dochodów.
Król oznajmił także Magellanowi, że po 15 maja 1514 roku nie będzie już dalej zatrudniony w królewskiej służbie. W roku 1517 Magellan – za zezwoleniem króla – formalnie wyrzekł się obywatelstwa i wyjechał, aby zaoferować swoje usługi dworowi Hiszpanii, zmieniając jednocześnie swoje nazwisko Fernão de Magalhães na Fernando de Magallanes.
Plany opłynięcia świata.
Magellan dotarł do Sewilli 20 października 1517 roku, a stamtąd udał się do Valladolid, aby spotkać się z nastoletnim królem, Karolem I (późniejszym władcą Świętego Cesarstwa Rzymskiego, Karolem V).
Korzystając z pomocy Juana de Aranda, jednego z trzech głównych oficjeli Domu Indyjskiego w Sewilli, i innych przyjaciół, zwłaszcza Portugalczyka Dioga Barbosy, Magellan naturalizował się jako Hiszpan. Zyskując w Sewilli znaczne wpływy, zdobył posłuchanie u króla Karola i potężnego Juana Rodrigeza de Fonseca, biskupa Burgos (niegdyś przeciwnika Krzysztofa Kolumba).
Nieco wcześniej Magellan odkrył mapę, naszkicowaną na podstawie wcześniejszych podróży, która wskazywała Rio de la Plata, dużą deltę w kształcie zatoki w Ameryce Południowej, domniemanego przejścia przez kontynent na Morza Południowe. Zdecydował się na pionierskie przejście tą drogą, aby dotrzeć do Wysp Korzennych, klucza do strategicznie ważnego i niezwykle lukratywnego handlu przyprawami. Podobno, aby zrealizować ten projekt, miał się zobowiązać do płynięcia na południe aż do 75 równoleżnika.
W planie tym pomógł mu Ruy Faleiro, astronom i uchodźca portugalski, który pozyskał nieocenione wsparcie finansowe ze strony Christophera de Haro, członka wielkiej firmy z Antwerpii, żywiącego urazę do króla Portugalii. 22 marca 1518 roku "Casa de Contratación de las Indias" (Urząd do spraw Indii) oraz król Karol zaaprobowali plan Magellana i przyznali mu hojne fundusze. W ramach umowy Magellan i Faleiro, jako dowódcy wyprawy, mieliby otrzymać 1/20 wszystkich zysków i oni, oraz ich potomkowie, mieliby rządzić wszelkimi odkrytymi ziemiami, nosząc tytuły "Adelantados" (gubernatorów prowincji granicznych).
Za pieniądze otrzymane od króla, Magellan i Faleiro zakupili 5 statków:
Wyprawa.
10 sierpnia 1519 roku flota pięciu statków pod dowództwem Magellana opuściła Sewillę i popłynęła rzeką Gwadalkiwir do Sanlúcar de Barrameda, gdzie pozostawała przez pięć tygodni. Władze hiszpańskie zachowywały ciągle rezerwę wobec portugalskiego admirała i niewiele brakowało, aby wstrzymały podróż, ale 20 września 1519 roku Magellan wyruszył ostatecznie z 270 członkami załogi.
Usłyszawszy o jego wypłynięciu, portugalski król Manuel I rozkazał jednostkom floty ścigać go, ale Magellan wymknął się Portugalczykom. Po krótkim postoju na Wyspach Kanaryjskich przybył na Wyspy Zielonego Przylądka, skąd wziął kurs na Przylądek Świętego Augustyna w Ameryce Południowej. 20 listopada przekroczył równik, a 6 grudnia załoga zobaczyła brzegi Brazylii.
Brazylia była w owym czasie terytorium portugalskim, Magellan podjął jednak ryzyko i 13 grudnia stanął na kotwicy w pobliżu dzisiejszego Rio de Janeiro, gdzie panowała dobra pogoda, a tubylcy byli przyjaźnie nastawieni. Flota uzupełniła zapasy, ale nieprzychylne warunki spowodowały, że wypłynięto z opóźnieniem. Następnie pożeglowano wzdłuż wybrzeża Ameryki Południowej, zatrzymując się 10 stycznia 1520 roku nad Río de la Plata. Gdy wyprawa szukała ciągle przejścia na morza po drugiej stronie lądu amerykańskiego, u wybrzeży Argentyny została zaskoczona nadejściem zimy.
Magellan zdecydował się spędzić zimę w Patagonii. 31 marca załoga założyła osadę, którą nazwano Puerto San Julian (port Św. Juliana). Wybuchł wtedy bunt, w którym wzięło udział trzech spośród pięciu kapitanów statków, jednak nieudany, gdyż załogi pozostały w dużej części lojalne. Quesada i Mendoza zostali straceni, a Cartagena i kapelan wyprawy zostali wysadzeni na brzeg.
Cieśnina Magellana przecina południowy koniec Ameryki Południowej, łącząc Atlantyk i Pacyfik. 24 sierpnia podróż została podjęta i „Santiago” został wysłany na ląd w misji zwiadowczej, ale rozbił się w drodze powrotnej. Tylko dwóch marynarzy wróciło drogą lądową, aby powiadomić Magellana, co się stało. Na 52° szerokości południowej, 21 października 1520 roku flota osiągnęła Cape de las Virgenes i stwierdziła, że przejście zostało znalezione, ponieważ woda była słona i głęboka. Cztery statki rozpoczęły powolne przedzieranie się przez liczące 372 mile przejście, które Magellan nazwał "Estreito de Todos los Santos" (Cieśnina Wszystkich Świętych), ponieważ flota wpłynęła na nie w dniu Wszystkich Świętych, 1 listopada. Obecnie nosi ona nazwę Cieśniny Magellana.
Magellan zlecił najpierw statkom „San Antonio” i „Concepcion” zbadanie cieśniny, ale ten pierwszy, dowodzony przez Gomeza, zdezerterował i zawrócił do Hiszpanii, gdzie dotarł w dziewięć dni po śmierci Magellana. 28 listopada pozostałe trzy statki wypłynęły na południowy Pacyfik. Magellan nazwał te wody "Mar Pacifico" (Morze Spokojne) z racji jego spokojnych przez cały pierwszy etap podróży wód.
Śmierć Magellana.
Kierując się na północ, 13 lutego 1521 roku załogi przekroczyły równik. 6 marca odkryły wyspy Guam i Rota w archipelagu Marianów, a 16 marca dotarły do wyspy Homonhon w Archipelagu Filipińskim. Do tej chwili pozostało tylko 100 członków wyprawy. Magellan był w stanie porozumiewać się z tamtejszymi mieszkańcami, gdyż jego malajski tłumacz rozumiał ich język. 7 kwietnia żeglarze dotarli do wyspy Cebu. Magellan zorganizował tam postój, polecając jednocześnie nawracanie miejscowej ludności na chrześcijaństwo. W zaledwie kilka dni od 14 kwietnia 1521 roku prawie wszyscy przywódcy wysp otaczających Cebu przyjęli chrzest i zgodzili się podpisać traktat z Hiszpanami. Traktat ten miał zapewnić Hiszpanii wyłączność na handel z wyspami będącymi pod panowaniem przywódcy Cebu w zamian za protekcję. Po niespełna tygodniu Magellan otrzymał wiadomość, że Lapu-Lapu – przywódca Mactanu, zbuntował się i odmawia dostarczania towarów przeznaczonych dla Hiszpanów. Magellan zapowiedział użycie siły. Władca Cebu zaoferował mu pomoc w postaci około tysiąca wojowników, jednakże Magellan odmówił, chcąc w sile własnych ludzi zademonstrować swoją potęgę. 21 kwietnia statki hiszpańskie udały się w kierunku cieśniny pomiędzy Cebu a Mactanem.
Potyczka z ludźmi Lapu-Lapu rozegrała się 27 kwietnia 1521 r., podczas której Magellan poniósł śmierć wraz z częścią swych towarzyszy.
Antonio Pigafetta, bogaty i żądny wrażeń obieżyświat, który zapłacił za możność uczestnictwa w wyprawie Magellana, jedyna osoba, która pozostała przy życiu i mogła opowiedzieć o jego śmierci, napisał:
Opłynięcie świata.
Magellan zawarł w swojej ostatniej woli, że jego malajski tłumacz zostanie po jego śmierci wolnym człowiekiem. Tłumacz, który został ochrzczony jako Enrique (Henryk) w Malakce, w 1511 roku, a został porwany przez sumatrzańskich handlarzy niewolników ze swojej rodzinnej wyspy, stał się pierwszym człowiekiem, który opłynął glob. Enrique został związany umową przez Magellana w czasie jego wcześniejszych podróży do Malakki i był u jego boku w czasie walk w Afryce, w czasie konfliktu Magellana z dworem portugalskim i wtedy, gdy organizowano flotę. Jednak po wydarzeniach na Mactan i śmierci Magellana pozostali kapitanowie statków odmówili uwolnienia Enrique. Ten zerwał umowę i zdołał zbiec. Jednak Antonio Pigafetta robił cały czas notatki językowe i najwyraźniej był w stanie prowadzić komunikację z miejscowymi w trakcie dalszej podróży.
Trzy pozostałe statki popłynęły na zachód do Palawan, który opuściły 12 czerwca 1521 roku, kierowane do Brunei na Borneo przez pilotów Moro, potrafiących nawigować na płytkich wodach. Zakotwiczyły na 35 dni w pobliżu Brunei, gdzie Pigafetta opisuje przepych dworu radży Siripada (złoto, perły wielkości kurzych jaj itd.). Co więcej, Brunei mogło się pochwalić poskramianiem słoni i uzbrojeniem w 62 działa, 5-krotnie więcej niż miały statki Magellana. W Brunei lekceważono korzenie, które miały się okazać droższe od złota po powrocie do Hiszpanii. Pigafetta wspomina niektóre wyroby na dworze, jak porcelana, która nie była szeroko dostępna w Europie, i okulary – monokle dopiero wchodziły w Europie do użytku.
Po dotarciu do Moluków (Wysp Korzennych), 8 listopada 1521 roku, udało im się zawrzeć układ z sułtanem Tidore, który był rywalem sułtana Ternate i sojusznikiem Portugalii, który teraz postanowił poddać się pod panowanie Hiszpanów. Statek „Concepcion”, niemal doszczętnie stoczony przez świdraki, został spalony, a przyprawy przeniesione na „Victorię” i „Trinidad”. Ten drugi został jednak przechwycony przez Portugalczyków w czasie próby powrotu drogą przez Pacyfik. „Victoria” wyruszyła więc do domu 21 grudnia 1521 roku samotnie i 22 maja 1522, dowodzona przez Juana Sebastiána Elcaño okrążyła Przylądek Dobrej Nadziei, mając jedynie ryż jako racje żywnościowe. 20 członków załogi zmarło z chorób i głodu, zanim Elcaño dotarł do Wysp Zielonego Przylądka, posiadłości portugalskiej, gdzie wysłał na ląd 13 dalszych członków załogi celem zakupu pewnej ilości ryżu. Ci jednak zostali przez Portugalczyków potraktowani jak szpiedzy i zatrzymani.
Powrót.
6 września 1522 Juan Sebastián Elcaño i pozostali członkowie wyprawy Magellana powrócili do Hiszpanii na ostatnim statku wyprawy, po niemal trzech latach od wyruszenia.
Elcaño otrzymał od króla Karola V szlachectwo i wysoką pensję 500 dukatów rocznie. Karol zażądał od swojego szwagra Joao III zwolnienia z portugalskiego aresztu na Wyspach Zielonego Przylądka 13 pozostałych członków załogi, co szybko wykonano. Zastępca Elcaño nazwiskiem Miguel de Rodas otrzymał od króla pensję 50 tysięcy "maravedi" rocznie (ponad dwukrotne zarobki oficera), aczkolwiek 3 lata później skarżył się, że nie otrzymał do ręki nic. Główny inwestor organizujący wyprawę Magellana, nazwiskiem Cristobal de Haro, mimo strat w ludziach i statkach, został bogaczem. Przywieziono ok. 25-26 ton goździków, które sprzedano za zawrotną kwotę 7 888 634 "maravedi", czyli 947 tys. dzisiejszych dolarów amerykańskich.
Tylko 4 z 55 pierwotnych członków załogi statku „Trinidad” powróciło w 1527 roku do Hiszpanii.
Odkrycia i ustalenia.
Wyprawa Magellana jako pierwsza opłynęła glob i jako pierwsza przepłynęła cieśninę (Cieśninę Magellana) w Ameryce Południowej łączącą Atlantyk i Pacyfik. Członkowie wyprawy byli też pierwszymi Europejczykami, którzy zauważyli następujące rzeczy:
Większym jednak od potwierdzenia kulistości (a tym samym niewyobrażalnej wówczas wielkości) naszej planety zdziwieniem dla załogi było niedoliczenie się jednego dnia. Zwyczajowo, cały czas podczas podróży prowadzono precyzyjne zapiski pomiarów czasu i miejsca. Magellan zabrał początkowo kilkadziesiąt 30-minutowych klepsydr. Podczas powrotu dookoła Afryki załoga została zmuszona do zatrzymania się na Wyspach Zielonego Przylądka. Tamże, w środę 9 lipca, załoga dowiedziała się, że jest właśnie czwartek (następny dzień kalendarzowy). Załoga mogła popełnić śmiertelny grzech niezachowania postu w piątki, mszy w niedziele itp. Kwestia wyjaśniła się dopiero po powrocie do Sewilli. Po ukończonej podróży okazało się, że „stracili” jeden dzień. Analiza błędu (przy okrążaniu Ziemi przy przekraczaniu każdego stopnia długości geograficznej słońce wstaje o 4 minuty wcześniej – lub później w zależności od kierunku podróży – co przy 360° daje różnicę wynoszącą 1440 minut, czyli jeden dzień) przyczyniła się w przyszłości do wyznaczenia linii zmiany daty.

</doc>
<doc id="1679" url="https://pl.wikipedia.org/wiki?curid=1679" title="Faustyna">
Faustyna



</doc>
<doc id="1680" url="https://pl.wikipedia.org/wiki?curid=1680" title="Fantastyka">
Fantastyka

Fantastyka – gatunek literacki, a także filmowy, komiksowy, gier a niekiedy i malarstwa. Polega ona na kreowaniu świata przedstawionego, który w całości, lub w części różni się od rzeczywistości, na przykład przez dodanie elementów nadnaturalnych, lub stworzonej przez autora technologii. Można ją też uznać za właściwość elementu fikcji literackiej, lub innej, sprawiająca, że choć czytelnik uznaje go za niezgodny z przyjętymi przez niego kryteriami empirycznej rzeczywistości i wedle jego najlepszej wiedzy nieposiadający odpowiednika w świecie pozaliterackim oraz nieobecny w tekstach, które uznaje za realistyczne, godzi się w ramach literackiej gry uznać go za prawdopodobny (choć niewiarygodny) w obrębie konstruowanej fikcji i decyduje, że nie narusza on spójności narracji.
Fantastykę cechuje duża swoboda w zakresie konstrukcji świata przedstawionego i postaci bohaterów, z zastrzeżeniem, że jej odbiorca powinien uznać dzieło jako spójną całość, działającą w ramach stworzonego przez autora świata. Wiele utworów z tej kategorii zostaje umieszczonych w pseudo-rzeczywistości (pewne aspekty są zgodne z faktami, np. historią czy fizyką) lub rzeczywistości wygenerowanej (wtedy reguły takie najczęściej spisane, odnoszą się do wszystkich utworów napisanych w danej serii – przykładem może być książka J.R.R. Tolkiena "Władca Pierścieni").
Fantastyka szczególnie ważną rolę pełniła w okresie romantyzmu (E.T.A. Hoffmann i Edgar Allan Poe). W XX w. można zaobserwować szczególny rozwój fantastyki naukowej oraz fantasy.
Podstawowe sposoby interpretacji terminu „fantastyka”.
„Fantastykoznawstwo” jako jedna z najmłodszych gałęzi współczesnej teorii literatury nie stworzyło dotychczas ogólnie zaakceptowanej i satysfakcjonującej większość badaczy metodologii badań i terminologii, które pozwoliłyby w sposób jasny i wiarygodny określić istotę i granice fantastycznych światów przedstawionych. Można wyłonić trzy podstawowe sposoby interpretacji terminu „fantastyka”. Najbardziej rozpowszechnione jest uznanie fantastyki za specyficzny typ twórczości, kreujący odmienny od realnego świat przedstawiony (np. Umberto Eco, Andrzej Niewiadowski, Antoni Smuszkiewicz, Tatiana Czernyszowa i inni).
Fantastyka może też być pojmowana jako ponadrodzajowy, ponadgatunkowy i ponadczasowy wyznacznik dzieła literackiego. Większość badaczy genealogii literatury fantastycznej uznaje za najstarszą odmianę fantastyki mit pierwobytny i baśń ludową, podkreślając jedność wszystkich fantastycznych światów przedstawionych niezależnie od rodzaju występujących w nich elementów fantastycznych (np. Stanisław Lem, Ryszard Handke, Kingsley Amis itd.). Niekiedy fantastyka rozumiana jest bardzo wąsko, jako cecha charakterystyczna jednego gatunku literackiego. Tego typu poglądy prezentuje w pracy "Od baśni do „science fiction”" Roger Callois.
Podział fantastyki.
Gatunek ten można podzielić na trzy główne nurty:
Wśród tych głównych nurtów można jednak wyróżnić też podgatunki, takie jak High fantasy, Low fantasy, Hard science fiction, , historia alternatywna, a także nurty przenikające granice między fantastyką naukową i fantasy, jak urban fantasy, Steampunk, Dieselpunk, space opera, czy też realizm magiczny. Gatunkami powstałymi poprzez połączenie horroru i innych nurtów fantastyki są np. , horror lovecraftowski czy weird fiction, gatunku literackiego na pograniczu fantastyki naukowej, fantasy, horroru oraz realizmu magicznego. Stanisław Lem uznawał jednak, że istotą tego nurtu jest przełamanie ustalonego porządku poprzez wtargnięcie elementów, które do tego porządku nie przystają.
Fantastyka w Polsce.
Rozwój fantastyki w Polsce był stosunkowo nierównomierny. Gdy za granicą fantasy cieszyło się już dużą popularnością, w Polsce praktycznie nie istniało aż do lat 80. XX wieku. Polskie utwory science-fiction zostały rozpropagowane między innymi dzięki sukcesowi Stanisława Lema, pisarza literatury science-fiction, w latach 50. XX wieku. Drugim istotnym twórcą okazał się Janusz A. Zajdel, uznawany za ojca polskiej fantastyki socjologicznej.
W 1982 roku powstał miesięcznik "Fantastyka", którego celem było propagowanie literatury z tego gatunku. Czasopismo istnieje do dziś, od 1990 roku pod zmienioną nazwą "Nowa Fantastyka". To na jego łamach pojawiło się jedno z dwóch pierwszych polskich opowiadań fantasy pod tytułem "Mag", autorstwa Jacka Piekary. W 1986 roku na łamach magazynu ukazało się również opowiadanie "Wiedźmin" Andrzeja Sapkowskiego.
Czytelnicy fantastyki.
Badania amerykańskiego magazynu „Analog” wykazały, że przeciętny czytelnik fantastyki ma wykształcenie ponadpodstawowe, z czego 45% ukończyło gimnazjum, a blisko 15% ma wykształcenie wyższe. Wśród tych osób 17% to inżynierowie, 9,5% nauczyciele, a elektronicy 10%. Badanie czytelnictwa fantastyki w Polsce z 2012 wykazało, że większość czytelników stanowią mężczyźni, a najbardziej popularna grupa wiekowa to 20-24 lata.

</doc>
<doc id="1681" url="https://pl.wikipedia.org/wiki?curid=1681" title="Ford FT-B">
Ford FT-B

Ford FT-B (Ford Tf-c, model 1920) – samochód pancerny konstrukcji polskiej z okresu wojny polsko-bolszewickiej, produkowany w Centralnych Warsztatach Samochodowych w Warszawie. Pierwsza tego typu polska konstrukcja i podstawowy wóz pancerny w wyposażeniu polskich sił pancernych w tamtym okresie.
Historia.
Latem 1920 roku sytuacja Wojska Polskiego na froncie wojny polsko-bolszewickiej była trudna. Jednym z problemów był brak pojazdów pancernych i niemożność ich zakupu za granicą. W tej sytuacji inż. Tadeusz Tański, cywilny pracownik Sekcji Samochodowej Ministerstwa Spraw Wojskowych, z własnej inicjatywy opracował projekt budowy samochodu pancernego na podwoziu popularnego w tym czasie samochodu osobowego Ford Model T. Podwozie zostało zmodyfikowane – wzmocniono tylny most i drążki kierownicze, pochylono bardziej kierownicę, zmieniono położenie zbiornika paliwa, wydłużono korbę rozruchową silnika, zmieniono deskę rozdzielczą. W ciągu dwóch tygodni prototyp był gotów do trwających kilka dni prób. Wypadły one pomyślnie. Rozpoczęto produkcję. Zbudowano 16 lub 17 samochodów.
Służba.
Początkowo planowano sformować jeden większy, samodzielny oddział złożony z samochodów Ford FT-B. Sytuacja wojenna (trwająca Bitwa Warszawska) spowodowała, że poszczególne pojazdy były, zaraz po zbudowaniu, wysyłane na front. Były tam używane w składzie różnych oddziałów. Dopiero w późniejszym okresie zrealizowano początkowy plan. Sformowano m.in. 1 kolumnę lekkich samochodów pancernych.
Po wojnie w uzbrojeniu wojska pozostało 12 samochodów tego typu. Początkowo większość służyła w 3 dywizjonie samochodów pancernych w Warszawie. Jesienią 1925 roku zostały rozdzielone do nowo sformowanych szwadronów samochodów pancernych. Jeden z nich o numerze rejestracyjnym "5021" trafił do 2 Szwadronu Samochodów Pancernych w Warszawie. Sześć egzemplarzy eksploatował 1 Szwadron Samochodów Pancernych.
W latach 1927-1928 były stopniowo wycofywane. W 1931 roku w wykazach figurował już tylko jeden samochód (nr rej. 5021).

</doc>
<doc id="1682" url="https://pl.wikipedia.org/wiki?curid=1682" title="Federacja Rodezji i Niasy">
Federacja Rodezji i Niasy

Federacja Rodezji i Niasy () – była brytyjska posiadłość w południowej Afryce, utworzona w 1953 roku z kolonii: Rodezja Południowa, Rodezja Północna i Niasa. 
Federacja została rozwiązana w 1963, a rok później niepodległość uzyskały Rodezja Północna jako Zambia i Niasa jako Malawi. Rodezja Południowa ogłosiła ją dwa lata później, jako Rodezja. Decyzji tej nie uznał Londyn, ponieważ tamtejsze władze prowadziły politykę dyskryminacji rasowej na wzór RPA. Gdy w 1979 roku sytuacja uległa zmianie, Wielka Brytania w rok później przyznała jej niepodległość. Państwo to nazywa się od tamtej pory Zimbabwe.

</doc>
<doc id="1683" url="https://pl.wikipedia.org/wiki?curid=1683" title="Francuskie Terytorium Afarów i Isów">
Francuskie Terytorium Afarów i Isów

Francuskie Terytorium Afarów i Isów (fr. "Territoire français des Afars et des Issas") – w latach 1967–1977 nazwa francuskiego terytorium zamorskiego położonego w północno-wschodniej Afryce. Wcześniej nosiło nazwę "Somali Francuskie". W roku 1977 uzyskało niepodległość jako Dżibuti.
Nazwa pochodziła od dwóch głównych grup etnicznych zamieszkujących terytorium: Afarów i Issów.

</doc>
<doc id="1684" url="https://pl.wikipedia.org/wiki?curid=1684" title="Friedrich Nietzsche">
Friedrich Nietzsche

Friedrich Wilhelm Nietzsche (wym. niem. ; ur. 15 października 1844 w Röcken, zm. 25 sierpnia 1900 w Weimarze) – niemiecki filozof, filolog klasyczny, prozaik i poeta, jeden z założycieli (obok Wilhelma Diltheya) filozofii życia w Niemczech. Kategorią centralną filozofii Nietzschego jest filozofia życia, ujmowanie rzeczywistości, a więc także człowieka, jako życia. Prowadzi to do zanegowania istnienia ukrytego sensu i układu świata – esencji, rzeczywistość staje się wobec tego chaosem. Konsekwencję tego stanowi radykalna krytyka chrześcijaństwa oraz współczesnej autorowi zachodniej kultury, jako opartych na tym złudzeniu. Podejście Nietzschego do religii i moralności cechował ateizm, psychologizm i historyzm; uważał je (religię i moralność) za wytwory ludzkie obarczone błędem zamiany przyczyny i skutku. Istotny u niego był także szacunek wobec wartości obecnych w antycznej kulturze greckiej, wraz z postulatem powrotu do niej.
Życiorys.
Młodość i studia.
Był synem luterańskiego pastora Carla Ludwiga (1813–1849) i Franziski (zd. Oehler) Nietzsche (1826–1897). Wychowywał się w rodzinie o tradycjach protestanckiej religijności szwabskiej, którą cechowało zamknięcie w ramach indywidualistycznej interpretacji Biblii. Zdaniem niektórych interpretatorów (np. Jaspersa) właśnie protestancka religijność wpłynęła na ukształtowanie pojęcia moralności niewolniczej, analizowanej później przez Nietzschego. Nietzsche przyznawał się wielokrotnie do polskiego pochodzenia. Badania drzewa genealogicznego niemieckiego filozofa (prowadzone przez Maxa Oehlera – kuratora Archiwum Nietzschego w Weimarze), które sięgają XVI wieku wskazują, że miał jedynie niemieckich przodków. Nietzsche chętnie przyznawał się do „polskości”, ponieważ był to rodzaj manifestacji przeciwko Niemcom i ich kulturze, którą uważał za upadłą. Polskie pochodzenie filozofa poruszył w artykule „Ueber Nietzches Polentum” Bernard Scharlitt, wywodząc ten ród od Nickich herbu Radwan z województwa płockiego.
Od śmierci ojca w 1849 roku domem Nietzschego zarządzały kobiety: babka, ciotki, młoda jeszcze matka i siostra Elisabeth Förster-Nietzsche; w 1850 roku cała rodzina przeniosła się do Naumburga (Saale), gdzie Nietzsche rozpoczął naukę w szkole miejskiej, którą kontynuował w gimnazjum przykatedralnym. W tym czasie samodzielnie zapoznał się z klasyczną muzyką niemiecką – dziełami Bacha, Händla, Mozarta, Haydna, Beethovena i Schuberta. Po ukończeniu gimnazjum zaproponowano mu bezpłatne kontynuowanie edukacji w Pforcie, gdzie odkrył poezję zupełnie nieznanego w tym czasie w Niemczech Hölderlina i uznał jego wielkość.
W październiku 1864 Nietzsche zapisał się na Uniwersytet w Bonn. Podjął tam studia w zakresie filologii klasycznej, teologii, historii Kościoła i sztuki. Zapoznał się wtedy z książką Davida Straussa "Das Leben Jesu. Kritisch bearbeitet" (1835), która poddawała Biblię analizie filologicznej, wskazując zdaniem autora na jej mityczne podłoże oraz zawarte w jej księgach bardzo liczne sprzeczności w opisie zdarzeń (w tym wielu cudów). Pod wpływem tej książki Nietzsche stracił wiarę. Po dwóch semestrach przeniósł się na Uniwersytet w Lipsku, gdzie jeszcze jako student stał się uznanym filologiem klasycznym i współtworzył publiczne towarzystwo filologiczne. W 1866 ukazały się drukiem jego pierwsze publikacje – rozważania o poezji Teognisa z Megary i komentarze do Arystotelesa.
Rok 1867 był jednym z przełomowych w życiu Nietzschego. Zapoznał się wtedy z filozofią Arthura Schopenhauera zawartą w dziele "Świat jako wola i przedstawienie". Schopenhauerowska analiza moralności, jednostki wybitnej, znaczenia sztuki w życiu człowieka pobrzmiewa echem w całym dziele Nietzschego, nawet gdy doszedł już do wniosku, że Schopenhauer się mylił i występował przeciw jego filozofii. Dzięki Schopenhauerowi Nietzsche zbliżył się też do Richarda Wagnera.
Bazylea.
Po odbyciu obowiązkowej służby wojskowej, dzięki zabiegom nauczyciela i przyjaciela Fryderyka Ritschla, któremu zawdzięczał też wcześniejsze publikacje, Nietzschemu zaoferowano profesurę nadzwyczajną na katedrze filologii klasycznej uniwersytetu w Bazylei. Oficjalne powołanie nastąpiło w lutym 1869, na podstawie już opublikowanych prac, jeszcze przed doktoratem, bez egzaminów i formalności habilitacyjnych. Nietzsche miał wtedy niecałe 25 lat. 28 maja 1869 wygłosił swój pierwszy wykład: "Homer i filologia klasyczna".
Była to błyskotliwa kariera, jednak Nietzsche czuł już, że jego przeznaczeniem jest filozofia. W 1872 roku ukazała się książka "Narodziny tragedii, czyli hellenizm i pesymizm", w której wyłożył swe koncepcje dionizyjskości i apollińskości. Książka przeszła bez echa, jeśli nie liczyć paru jawnie negatywnych recenzji, których efektem była utrata zaufania studentów do Nietzschego jako filologa. Mimo talentu wykładowcy (potwierdzonego we wspomnieniach), w semestrze 1872/1873 na jego wykłady uczęszczało zaledwie dwóch słuchaczy. Jedyne wsparcie przyszło ze strony Richarda Wagnera.
W Bazylei zawiązał też Nietzsche przyjaźń z młodszym od siebie o pięć lat Paulem Rée – wywarła ona pewien wpływ na jego twórczość i znacznie większy na życie osobiste.
W Bazylei, w wieku 25 lat, Nietzsche złożył podanie o anulowanie obywatelstwa pruskiego. Oficjalną odpowiedź otrzymał w dokumencie z dnia 17 kwietnia 1869 roku, tak skomentowanym przez Curta Paula Janza:
Richard Wagner.
Nietzsche poznał Wagnera jeszcze w Lipsku, w roku 1868, we wczesnym okresie swojej fascynacji muzyką kompozytora. Połączyła ich właśnie miłość do muzyki i zainteresowanie filozofią Schopenhauera. Wagner był poruszony przeciwstawieniem apollińskość-dionizyjskość, swoje opinie wyraził w liście do Nietzschego.
Lata 1873–1876 to okres silnego zbliżenia z Wagnerem. Nietzsche był stałym gościem u Wagnerów w Triebschen, a po ich przeprowadzce również w Bayreuth. Mógł przyglądać się z bliska życiu wielkiego kompozytora i analizować motywy jego postępowania, a te uznał za dalekie od ideałów chrześcijańskich. Wagner stanowił w oczach Nietzschego dowód, że wielkie dzieło często idzie w parze z „niskimi” pobudkami twórcy – samolubstwem, zawiścią, pragnieniem sławy, zadufaniem w sobie, autorytaryzmem i małostkowością. Między innymi te cechy Wagnera, oprócz późniejszej jego sympatii dla nacjonalizmu i chrześcijaństwa oraz panującego w otoczeniu artysty antysemityzmu, doprowadziły do rozstania Nietzschego z kompozytorem pod koniec 1876 roku.
Lecz w roku 1873 Nietzsche był jeszcze pod wpływem Wagnera – reformatora muzyki i koncentrował się na krytyce współczesnej sobie kultury. Po zwycięstwie nad Francją w 1870 roku i utworzeniu w 1871 Cesarstwa Niemieckiego, Niemcy ogarnęła narodowa euforia. Nietzsche stanowczo przeciwstawiał się powszechnemu przekonaniu o rzekomym zwycięstwie „wyższej” kultury niemieckiej nad „niższą” francuską, konstatując jednocześnie symptomy powszechnego upadku sił duchowych kultury zachodniej. W swych późniejszych dziełach określał tę sytuację mianem "nihilizmu"; na razie ograniczał się do wskazywania na negatywne dla kultury zjawiska: postępującą specjalizację w nauce, wzrost znaczenia prasy jako środka komunikacji społecznej i schematyczność systemu edukacji. Efektem tych zjawisk społecznych był "wykształcony filister" – odpowiednik dzisiejszego „konsumenta kultury”. Zarysowane tu problemy Nietzsche przedstawił dokładniej w "Niewczesnych rozważaniach", które powstały w latach 1873–1874. Współcześni interpretatorzy wskazują na aktualność tych zagadnień.
Ludzkie, Arcyludzkie.
Zerwanie z Wagnerem miało jednak głębsze podłoże niż niezgodność poglądów, było wynikiem przemian, jakie zachodziły w samym Nietzschem. W lipcu 1876, jeszcze w czasie poprawnych stosunków z Wagnerem, Nietzsche napisał pierwsze zdania dzieła "Ludzkie, Arcyludzkie", w którym wstępnie zarysował niektóre koncepcje, rozwijane następnie w późniejszych pracach – krytyka teorii poznania, języka i chrześcijaństwa.
"Ludzkie, Arcyludzkie" jest też pierwszym dziełem, w którym Nietzsche wypróbował formę aforyzmu. Wszystkie jego książki napisane po 1878 roku mają postać zbioru luźnych, choć pogrupowanych tematycznie, krótkich ustępów, czy wręcz pojedynczych zdań. Było to z jednej strony wynikiem poszukiwań najbardziej odpowiedniej dla wyrażenia myśli formy literackiej, a z drugiej koniecznością – w tym właśnie okresie nasiliły się objawy choroby, na którą Nietzsche cierpiał właściwie od wczesnej młodości.
Choroba.
Pierwsze bóle głowy wystąpiły u Nietzschego w 1856 roku, przybierając w późniejszym okresie formy nawet kilkudniowych ataków migreny, połączonych z wymiotami i bólem oczu, co groziło mu ślepotą. Próby kuracji nie dawały rezultatu, Nietzsche uznał zresztą, że sam będzie dla siebie najlepszym lekarzem. Podawano wiele przyczyn wyjaśniających podłoże choroby: od obciążenia dziedzicznego (ojciec Nietzschego zmarł na guza mózgu), po zarażenie syfilisem, jednak żadne z nich nie zyskało powszechnej akceptacji. W 1876 roku Nietzsche zmuszony był prosić na uniwersytecie o urlop zdrowotny, a w 1879 roku, już po ukazaniu się książki "Ludzkie, Arcyludzkie", ostatecznie zrezygnował z profesury. Jako byłemu profesorowi bazylejczycy przyznali mu rentę, co pozwalało na skromne, lecz niezależne życie.
W 1889 roku, po totalnej katastrofie psychicznej w Turynie, choroba Nietzschego została zdiagnozowana jako "paralysis progressiva" (trzeciorzędny syfilis). Mimo braku jakiegokolwiek klinicznego dowodu (pierwszy i niespecyficzny test Wassermana pojawił się w 6 lat po śmierci Nietzschego) diagnoza ta, powtarzana w różnych biografiach, przetrwała przeszło stulecie. Eva Cybulska podważyła tę tezę w 1996 na konferencji nietzscheańskiej w Manchesterze, proponując w zamian diagnozę zespołu maniakalno-depresyjnego z waskularną demencją.
Tako rzecze Zaratustra.
Po złożeniu profesury Nietzsche często zmieniał miejsce pobytu, nawet kilkakrotnie w ciągu roku. Poszukując odpowiedniego dla siebie klimatu przemierzał całe Włochy, to znów wracał do Niemiec i Szwajcarii, a jedynym w miarę stałym miejscem pobytu było Sils-Maria nad jeziorem Silvaplana w dolinie Innu w Szwajcarii. To tam miał przeżyć wizję "wiecznego powrotu", który jest jednym z podstawowych pojęć jego filozofii, tam też naszła go myśl o przewartościowaniu wszelkich wartości. W latach 1880–1882 wyszły kolejno książki: "Wędrowiec i jego cień", "Jutrzenka" oraz "Wiedza radosna" będące efektem zmagań Nietzschego z problemami śmierci Boga, nihilizmu, krytyki chrześcijaństwa i koncepcją nadczłowieka jako ich przezwyciężenia. Wreszcie, w roku 1883 ukazała się „książka dla wszystkich i dla nikogo” – "Tako rzecze Zaratustra", "opus vitae" Nietzschego. W zasadzie nie ma w tej książce nic nowego, wszystkie wyłożone w niej myśli zawarte są już we wcześniejszych dziełach, ale użyty tu język uczynił z niej jedno z najważniejszych osiągnięć literatury niemieckiej. 
Lou Andreas-Salomé.
Lou von Salome, znana później jako Lou Andreas-Salomé (1861–1937), urodzona w Petersburgu w rodzinie francusko-niemieckiej, niekiedy błędnie uważana za Finkę, była córką oficera armii carskiej. Nietzsche poznał ją wiosną 1882 roku, w trakcie jednej ze swych wizyt w Rzymie, a znajomość ta szybko przerodziła się z jego strony w gorące uczucie. Planował szybki ślub. Lou von Salome wydawała się Nietzschemu osobą bardzo bliską duchowo – niektóre jej wiersze sprawiają wrażenie, jakby były napisane ręką samego filozofa. Do jednego z nich, "Hymnu do życia", Nietzsche napisał muzykę, komponując utwór na chór mieszany i orkiestrę. Lou von Salome nie odwzajemniała jednak uczuć Nietzschego i w listopadzie 1882, po nieudanej próbie oświadczyn, nastąpiło rozstanie, a niedoszła narzeczona wyjechała z Rée do Berlina. Wkrótce wyszła za mąż za filologa, Freda Charlesa Andreasa. Na domiar złego siostra Friedricha robiła co mogła, by zniszczyć ten jedyny w jego życiu związek, posuwając się nawet do sfałszowania dwóch listów, rzekomo od byłej ukochanej.
Lou von Salome stała się potem znana jako autorka jednej z pierwszych książek o Nietzschem.
Choroba i śmierć.
Kolejne dzieła Nietzschego, zawierające kontynuację i rozwinięcie myśli wyłożonych w "Zaratustrze", to "Poza dobrem i złem" (1886), "Z genealogii moralności" (1887), "Zmierzch bożyszcz" oraz "Antychryst" (1888). Owocem roku 1888 są też "Dytyramby dionizyjskie", poświęcona Wagnerowi rozprawa "Nietzsche kontra Wagner" i autobiograficzna książka "Ecce Homo", która "post factum" może być odczytana jako zapowiedź tragicznego końca filozofa.
Stale obecna w twórczym życiu Nietzschego choroba rozwijała się bowiem bez przerwy i w końcu przerodziła się w obłęd. Listy Nietzschego z końca grudnia 1888 i początku stycznia 1889 roku wydają się świadczyć o gwałtownej utracie rozeznania w sytuacji i kontaktu z otoczeniem. Trzeciego stycznia 1889 roku nastąpiło załamanie – kilka dni później Nietzsche został przewieziony do kliniki psychiatrycznej w Bazylei, a stamtąd do kliniki w Jenie. W marcu 1890 żyjąca jeszcze matka zabrała go do Naumburga, zaś po jej śmierci w 1897 roku siostra Nietzschego wynajęła dom w Weimarze, gdzie pogrążony w apatii filozof spędził ostatnie trzy lata życia bez kontaktu ze światem. Pochowano go obok ojca na cmentarzu w Röcken.
Wola Mocy.
Książką, wokół której narosło najwięcej kontrowersji, jest wydana po śmierci filozofa "Wola mocy" – zbiór notatek, które częściowo wiążą się tematycznie z wcześniejszymi pracami, a częściowo stanowić mogą wstępną wersję dzieła, które Nietzsche zamierzał napisać. Kontrowersje dotyczą przede wszystkim wykorzystania pewnych pojęć filozofii Nietzschego przez ideologię nazistowską – w szczególności takich pojęć, jak „nadczłowiek” i „moralność panów”.
Po śmierci Nietzschego prawa do jego spuścizny odziedziczyła opiekująca się nim do końca siostra, Elisabeth Förster-Nietzsche. Jej zasługi dla ocalenia dorobku filozofa są trudne do przecenienia, nie ograniczyła się ona bowiem do złożenia spuścizny po bracie w archiwum. Dokonana z jej inspiracji redakcja dzieł Nietzschego, zwłaszcza zaś "Woli Mocy" jest chaotyczna i zawiera wiele fragmentów, które Nietzsche z całą pewnością traktował jedynie jako wstępne szkice. Te fragmenty sprzyjały następnie wulgarnej interpretacji myśli Nietzschego, dokonanej przez filozofów nazistowskich, tym bardziej że Elisabeth nie ukrywała swej sympatii dla Adolfa Hitlera, któremu podarowała pamiątki po bracie: szpadę i laskę. Hitler był też obecny na pogrzebie Elisabeth w 1935 roku. Główną rolę w zawłaszczaniu filozofii Nietzschego przez nazistów odegrali Alfred Baeumler i Alfred Rosenberg. Do postrzegania Nietzschego jako ideologa faszyzmu przyczyniły się lewicowe nurty filozofii, zwłaszcza po II wojnie światowej.
Tym łatwiej przyszło uczynić z Nietzschego proroka faszyzmu, że przewidywał on, jak wielkie będzie kiedyś oddziaływanie jego filozofii.
Poglądy.
Nietzsche był przeciwny filozofii akademickiej, wykładanej "ex cathedra". Jego zdaniem "sens ma jedynie taka filozofia, którą się przeżywa". W "Tako rzecze Zaratustra" pisał o tym:
W swoich dziełach powracał wielokrotnie do kilku fundamentalnych zagadnień, za każdym razem dokonując ich pogłębionej analizy i spoglądając na nie z różnych punktów widzenia. Jednocześnie jego filozofia pełna jest wieloznaczności i podatna na rozmaitość odczytań – takie działanie Nietzschego było głęboko zamierzone, bo "fakty nie istnieją, tylko interpretacje".
Apollińskość i dionizyjskość.
W swej pierwszej książce "Narodziny tragedii" Nietzsche wprowadził koncepcje apollińskości i dionizyjskości. Pojęcia te, pochodzące od imion greckich bogów Apollina i Dionizosa, stanowiły podstawę Nietzscheańskiej metafizyki artysty i wczesnej teorii tragedii.
Apollo był dla Nietzschego symbolem światła, miary i powściągliwości. Apollińskości odpowiada świat zjawisk lub marzeń – jest ona dążnością do obrazowania świata, tworzenia jego pozoru, który przesłania prawdę o życiu i przez to pomaga je znosić.
Przeciwstawieniem apollińskości jest dionizyjskość, żywioł będący istotą życia, dziką nieokreślonością, chaotycznością i nieokiełznaniem. Jako stan upojenia i ekstazy, umożliwia ona doświadczenie rzeczywistości takiej, jaka jest ona sama w sobie. Dionizyjskość jest przejawem przepełniającej życie woli mocy i stałym motywem pojawiającym się w pismach Nietzschego.
Relacja między żywiołem dionizyjskim i apollińskim nie jest taka sama we wszystkich sztukach. Z żywiołu apolińskiego wyrastają sztuki plastyczne i epika, z dionizyjskiego – liryka i muzyka. Do zjednoczenia tych elementów doszło, zdaniem Nietzschego, w starożytnej Grecji w postaci klasycznej tragedii (za pośrednictwem muzyki: według Nietzschego początkiem antycznej tragedii był chór).
Sokrates i Platon.
Zdaniem Nietzschego wzniosła kultura i sztuka Grecji antycznej została zniszczona. Demontażu jej podstaw dokonał Sokrates, a dzieła destrukcji dopełnił Platon. Obaj doprowadzili do przekształcenia żywiołu apollińskiego w realność, absolutyzując pojęcia prawdy, dobra i piękna, podchwycone następnie przez religię chrześcijańską.
Wpływ Sokratesa podsumował Nietzsche w następujący sposób:
Wolna wola.
Nietzsche zaprzeczał prawdziwości pojęcia „wolna wola”, uznawał je za fałszywe. Zdając sobie sprawę z typowych argumentów strony przeciwnej, uważał, że wiara w wolność woli jest na pewno nie do wybronienia. W swych pracach popierał fatalizm, jednak bez jego radykalnych, odwracających się od działania interpretacji. O znaczeniu wolnej woli pisał tak:
Razem z koncepcją wolnej woli sens tracą też, zdaniem Nietzschego, wszystkie koncepcje tzw. moralnego porządku świata – stają się nielogiczne, gdyż opierają się na istnieniu pewnego metafizycznego zadania (zbioru nakazów i powinności), które w ogólności zwykle jest sprzeczne z naturą i ludzkim losem (wdrożenie go wyklucza brak wolnej woli):
Śmierć Boga i nihilizm.
"Gott ist tot" – "Bóg umarł". Zdaniem Nietzschego, człowiek współczesny przestał odczuwać obecność Boga w świecie, w odróżnieniu od ludzi żyjących w starożytności czy choćby w średniowieczu. W owych czasach czyny człowieka pociągały za sobą prędką reakcję Boga w życiu doczesnym, Boga należało się bać tu i teraz. Dziś każdy może liczyć na jego miłosierdzie – rozważa się problem, czy piekło w ogóle istnieje. Komu potrzebny jest jeszcze taki Bóg? Dlatego musiał on przenieść się w sferę abstrakcji, odejść w zapomnienie. Jest jeszcze, lecz jako martwy.
Razem z Bogiem usunięta została z naszego świata metafizyczna podstawa wszelkich wartości, od tej pory zawieszone są one w próżni. Nie ma prawdy, ani dobra, nie możemy więcej się do nich odwoływać, bo umarły razem z Bogiem: on sam umarł wskutek relatywizmu (Nietzsche powiada przenośnie: „z litości do człowieka”). Jedną z bezpośrednich przyczyn końca wiary, jak wprost stwierdził Nietzsche, stała się też nowożytna nauka i w szczególności filologia biblijna: „święte księgi (...) wpadły w końcu w ręce filologów, to znaczy niszczycieli wiary”. Człowiek musi na nowo podjąć trud stanowienia wartości, lecz zdaniem Nietzschego nie jest w stanie. Jesteśmy bowiem tworami chorej kultury, opartej właśnie na odwołaniu do Boga, a nie możemy się do niego odwołać, bo jego już nie ma w naszym świecie. Mówiąc językiem teologii – przestaliśmy odczuwać sacrum, a nie potrafimy bez niego żyć. Jednym z fałszywie interpretowanych wytworów kultury opartej o to odwołanie jest wielki rozrost tego, co nazywa się sumieniem, i ideałów stoickiego spokoju, wolności od wszelkiego niepokoju.
Stąd bierze się nihilizm – udawanie, że nic się nie stało, zaprzeczanie, że istnieją jakiekolwiek problemy. Nihilizm w rozumieniu Nietzschego to akceptacja pozorów, uznanie za wartość tego, co jest już bez-wartością. Łatwe i wygodne życie, przyjemne i pozbawione napięcia. Tak rozumiany nihilizm jest wrogi życiu, bo domaga się wpasowania w obowiązujący system norm oparty na moralności niewolniczej.
W drugiej rozprawie "Genealogii moralności" Nietzsche proponuje swoją teorię tego, czym jest tzw. nieczyste sumienie (jeśli nie jest po prostu objawem fizjologicznym, pewnym rodzajem naturalnego wrażenia): jest to mianowicie, jego zdaniem, specyficzne życie wewnętrzne, które ma swą przyczynę w uwewnętrznieniu naturalnych sił życiowych i umysłowych, skoro wskutek „udomowienia” człowieka przestały się one uzewnętrzniać, skoro odebrano warunki, w których te naturalne instynkty mogą się przejawiać w świecie. Jest ono więc produktem (nihilistycznej) moralności zaprzeczenia woli, efektem wpisanych w naszą kulturę, pustych już ascetycznych ideałów.
O nihilizmie mówi Nietzsche wtedy, gdy „najwyższe wartości tracą wartość”. Z tego względu mamy w dziejach do czynienia z podwójnym nihilizmem: pierwszym, którego symbolem jest chrześcijaństwo (i jego poprzednicy) – zanik wartości życia, tego, co w życiu było dotychczas ważne – i drugim, z którym dodatkowo mamy do czynienia teraz, gdy tradycyjne formuły teologiczne i uzasadnienia tracą swą moc. W "Woli mocy" rozróżnia nihilizm czynny (burzący), reprezentowany np. przez chrześcijaństwo – walczy on z wartościami, głosząc powściągnięcie woli życiowej, pokój i odwrócenie się od świata doczesnego, czyli jedynego – od biernego (czy znużonego), który już nie walczy (swego rodzaju fatalizm psychologiczny i zobojętnienie): „żyje, jak popadnie”, nie wyznając żadnych wyższych wartości.
Moralność panów i moralność niewolników.
Nietzscheańska analiza moralności wyróżnia dwa zasadnicze jej typy.
Dla moralności panów pierwotnym jest pojęcie dobra, pojmowanego jako wszystko, co wzmaga wolę mocy jednostki. Dobre jest to, co jej służy, wzmacnia ją, daje poczucie siły. Dobry jest wróg, bo można z nim walczyć, dobry jest ból, bo można go znosić, dobry jest wysiłek – dobre jest cokolwiek, co napina wolę mocy. „Zło” jest pojęciem wtórnym i nie oznacza zła „metafizycznego”, a jedynie to, co wolę mocy osłabia. W odniesieniu do człowieka „zły” (czy, w "Genealogii moralności", „kiepski” lub „lichy”) znaczy tu tyle, co „nieudany”, „pogardy godny”.
Moralność niewolnicza bazuje z kolei na pojęciu zła jako pierwotnym, które rozumiane jest jako cierpienie i wysiłek. Złe jest to, co zadaje ból, co zmusza do wysiłku i walki. Dobro jest ulgą w cierpieniu, spokojem. Dla moralności niewolników ważne jest współczucie, cierpliwość, pokora, serdeczność. Cechą charakterystyczną moralności niewolniczej jest resentyment, uraza wobec świata, żal do życia, które nie spełnia oczekiwań. Jest to moralność reaktywna, gdyż za dobre uznaje własne cnoty moralne, a za złe cnoty, których nie może osiągnąć. Spełnia ona dwie funkcje: pozwala uszlachetnić swoją słabość i zemścić się na panach. Jej wcieleniem jest, wedle Nietzschego, moralność chrześcijańska.
Resentyment jest także formą świadomości, strategią działania i postawą względem świata jednocześnie. Resentyment jest wreszcie w "Z genealogii moralności" przejawem historycznego myślenia. Nietzsche pokazuje w swych pracach moralność panów jako pierwotną i niezależną, a moralność niewolniczą jako późniejszą i zależną (swe powstanie zawdzięcza tamtej), jako wynik swoistej ewolucji (którą jednakże Nietzsche ocenia negatywnie). Taka też jest odpowiedź Nietzschego na problem zła i związane z tym kwestie teologiczne: nie ma sensu pytać o "przyczynę" „zła”, gdyż jest ono w tej filozofii naturalne; sens ma jedynie pytanie o to, "dlaczego" uważa się je za zło, tj. kiedy i z jakich przyczyn zaczęto odpowiednie postępowanie coraz bardziej masowo potępiać oraz skąd wzięły się współczesne wartościowania, reprezentowane przez słowo „dobro”.
Moralność człowieka jest zawsze wypadkową obu typów, nie ma typów czystych, a różnice występują jedynie w przewadze jednego nad drugim.
Kontrowersje budziła interpretacja nietzscheańskiego podziału na moralność panów i niewolników: niemiecki filozof określał panów jako:
 Oryginalne brzmienie „płowej bestyi” ("blonde Bestie") prowokowało część filozofów (zwłaszcza nazistowskich) do odczytywania tego fragmentu jako gloryfikacji określonego typu biologicznego, a choć nie ma wiele wspólnego z nietzscheańskim rozumieniem nadczłowieka, „zdradza ślady tęsknoty wyrafinowanego intelektualisty za barbarzyńską prostotą i nagą siłą”.
Wola mocy.
Wola mocy to jedno z centralnych pojęć filozofii Nietzschego, a jednocześnie jedno z mniej dopracowanych. Pewne fragmenty pism filozofa pozwalają na interpretację woli mocy w sposób podobny do Heraklitejskiego ognia, który bezustannie przeprowadza jedne rzeczy w inne i jest podstawą istnienia świata, wydaje się jednak, że dla późnych poglądów Nietzschego reprezentatywne jest wyobrażenie woli mocy jako siły, której przejawem jest wszelkie życie i jego dążenia. Siła ta znajduje swe potwierdzenie i spotęgowanie w samej sobie w momentach najwyższej mobilizacji i przełamywania oporów. W życiu człowieka wola mocy wyraża się poprzez poczucie siły jednostki i panowania nad sobą, w wytrwałości i zwycięskim znoszeniu przeciwieństw.
Wola mocy u Nietzschego odpowiada mniej więcej temu, co inni nazywają „istnieniem”: jest to pojęcie pierwotne, w najlepszym razie może zostać zdefiniowane jako „wywierać wpływ”. Nie ma istnienia bez woli mocy; ona sama zaś jest jedyną rzeczywistością. Nowa nazwa pociąga za sobą inne psychologiczne skojarzenia, a jednocześnie odrzuca pewne elementy metafizyczne, które tkwią w mentalności europejskiej: np. bytowanie niezależne od czasu, w ogóle wszelka niezmienność, nadto pierwsza przyczyna (w tym ujęciu ruch w świecie nie potrzebuje przyczyny, gdyż cały świat jest pewnym "quantum" woli mocy: spoczynek "nie" jest stanem normalnym) czy wreszcie nieskończona siła jako „z pojęciem siły nie dająca się pogodzić” (WM 380). Świat jest stawaniem się, ścieraniem się sił (ich wzajemny stosunek decyduje o prędkości procesu, o rozkładzie czasowym): to jest jedyna realność. Ponieważ Nietzsche postrzega koncepcje podmiotu (zwłaszcza tzw. rzeczy samej w sobie) jako wtórne, więc logicznym wydaje się, że traktował wolę mocy jako coś w rodzaju rozproszonego i wszechogarniającego pola fizycznego, z lokalnymi skupiskami.
Uzasadnione wydaje się traktowanie samej mocy odpowiednio do jej sensu fizycznego (energia wydatkowana w czasie), zaś woli mocy jako przyrostu mocy w czasie (kolejna pochodna) – jest to jednak kwestia sporna. Natomiast bezspornie Nietzsche uważa kryteria przyjemny-nieprzyjemny u istot świadomych jako towarzyszące woli mocy (wprawdzie nie są one traktowane opozycyjnie), np. jako potwierdzenie wzrostu: jest to opisane w WM.
Wola mocy nie jest wolą życia, bo to sztuka przetrwania za wszelką cenę, a poczucie mocy może osiągnąć apogeum w chwili świadomej śmierci (koncepcja woli mocy odrzuca właśnie samozachowanie, utrzymanie tego, co jest, jako esencję życia – w istocie porzuca się tu jakikolwiek kres, stan ustalony). Nie jest też pragnieniem panowania nad innymi, bo często większe poczucie mocy daje służebność. Wola mocy w życiu człowieka to raczej chęć zwyciężania, pragnienie i uczucie posiadania siły zdolnej łamać wszelkie przeszkody, połączone z radością zwycięzcy i twórczą ekstazą. To pęd ku sile. Np. chrześcijańska moralność w oczach Nietzschego jest z jednej strony wielką siłą społeczną, z drugiej jednak – patrząc od strony ogólnego wpływu na jednostkę – jest „niemocą mocy”, samoograniczeniem, zanikiem, przeciwieństwem życia (Z tego też wynika, że „istnieją zamiary i postępki tylko niemoralne”, WM 354).
Tak rozumiana wola mocy jest jednym z kilku pojęć, na których Nietzsche opierał swą filozofię pozytywną.
Afirmacja i umiłowanie losu.
Przeciwieństwem resentymentu, które w swych książkach Nietzsche przedstawia jako „boski” czy „nadludzki” sposób oceny, jest afirmacja życia, to znaczy zrozumienie konieczności tego, co było, i uznanie wszystkich dotychczasowych wydarzeń za pożądane ze względu na ich skutek, jakim jest ten, który je ocenia – a który sam siebie uważa za udanego. Nie można wszak chcieć, by w świecie przenikniętym współzależnością wszystkiego od wszystkiego przeszłe uwarunkowania obecnego stanu rzeczy się zmieniły, bez przekreślania samego tego obecnego stanu. Jest to więc koncepcja "amor fati" (umiłowania losu):
Nieprzypadkowo Nietzsche nie posługuje się tu pojęciem „zło”, a jedynie „brzydota”. Stosowanie kryteriów estetycznych zamiast moralnych (tj. potępiających i dążących do zniszczenia czegoś) przyświeca praktycznie całej jego twórczości (immoralizm). Taka optyka jest obca ludziom opanowanym przez resentyment i moralność niewolniczą: skupiając się na swej krzywdzie, najchętniej usunęliby z historii pewne wydarzenia, tym samym jednak także przekreślając nie tylko siebie w swym obecnym "i przyszłym" kształcie, ale i w ogóle wszystko, co jak dotąd miało i "będzie mieć" miejsce – jest to postawa nihilistyczna, odwracająca się od życia.
Słynny stał się aforyzm Nietzschego zatytułowany „Ze szkoły wojennej życia”: „Co mnie nie zabija, czyni mnie silniejszym”.
Wieczny powrót.
Koncepcje wiecznego powracania świata pojawiały się już w starożytności, u Heraklita z Efezu, którego Nietzsche cenił wysoko, i stoików, jednak Nietzschego nie interesował kosmiczny wymiar tej teorii, a wyłącznie jej konsekwencje dla ludzkiego życia. W opinii wielu filozofów nauka o wiecznym powracaniu jest zwornikiem, łączącym poszczególne propozycje pozytywne Nietzschego. Niestety, nauka ta przedstawiona jest jedynie jako zapowiedź w "Wiedzy radosnej" i rozwinięta niewiele bardziej w "Tako rzecze Zaratustra". Trudno więc z całą pewnością stwierdzić, jak Nietzsche mógł ją rozumieć i wykorzystać w przyszłych dziełach.
Cytat ten ma uświadomić fundamentalne znaczenie nauki o wiecznym powrocie dla ludzkiego życia i sposobu w jaki się je przeżywa – oto bowiem w każdej chwili dokonujemy wyboru na wieczność.
Wydaje się, że aby w pełni zaakceptować tak rozumiany wieczny powrót, nie można żyć na sposób, jaki zaszczepiany jest każdemu z nas w procesie socjalizacji. Zobowiązania społeczne, zasady moralne, głęboko odczuwane pojęcie „ja”, wreszcie bezustanne odnoszenie życia do czasu, czynią akceptację wiecznego powrotu skrajnie trudną, jeśli nie niemożliwą. Zadaniu temu, zdaniem Nietzschego, sprostać mógłby jedynie nadczłowiek.
Nadczłowiek.
Również koncepcji nadczłowieka (niem. "Übermensch") Nietzsche nie zdążył wypracować w jasnej formie. Pojawia się ona jedynie w zarysie i pod postacią metafor w "Zaratustrze" i "Woli mocy". Niektóre jej interpretacje odwołują się do nauki o wiecznym powrocie i przedstawiają nadczłowieka jako istotę, która może zaakceptować wieczny powrót. Oznacza to pełną zgodę na powtarzalność całej historii własnej osoby i świata, włącznie z wszelkimi dokonanymi w niej gwałtami i mordami, ale i pełnią twórczej ekstazy. Trudno jednak wydać zgodę na gwałty, wartościując czyny z punktu widzenia powszechnie przyjmowanych zasad moralnych – aby to uczynić należałoby wpierw dokonać przewartościowania wartości w duchu nietzscheańskim.
Nietzscheański nadczłowiek miałby być istotą obdarzoną wielką wolą mocy, jednostką twórczą, której każda chwila życia jest doskonale pełna i warta ponownego przeżywania. Istota ta żyłaby poza zasadami obowiązującymi resztę ludzkości, poza rządzącymi nią obecnie prawami (słów „nadczłowiek” lub „rasa silniejsza” używa Nietzsche w opozycji do „człowiek średni” dzisiejszych czasów, zwłaszcza do jego moralności). Jednak, w ujęciu Nietzschego ukazanym w "Poza dobrem i złem", żyłaby ona jak najbardziej właśnie w tym świecie – w przeciwieństwie do tęskniącej za „zaświatami” reszty.
Zdaniem Nietzschego, w historii ludzkości nadczłowiek pojawiał się kilkakrotnie, zawsze jako efekt przypadku i zawsze dla współczesnych był uosobieniem zła, jawiąc się jako potwór. Jako przykłady nadczłowieka filozof podawał Aleksandra Wielkiego, cesarza Fryderyka II, Cezara Borgię i Napoleona. Twórcze zadanie ludzkości polegałoby na „hodowli nadczłowieka”, jako jej apoteozy.
Jednak, wbrew określeniu „hodowla nadczłowieka”, jest on ideałem indywidualnym – nie gatunkowym. Nadczłowiek to ktoś, kto stale przezwycięża siebie (Giorgio Penzo stwierdził, że Nietzsche wprowadził w ten sposób nowy rodzaj transcendencji, której istotą jest przekraczania samego siebie – transcendencję egzystencjalną). Dlatego też nie można określić istoty nadczłowieka – jest on nieustannym przezwyciężaniem, które nie ma skończonego, określonego celu.
Koncepcję tę częściowo wykorzystali naziści: oczywiście pojmując ją odpowiednio do swoich celów, nie zawsze zgodnie z pierwotną myślą Nietzschego. Doprowadziło to do stworzenia terminów „czystość rasowa” i „podczłowiek”, których sam filozof nigdy nie użył, choć w okresie nazistowskim uznane były one za naturalną konsekwencję nietzscheańskich tez. Nadludźmi, wedle tego systemu, mieli być aryjczycy, którzy swoje „dobre” geny przekazywali przyszłym pokoleniom, tworząc nową, lepszą rasę – zjawiskiem towarzyszącym była wówczas eugenika i zamiar eksterminowania ze społeczeństwa jednostek słabszych.
Dekadencja.
Charakterystyczna dla myśli Nietzschego jest ocena, że cywilizacja Zachodu znalazła się w aksjologicznym upadku. Przyczyną jest moralność pochodząca z chrześcijaństwa, odwracająca się od życia doczesnego i woli mocy, a głosząca apoteozę pokoju, wyzucie się z siebie (tu np. „czyste, wolne od woli poznanie”), służbę narzuconym i heteronomicznym obowiązkom moralnym oraz szeroko pojęte współczucie i zaprzeczenie zasadzie selekcji w społeczeństwach. Nietzsche twierdził, że zaprzeczenie woli można znaleźć na dnie wszelkich popularnych kazań moralnych: „wszystkie wartości, w których ludzkość teraz swe najwyższe pożądania zebrała, są wartościami "décadence"”. Takie wartości cechuje „utrata instynktów”, a ich reprezentant będzie często „wybierać, przedkładać nad inne to, co dlań szkodliwe”. Przyczyną dekadencji u indywiduum jest w dużej mierze (często niezauważalna) fizjologiczna degeneracja, zaś wartości dekadenckie (naturalnie zresztą głoszone lub akceptowane z zewnątrz przez takie upośledzone osobniki) prowadzą w dziejach ludów do dodatkowego narastania odpowiedniego biologicznego zepsucia.
Szczególnie krytycznie Nietzsche oceniał liczne idee dochodzące do głosu w nowożytnej filozofii, takie jak demokratyzm czy równość płci.
Krytyka etyki. Przewartościowanie wartości.
Świat, w jakim żyjemy, nie tylko nie składa się z samego dobra, ale nawet przez prawie 2000 lat nie udało się go idealnie dobrym uczynić. Kwestia problemu zła nurtowała wielu filozofów, a Nietzsche daje na nią dosyć oryginalną, jak na swą epokę, odpowiedź. O ile w chrześcijaństwie przyczyną zła jest szatan (lub przypadłości ludzkiego niedoskonałego rozumu i woli wynikające z okoliczności), o tyle dla Nietzschego sama idea dobra jest wątpliwa.
Już w "Poza dobrem i złem" (na samym początku tej książki) podaje on w wątpliwość postulowane przeciwieństwo dobra i zła: po pierwsze, czy w ogóle można mówić sensownie o takim przeciwstawieniu charakteru ludzkich uczynków; po drugie, czy przeciwstawienia takie nie są tylko chwilowymi osobistymi perspektywami, niemającymi nic wspólnego z istotą świata. Sam świat jest bowiem, zdaniem Nietzschego, na moralność obojętny i kieruje się swymi prawami „poza dobrem i złem”; w tym sensie ona sama jest rzeczą nierealną, nie mającą oparcia w rzeczywistości. Nietzscheańska krytyka etyki opiera się na kilku powtarzających się motywach, m.in.:
Nietzsche nie ukończył dzieła, które miało być propozycją przewartościowania wartości. O tym, że miało powstać, wiadomo jedynie z luźnych zapisków odnalezionych w spuściźnie filozofa. Są podstawy, by sądzić, że przymiarkami do niego były książki "Poza dobrem i złem" oraz "Zmierzch bożyszcz", być może także "Antychryst". Wolno wszakże domyślać się, że chodziło o przywrócenie wartości temu, co w kulturze Zachodu od dawna uchodzi za „złe” – m.in. wyniosłości, rozkoszy, żądzy panowania, samolubstwu, walce (nie o sam pokój) między jednostkami lub ludami, wreszcie uznaniu zasady selekcji jednostek, ludów i kultur najwartościowszych (w tym także zanikania i wymierania tego, co mniej wartościowe) oraz naturalnej nierówności między ludźmi i „niesprawiedliwości”, polegającej na praktycznym zaprzeczeniu idei równych praw. Byłby to wstęp do przezwyciężenia nihilizmu.
Analiza języka, filozofia interpretacji.
Nietzsche zajmował się analizą języka we wczesnych latach siedemdziesiątych, a wyniki swych dociekań opublikował w niewielkiej rozprawie "O prawdzie i kłamstwie w pozamoralnym sensie". Analizując narzędzie, jakim z racji wykonywanego zawodu się posługiwał, zwrócił przede wszystkim uwagę na metaforyczność języka, jego bezustanne odwoływanie się do wspólnych doświadczeń mówiących. Właściwym twórcą świata jest ten, który pierwszy wynajduje metaforę odsyłającą do innych przedmiotów i w ten sposób ukierunkowuje nasze postrzeganie. Jednocześnie następuje tu zafałszowanie świata, bo nazwany byt musi wpasować się w logiczną strukturę języka, zaś świat jest, zdaniem Nietzschego, alogiczny. Oznacza to też, że nie ma żadnej prawdy o świecie, a co najwyżej prawda o języku. Lecz i tu tkwimy w kręgu metafor, a więc "fakty nie istnieją, jedynie interpretacje".
Dla Nietzschego zbiorowym twórcą języka jest społeczeństwo wraz ze swoimi nakazami i normami moralnymi, dlatego też filozof programowo unikał systematycznego, akademickiego wykładu a swoje poglądy wyrażał przy pomocy strumienia aforyzmów, słownych żartów i językowych paradoksów.
Niekiedy uznaje się, że stanowisko twórcy "Tako rzecze Zaratustra" logicznie prowadzi do relatywizmu pojęciowego. Takie jednak ujęcie jest niezgodne z większością współczesnej literatury przedmiotu, w której podkreśla się znaczenie perspektywizmu, tj. tezy, iż wszelkie poznawane obiekty są poznawane z pewnej perspektywy, a nie znikąd, z boskiego punktu widzenia. Innymi słowy, Nietzsche twierdzi raczej, że wszelkie widzenie, wszelkie rozumienie jest perspektywiczne, osadzone w kontekście; taka teza jednak nie jest relatywistyczna w sposób radykalny, a jedynie umiarkowany. Umiarkowany zaś relatywizm nie jest stanowiskiem podlegającym obaleniu na mocy sprzecznych założeń (które przypisuje się radykalnemu relatywizmowi). Krótko mówiąc, Nietzsche mógł posługiwać się pojęciem prawdy bez obawy o popadnięcie w sprzeczność; i pojęciem tym chętnie się posługiwał w celach polemicznych.
Wpływ.
Literatura.
Sam Nietzsche nie doczekał się uznania dla swych idei, jednak ich oddziaływanie na niektóre kręgi społeczne było już na przełomie stuleci znaczne. Chodzi tu przede wszystkim o środowisko pisarzy: nawiązywali do niego bracia Heinrich Mann i Thomas Mann, Rainer Maria Rilke, Hermann Hesse, Paul Heyse, Georg Heym, Georg Trakl, Franz Rosenzweig z twórców niemieckich, Brandes, August Strindberg ze Skandynawów, W.B. Yeats i G.B. Shaw z Irlandczyków, André Gide z pisarzy francuskich, Julius Evola oraz Gabriele d’Annunzio z włoskich. Z pisarzy polskich silnie oddziałał Nietzsche na Przybyszewskiego, Micińskiego, Brzozowskiego, Berenta. Wiele spośród poglądów Nietzschego, zarówno literackich, jak i filozoficznych, znalazło wydźwięk we wczesnej twórczości Antoniego Langego, zwłaszcza z cyklu "Rozmyślań". Do myśli Nietzschego nawiązywał również Jim Morrison (zobacz też: Poezja Jima Morrisona).
Filozofia.
Wpływ Nietzschego na filozofię był wolniejszy, niż na literaturę. Zapożyczenie koncepcji nietzscheańskich widać jednak wyraźnie w psychoanalizie Freuda, którego libido jest zubożoną odmianą woli mocy.
Na początku XX wieku do serii swoich wykładów włączył jego filozofię Georg Simmel, a tuż przed wybuchem I wojny światowej z pozycji chrześcijańskich polemizował z Nietzschem Max Scheler.
Dopiero egzystencjalizm zwrócił poważniejszą uwagę na problemy postawione przez Nietzschego i podjął z nim dialog: prace poświęcone Nietzschemu pisali Martin Heidegger i Karl Jaspers, analizował go Albert Camus. Jacques Maritain próbował godzić Nietzschego z personalizmem, a ze współczesnych za swojego ojca duchowego uznawali go postmoderniści z Rortym i Derridą na czele. Michel Foucault pod koniec swojego życia stwierdził „jestem nietzscheanistą”.
Jawnie inspirowane filozofią Nietzschego jest wczesne stadium obiektywizmu Ayn Rand, jak również kulturalizm Jana Stachniuka.
Przekłady na polski.
Tytuły w nawiasach oznaczają alternatywne tłumaczenia zaproponowane przez współczesnych tłumaczy.

</doc>
<doc id="1685" url="https://pl.wikipedia.org/wiki?curid=1685" title="Fauna ediakarańska">
Fauna ediakarańska

Fauna ediakarańska (, "fauna ediakarska", wcześniej "fauna wendu") – zespół wymarłych organizmów okresu ediakaru, reprezentujący najstarsze znane organizmy wielokomórkowe. Pojawiły się one wkrótce po zakończeniu epoki lodowej okresu kriogeńskiego, i w większości zniknęły przed wzrostem bioróżnorodności znanym jako eksplozja kambryjska. Fauna ediakarańska w niewielkim tylko stopniu przypomina schematy budowy późniejszych organizmów.
Organizmy ediakarańskie pojawiły się około 610 milionów lat temu i pozostawały w rozkwicie do początku kambru 542 milionów lat temu. Nie są spotykane później niż u schyłku ediakaru, pozostawiając po sobie co najwyżej relikty wcześniej kwitnących ekosystemów. Rzadkie skamieniałości mogące do nich przynależeć znajdywano w warstwach środkowego kambru (510–500 milionów lat temu). Sformułowano liczne hipotezy tłumaczące zniknięcie, w tym o braku zapisu kopalnego ("preservation bias"), zmianę środowiska, pojawienie się drapieżnictwa i konkurencji z innymi organizmami.
Niektóre ediakarańskie organizmy mogły być blisko spokrewnione z później znaczącymi grupami; przykładowo "Kimberella" wykazuje pewne podobieństwa do mięczaków, a u innych organizmów występuje symetria dwuboczna. Skamieniałe ślady ryjących, przypominających robaki organizmów prawdopodobnie również zostawiły zwierzęta dwubocznie symetryczne. Jednak większość niemikroskopowych skamieniałości różni się morfologicznie od późniejszych form życia, i przypomina dyski, torby wypełnione mułem oraz pikowane materace. Klasyfikacja jest trudna, a przypisanie niektórych gatunków nawet na poziomie królestwa – zwierząt, grzybów, protistów lub innego – niepewne. Adolf Seilacher uzyskał nawet pewne poparcie dla idei oddzielnego królestwa Vendobionta (teraz zwanego Vendozoa). Ich pozorny brak podobieństwa do późniejszych organizmów spowodował, że niektórzy uznali je za „nieudany eksperyment” wielokomórkowego życia a późniejsze formy wielokomórkowe wyewoluowały niezależnie.
Historia.
Pierwszymi odkrytymi skamieniałościami fauny ediakarańskiej były dyskokształtne "Aspidella terranovica", odkryte w 1868 roku. Ich odkrywca, A. Murray, geodeta i geolog, uznał te skamieniałości za pomocne w ustaleniu wieku skał okolic Nowej Fundlandii. Jednak, jako że warstwa, w której się znajdowały, leżała poniżej „Primordial Strata” (czyli kambryjskich), uważanych wówczas za zawierające pierwsze ślady życia, zajęło cztery lata nim Elkanah Billings odważył się wysunąć hipotezę, że mogą to być skamieniałości organizmów żywych. Ich prosty kształt był powodem, dla którego współcześni odrzucili propozycję, uznając je za struktury powstałe skutek ucieczki gazu, nieorganiczne konkrecje lub nawet sztuczki, spłatane przez złośliwego Boga, aby odwieść ludzi od wiary. Nie znano wówczas podobnych struktur z żadnego innego miejsca, a jednostronna debata wkrótce została zapomniana. W 1933 roku Gürich odkrył skamieniałości w Namibii, ale pogląd o kambryjskich początkach życia sprawił, że przypisano je do kambru i nie powiązano ich z "Aspidella". W 1946 Reg Sprigg odnotował obecność skamieniałości „meduz” w skałach australijskich Wzgórz Ediacara w Górach Flindersa, ale w owym czasie skały te datowano na wczesny kambr, i mimo pewnego zainteresowania odkrycie nie zyskało należytej uwagi.
Dopiero w wyniku odkrycia "Charnii" w 1957 zaczęto poważnie rozważać możliwość, że skamieniałości ediakarańskie zawierają ślady życia. Tę przypominającą kształtem liść paproci skamieniałość odkryto w angielskim lesie Charnwood Forest, a dzięki szczegółowym mapom geologicznym British Geological Survey nie było wątpliwości, że znajdują się w skałach prekambryjskich. W końcu paleontolog Martin Glaessner zauważył związek z wcześniejszymi odkryciami, a połączenie poprawionego datowania istniejących okazów i nowa energia w poszukiwaniach spowodowały, że odkryto wiele innych przypadków.
Wszystkie okazy odkryte przed 1967 pochodziły z gruboziarnistego piaskowca, w którym nie zachowały się drobne szczegóły, co utrudniało interpretację znalezisk. Dokonane przez S.B. Misra odkrycie sfosylizowanych warstw wulkanicznego popiołu w zbiorze skamieniałości w Mistaken Point w Nowej Fundlandii umożliwiło opisanie cech które były wcześniej niewidoczne.
Kiepska komunikacja połączona z problemami w korelacji globalnie różnych formacji, prowadziły do powstania wielu nazw na faunę. W 1960 francuską nazwę „Ediacarien” – pochodzącą od wzgórz Ediacara w południowej Australii, które z kolei zostały nazwane od aborygeńskiego „diyakra” (oznaczającego obecność wody) – dodano do konkurencyjnych form „Sinian” i „Verdian” dla oznaczenia bezpośrednio prekambryjskich skał. Nazw tych używano również względem organizmów żywych. „Ediacaran” i „Ediacarian” były później zastosowane jako nazwa epoki geologicznej i odpowiadających jej skał. W marcu 2004 Międzynarodowa Unia Nauk Geologicznych (IUGS) zakończyła niespójność, formalnie nazywając końcowy okres Neoproterozoiku, od australijskich wzgórz „ediakariańskim”.
Utrwalenie w materiale kopalnym.
Zazwyczaj wszystkie skamieniałości, poza najmniejszymi ich fragmentami, odzwierciedlają twarde elementy szkieletów martwych organizmów. Ponieważ ediakariańskie organizmy miały miękkie ciała, pozbawione szkieletu, ich liczne utrwalenie jest zaskakujące. Niewątpliwie pomógł brak ryjących stworzeń żyjących w osadach; od czasu ich wyewoluowania w kambrze części miękkie były najczęściej naruszane nim uległy fosylizacji.
Maty mikrobiotyczne.
 to obszary osadu ustabilizowane przez kolonie mikrobów, które wydzielają klejące substancje lub w inny sposób wiążą cząsteczki osadu. Wzrost kolonii umożliwia wchłonięcie przykrywającej ją cienkiej warstwy osadu, choć pojedynczy jej członkowie nie przemieszczają się. Jeżeli odłoży się zbyt gruba warstwa osadu, część kolonii ginie, pozostawiając skamieniałość o charakterystycznym wyglądzie pomarszczonej „słoniowej skóry” i gruzełkowatej teksturze. Większość ediakarańskich warstw o takiej strukturze zawiera skamieniałości innych organizmów. Równocześnie skamieniałości takie praktycznie nigdy nie są znajdowane poza matami. Ewolucja organizmów żywiących się matami w kambrze znacząco zredukowała ich powszechność, i są one obecnie ograniczone do niegościnnych ostoi.
Fosylizacja.
Powstanie ediakarańskich skamieniałości jest zagadkowe, jako że miękkie organizmy zazwyczaj ich nie pozostawiają. Inaczej niż późniejsze skamieniałości miękkich organizmów (takie jak te znajdowane w łupkach z Burgess, lub w wapieniu górnojurajskim (tyton) z okolic Solnhofen) organizmy z tego okresu, nie są znajdowane tylko w nielicznych środowiskach o niezwykłych specyficznych warunkach; był to globalny fenomen, a proces, który do tego doprowadził, był powtarzalny i powszechny. Ediakar cechowały zatem odmienne od późniejszych epok warunki. Panuje przekonanie, że skamieniałości powstawały dzięki szybkiemu przykryciu przez popiół lub piasek ciał organizmów, zachowując je w błocie lub macie mikrobiotycznej, na której żyły. Pokłady popiołu utrwalają więcej szczegółów, a ich wiek może być określany z dokładnością do miliona lat lub większą, dzięki datowaniu izotopowemu. Niemniej skamieniałości ediakarańskie częściej są znajdowane pod warstwami piasku naniesionymi przez sztormy lub przydenne prądy zawiesinowe o wysokiej energii, odkładające turbidyty. Współcześnie miękkie organizmy niemalże nigdy nie zachowują się w takich warunkach, ale obecność szeroko rozpowszechnionych mat mikrobiotycznych prawdopodobnie wspomogła fosylizację poprzez stabilizowanie odcisków martwych organizmów poniżej górnej warstwy.
Różnorodność.
Najbardziej znane rodzaje występujące w faunie ediakarańskiej:

</doc>
<doc id="1686" url="https://pl.wikipedia.org/wiki?curid=1686" title="Feniks (ujednoznacznienie)">
Feniks (ujednoznacznienie)



</doc>
<doc id="1687" url="https://pl.wikipedia.org/wiki?curid=1687" title="Fałsz">
Fałsz

Fałsz – jedna z dwóch podstawowych wartości logicznych (drugą jest prawda). Fałsz jest niezgodnością treści sądu (zdania) z tym, do czego się odnosi.
W logice matematycznej fałsz jest jedynie symbolem, nie ma żadnego głębszego znaczenia.
W mowie potocznej i logice tradycyjnej za fałsz uważa się stwierdzenie czegoś, co nie miało miejsca w rzeczywistości lub zaprzeczenie czemuś, co miało miejsce.
W pozytywizmie logicznym uwzględnia się też wartość "not even false", dla twierdzeń nieweryfikowalnych, co oznacza, że dane zdanie nie ma w ogóle sensu faktycznego czyli nie odnosi się do żadnych faktów.
W matematyce przy danym układzie aksjomatów niektóre twierdzenia są nierozstrzygalne, a więc nie są ani prawdziwe ani fałszywe.

</doc>
<doc id="1688" url="https://pl.wikipedia.org/wiki?curid=1688" title="Funkcja Ackermanna">
Funkcja Ackermanna

Funkcja Ackermanna – funkcja matematyczna odkryta przez Wilhelma Ackermanna w 1928 roku. Cechą charakterystyczną tej dwuargumentowej funkcji jest jej nadzwyczaj szybki wzrost. Funkcja Ackermanna jest prostym przykładem funkcji rekurencyjnej, niebędącej funkcją pierwotnie rekurencyjną. Funkcje pierwotnie rekurencyjne to większość znanych funkcji, między innymi dodawanie, funkcja wykładnicza itp.
Funkcja Ackermanna często jest używana do testowania jakości optymalizacji kompilatorów – stosunki czasu wykonania pomiędzy różnymi kompilatorami tego samego języka czasami sięgają tysiąca. Jakość kodu generowana dla tego typu funkcji jest szczególnie ważna w językach funkcyjnych, gdzie używa się rekursji.
Inną funkcją o własnościach podobnych do funkcji Ackermanna (tzn. będąca rekurencyjną i nie pierwotnie rekurencyjną) jest funkcja Sudana.
Definicja.
Matematyczna definicja funkcji opisana jest wzorem formula_1
Własności.
Funkcja Ackermanna jest rekurencyjna.
Schemat dowodu twierdzenia "funkcja Ackermanna nie jest pierwotnie rekurencyjna": definiuje się rodzinę funkcji Ackermanna formula_3 (każda z tych funkcji jest pierwotnie rekurencyjna). Z tego wynika, że każda funkcja pierwotnie rekurencyjna jest majoryzowana przez pewną funkcję Ackermanna. Następnie dowodzi się, że wszystkie jednoargumentowe funkcje Ackermanna będą z kolei majoryzowane przez funkcję formula_4 Wynika z tego, że formula_5 nie jest pierwotnie rekurencyjna.
Dowód przez indukcję:
zgodnie z definicją dla funkcji posiadającej 0 jako pierwszy argument. formula_8 z kolei jest zawsze większe od 0, ponieważ już w przypadku n = 0, n + 1 = 0 + 1 = 1.
pokazujemy poprzez indukcję na argumencie n, że
i poprzez to
wedle definicji funkcji dla argumentu n = 0.
pokazujemy, że
a następnie zgodnie z hipotezą 1:
oraz wedle definicji funkcji dla argumentów różnych od 0:
Łącząc te trzy relacje otrzymujemy dowód twierdzenia:
Tabela wartości.
! "m"\"n"
! 0
! 1
! 2
! 3
! 4
! n
! 0
! 1
! 2
! 3
! 4
\begin{matrix}\underbrace

</doc>
<doc id="1689" url="https://pl.wikipedia.org/wiki?curid=1689" title="Francis Picabia">
Francis Picabia

Francis Picabia, właśc. Francis-Marie Martinez Picabia (ur. 22 stycznia 1879 w Paryżu, zm. 30 listopada 1953 tamże) – francuski malarz, rysownik, ilustrator, grafik i poeta; jeden z najbardziej wpływowych artystów XX wieku.
Życiorys.
Matka była Francuzką, a ojciec, hiszpańskiego pochodzenia, był pracownikiem ambasady Kuby w Paryżu.
Picabia kształcił się w paryskich uczelniach artystycznych. Pod koniec lat 90. XIX w. studiował m.in. u Fernanda Cormona w Państwowej Wyższej Szkole Sztuk Dekoracyjnych, a w latach 1903–1908 znalazł się pod wpływem impresjonistów (Sisley). Od 1909 r. wpływ ma niego zaczęli wywierać kubiści. W latach 1911–1912 dołączył do grupy mającej swoje spotkania w wiosce Puteaux w atelier Jacques’a Villona: Apollinaire, Gleizes, La Fresnaye, Léger, Metzinger. Zaowocowało to stworzeniem grupy "Section d’Or". Picabia znał dobrze Marcela Duchampa.
W latach następnych Picabia odegrał znaczącą rolę w ruchu dadaistycznym zarówno jako artysta plastyk, jak i poeta. Przebywał kilkakrotnie w Nowym Jorku, gdzie był jednym z animatorów dadaizmu – razem z Duchampem założyli czasopismo „291”. W 1916 r. w Barcelonie założył własne pismo dadaistyczne „391”, które ukazywało się nieregularnie do 1924 r. Później przebywał w Zurychu i Paryżu gdzie kontynuował swoje związki i dokonania w ruchu dadaistycznym. W 1921 r. ostatecznie zerwał z dadaizmem, co ogłosił w piśmie „Cannibale”. Po czym na pewien czas, do 1925 r., związał się z surrealistami (André Breton).
W latach 30. obok abstrakcji geometrycznej, zajmował się też malarstwem naturalistycznym i figuratywnym. Przed końcem II wojny światowej powrócił do malarstwa abstrakcyjnego i do poezji.
W 1949 r. w Paryżu w Galerie René Drouin odbyła się retrospektywna wystawa dzieł Francisa Picabii.

</doc>
<doc id="1690" url="https://pl.wikipedia.org/wiki?curid=1690" title="Flaga Chile">
Flaga Chile

Flaga Chile – flaga Chile, w obecnej formie ustanowiona 18 października 1817 r. Jej proporcje wynoszą 2:3.
Kolory flagi chilijskiej symbolizują:

</doc>
<doc id="1691" url="https://pl.wikipedia.org/wiki?curid=1691" title="Flex (program)">
Flex (program)

Flex ("The Fast Lexical Analyzer") – wolna wersja programu lex napisana przez Verna Paxsona (pracownika uniwersytetu Berkeley), udostępniana na licencji BSD.
Program jest używany zwykle w połączeniu z programem bison.
Cechy charakterystyczne programu Flex:

</doc>
<doc id="1692" url="https://pl.wikipedia.org/wiki?curid=1692" title="Faustyna Starsza">
Faustyna Starsza

Faustyna Starsza, "Annia Galeria Faustina" (98/105 – 140/141) – córka Marka Anniusa Verusa (konsula) i Faustyny Rupilii, ciotka Marka Aureliusza. 
Była żoną cesarza Antoninusa Piusa (138–161), matką czworga dzieci (dwóch synów i dwóch córek); w tym najmłodszej Faustyny Młodszej. Wszystkie inne jej dzieci zmarły przed 138, jedno z nich (nieznane z imienia – być może adoptowane) było przodkiem Fabii Orestilli – żony Gordiana I – cesarza w 238 roku.
Wywód przodków:
Małżeństwa i potomstwo:

</doc>
<doc id="1693" url="https://pl.wikipedia.org/wiki?curid=1693" title="Florencja">
Florencja

Florencja (, MAF: []) – miasto w środkowych Włoszech, nad Arno, u stóp Apeninów, stolica Toskanii i prowincji Florencja, stolica Włoch w latach 1865–1871. W 2006 roku liczyła 366 tys. mieszkańców.
Historia.
Założone przez Etrusków jako "Faesulae", zburzone przez Sullę w 82 p.n.e. Następnie odbudowane przez Juliusza Cezara w 59 p.n.e. jako kolonia dla byłych żołnierzy i nazwane "Florentia". Zabudowa tego okresu została założona na planie obozu wojskowego. Z tego okresu pochodzi zachowany szachownicowy układ ulic. Początkowo, w czasach Cesarstwa rzymskiego, Florencja była ośrodkiem handlu i rzemiosła. W 405 obroniła się przed najazdem Ostrogotów, w 539 uzależniona od Bizancjum, a od 541 od Gotów. W 570 została podbita przez Longobardów, którzy z kolei utracili miasto na rzecz Franków.
W XI wieku odzyskała znaczenie jako ośrodek handlu, przede wszystkim z uwagi na swoje położenie – na drodze z Francji do Rzymu. W 1078 rozpoczęto budowę nowych fortyfikacji.
Republika Florencka.
Od 1115 miasto uzyskało status niezależnego miasta, od 1167 weszło w skład Ligi Lombardzkiej, a w 1183 Florencja ogłosiła się komuną miejską. Mimo trwających w tym okresie zatargów pomiędzy stronnictwami gwelfów i gibelinów, licznych sporów i wojen, był to trwający wiele stuleci czas rozkwitu i rozwoju miasta. Oprócz handlu w XII wieku rozwinęło się sukiennictwo, a później także jedwabnictwo. W XIII wieku powstały pierwsze domy bankowe. W 1252 rozpoczęto bicie złotych monet – florenów, które szybko stały się główną monetą Europy. W mieście działali Brunetto Latini, Dante Alighieri, Giotto di Bondone i Arnolfo di Cambio. Jednocześnie narastał konflikt pomiędzy bogatymi kupcami i rzemieślnikami, reprezentowanymi przez cechy, a pozbawioną praw ubogą ludnością.
Na przełomie XIII i XIV wieku było to jedno z największych miast Europy z ponad 100 tysiącami mieszkańców. Zaraza z 1348, która spowodowała dalsze zubożenie społeczeństwa, doprowadziła do wybuchu powstania w 1378, które zmieniło formę sprawowania władzy. Od 1434 miastem począł rządzić ród Medyceuszów, który doprowadził miasto do największej potęgi gospodarczej i kulturalnej (druga połowa XV w.). Pod rządami Wawrzyńca Wspaniałego ("Il Magnifico") miasto wzbogaciło się o znaczną liczbę wielkich dzieł sztuki oraz liczne budowle. Po obaleniu Savonaroli w 1498 faktyczną władzę w Republice Florenckiej jako dożywotni gonfalonier przejął Piero Soderini, a jego bliskim współpracownikiem był Niccolò Machiavelli. W 1512, po obaleniu republiki, do władzy powrócili Medyceusze – od 1569 roku jako Wielcy książęta Toskanii. Ich rządy trwały do 1737 roku.
Wielkie Księstwo Toskanii.
Był to okres największego rozkwitu miasta zarówno pod względem gospodarczym, jak i kulturalnym. W tym czasie tworzyli w nim Leonardo da Vinci i Michał Anioł ("Michelangelo"). Pod koniec XVI wieku powstała tam także grupa kompozytorów, poetów i teoretyków muzyki, zwana Cameratą florencką. Z jej działalnością łączy się też powstanie pierwszej w historii światowej muzyki opery – "Dafne" Jacopa Periego, wystawionej w pałacu Jacopa Corsiego w karnawale 1598.
Po wygaśnięciu dynastii Medyceuszy w 1737 roku Toskania wraz z Florencją została podporządkowana bocznej linii austriackich Habsburgów, a stery władzy przejęła rodzina Lorena (wł. Asburgo-Lorena). W latach 1801–1808 Florencja była stolicą Królestwa Etrurii, następnie ponownie znalazła się pod rządami dynastii habsbursko-lotaryńskiej.
Zjednoczone Włochy.
Od 1860 włączona do Królestwa Sardynii, przejściowo była stolicą Zjednoczonych Włoch w latach 1865–1870.
Podczas II wojny światowej Florencję w marcu 1944 objął strajk generalny ogłoszony przez włoski ruch oporu. W lipcu 1944 miasto doznało poważnych zniszczeń od bombardowań i ostrzału artyleryjskiego w trakcie walk toczonych po obu stronach Arno przez oddziały nowozelandzkie 8 Armii brytyjskiej i lokalne grupy ruchu oporu z wojskami niemieckimi i oddziałami włoskich faszystów.
4 listopada 1966 Florencję nawiedziła wielka powódź wskutek wylania Arno, co spowodowało podtopienie i zniszczenie wielu cennych zabytków. M.in. zalany został most Ponte Vecchio; woda uniosła część biżuterii ze sklepów na moście, którą mieszkańcy próbowali potem wyłowić.
Sport.
We Florencji działa klub piłkarski ACF Fiorentina, który gra w rozgrywkach Serie A.
Gospodarz finału Ligi Światowej siatkarzy w 2014 roku.

</doc>
<doc id="1694" url="https://pl.wikipedia.org/wiki?curid=1694" title="Fentanyl">
Fentanyl

Fentanyl () – organiczny związek chemiczny, pochodna piperydyny; syntetyczny środek przeciwbólowy i anestezjologiczny. Jest agonistą receptorów opioidowych. Pobudza wytwarzanie serotoniny, zmniejsza stężenie endorfin w osoczu. Ma bezpośredni wpływ na ośrodkowy układ nerwowy. Podobnie jak inne opioidy powoduje bardzo silne uzależnienie.
Historia.
Po raz pierwszy otrzymany w Belgii pod koniec lat 50. XX wieku, a wprowadzony do lecznictwa w latach 60.
Analogi.
Znanych jest wiele analogów fentanylu o znaczeniu farmaceutycznym, np. alfentanyl, butyrfentanyl, sufentanyl, remifentanyl i karfentanyl. Wszystkie wykazują podobne właściwości, choć różnią się aktywnością.
Synteza.
Fentanyl otrzymuje się w wieloetapowej reakcji, której substratem wyjściowym jest najczęściej 4-piperydon.
Działanie.
Fentanyl jest bardzo silnym lekiem przeciwbólowym i anestezjologicznym z grupy agonistów receptorów opioidowych (silny agonista receptora μ i słaby δ i κ). Działa natychmiast po podaniu dożylnym (szczyt po około dwóch minutach), ale dosyć krótko (20–30 minut), natomiast przy podaniu doustnym działanie rozpoczyna się po około 30–60 minutach i utrzymuje się do 6 godzin. W małych dawkach wywołuje efekty podobne do innych opioidów: znieczulenie, senność, zanik lęku. Duże dawki stosowane w anestezjologii (najczęściej podawane pacjentom przed zabiegami operacyjnymi) wywołują silne objawy ze strony układu nerwowego.
Występują przede wszystkim:
Działanie przeciwbólowe fentanylu jest związane z odcięciem pacjenta od świata zewnętrznego i skupienie jego uwagi na odbiorze sygnałów z podświadomości, przy czym występuje tzw. sen na jawie. Lek ma niewielki wpływ na układ krążenia. Wykazuje wszystkie typowe dla opioidów działania ośrodkowe. Przy zanikaniu jego działania można odczuwać stany rozdrażnienia, lęku oraz paranoi psychotycznej objawiającej się urojeniami.
Po przedawkowaniu fentanyl powoduje zgon przez depresję układu oddechowego (uduszenie). Dokładne dawki graniczne fentanylu powodujące śpiączkę oraz śmierć nie są obecnie znane.
Stosowane dawki:
W medycynie używany w formie tabletek, roztworu do iniekcji oraz plastrów 12, 25, 50, 100 µg/h (plastry zaczynają działać po około 15 godzinach od przyklejenia i wtedy dopiero odpowiednia dawka dostaje się do krwi) i ampułek 100 i 500 µg do iniekcji.
Inną formą podawania fentanylu są lizaki "Actiq" firmy Cephalon, w których znajduje się cytrynian fentanylu w dawkach od 200 µg do 1,6 mg. Lizaki są produkowane w sześciu dawkach i oznaczone kolorami:
Dawki te wydają się bardzo duże, lecz biodostępność leku przy wchłanianiu przez błonę śluzową ust i po połknięciu nie jest całkowita – wynosi około 33%. Generyczną wersję "Actiq" wprowadził pod koniec września 2006 roku Barr Pharmaceuticals Inc. Lizaki z fentanylem znalazły zastosowanie w taktyce czerwonej (ratowanie rannych na polu walki) w wojsku amerykańskim.
Właściwości farmakokinetyczne
W 84% wiąże się z białkami osocza. Metabolizowany jest głównie w wątrobie (około 90% – N-dealkilacja, utlenianie), metabolity wydalane z moczem, około 10% wydalane w postaci niezmienionej a około 75% podanej dawki jest usuwane z organizmu w ciągu 24 godzin.
Zastosowanie.
Zastosowania fentanylu:
Obecnie około 70% operacji w Stanach Zjednoczonych przeprowadzanych jest z użyciem fentanylu. Jest także stosowany na oddziałach intensywnej terapii w celu uzyskania analgosedacji. Preparaty fentanylu stosuje się również w znieczuleniach przewodowych, na przykład zewnątrzoponowych i podpajęczynówkowych.
Fentanyl może działać uzależniająco i dlatego obrót nim podlega takim samym przepisom jak innych narkotycznych leków przeciwbólowych. Przewlekłe stosowanie fentanylu powoduje wystąpienie tolerancji farmakologicznej.
Przeciwwskazania.
Nie należy stosować fentanylu w przypadku nadwrażliwości na lek, a także podczas jednoczesnego leczenia inhibitorami MAO. Jeśli chory nie jest wentylowany mechanicznie może wystąpić depresja oddechowa i choroby zaporowe płuc. Należy zachować ostrożność u pacjentów z przewlekłą niewydolnością oddechową, miastenią, niedoczynnością tarczycy, ciężkim upośledzeniem czynności wątroby, jak również u pacjentów z astmą (z uwagi na możliwość wystąpienia skurczu oskrzeli). Fentanyl nie może być zastosowany przy operacjach cesarskiego cięcia do czasu wydobycia płodu. Fentanyl przechodzi do mleka matki.
Działania niepożądane.
Depresja ośrodka oddechowego, czasami nudności, wymioty, bradykardia, hipotonia, wyjątkowo skurcz oskrzeli. Po dużych dawkach obserwuje się niewielką sztywność mięśni klatki piersiowej, co może utrudniać sztuczną wentylację.
Interakcje.
Nasila działanie ketaminy oraz leków wpływających depresyjnie na OUN. Alkohol nasila działania niepożądane. Czyste antagonisty receptora opioidowego (np. nalokson) znoszą działanie fentanylu przez wypieranie go z receptorów.
Pozamedyczne zastosowania fentanylu.
Właściwości bojowe.
Istnieją podejrzenia, że fentanyl i być może jego pochodne (3-metylofentanyl) są stosowane przez rosyjskie siły specjalne Specnaz. Według agencji informacyjnych władze rosyjskie potwierdziły użycie tych gazów w 2002 roku w akcji odbijania zakładników przetrzymywanych w teatrze na Dubrowce. Podczas samej operacji zginęło około 100 osób, a po jej zakończeniu zmarło następne kilkadziesiąt. Według władz Rosji przyczyną śmierci nie był sam gaz, lecz osłabienie zakładników.
Właściwości narkotyczne.
Fentanyl (i jego pochodne) jest produkowany nielegalnie (między innymi w Meksyku) i używany jako substytut heroiny. Powoduje błogostan, znieczulenie, przyjemne otępienie, rozluźnienie mięśni, rozleniwienie, senność, delikatną euforię, poza tym posiada charakterystyczne dla innych opioidów działanie powodujące odsunięcie wszelkich problemów, uczucie że zło zniknęło. Sny w trakcie odurzenia fentanylem są niezwykle realistyczne, w niektórych przypadkach są to sny świadome. Właściwości te sprawiają, że jest to niezwykle silnie uzależniająca substancja, która może powodować uzależnienie fizjologiczne.
W celach rekreacyjnych fentanyl przeważnie jest palony, wstrzykiwany lub zażywany doustnie (wchłania się przez błony śluzowe jamy ustnej). Związek ten ma niską temperaturę parowania, około 40 °C. Rozprowadzany jest w formie proszku i roztworu, który podgrzewa się, nie dotykając go ogniem, i wdycha opary (ten sposób zażywania niesie za sobą najmniejsze ryzyko śmierci).
Fentanyl jest niezwykle niebezpiecznym narkotykiem ze względu na ograniczoną możliwość dawkowania. W większych dawkach powoduje senność nie do pokonania oraz wymioty. Przedawkowanie go powoduje depresję ośrodka oddechowego, co prowadzi do śmierci w mechanizmie uduszenia.

</doc>
<doc id="1695" url="https://pl.wikipedia.org/wiki?curid=1695" title="Fotosynteza">
Fotosynteza

Fotosynteza ( – światło, – łączenie) – proces wytwarzania związków organicznych z materii nieorganicznej zachodzący w komórkach zawierających chlorofil lub bakteriochlorofil, przy udziale światła. Jest to jedna z najważniejszych przemian biochemicznych na Ziemi. Proces ten utrzymuje wysoki poziom tlenu w atmosferze oraz przyczynia się do wzrostu ilości węgla organicznego w puli węgla, zwiększając masę materii organicznej kosztem materii nieorganicznej.
Fotosynteza zachodzi w dwóch etapach – faza jasna (określana jako faza przemiany energii), w której światło jest absorbowane, a jego energia jest zamieniana na energię wiązań chemicznych, a jako produkt uboczny wydzielany jest tlen, oraz faza ciemna (określana jako faza przemiany substancji), w której energia wiązań chemicznych, związków powstałych w fazie świetlnej, jest wykorzystywana do syntezy związków organicznych. Obie fazy zachodzą jednocześnie. Wydajność zamiany energii światła na energię wiązań chemicznych węglowodanów wynosi 0,1–8%. W uproszczonej formie sumaryczny przebieg fotosyntezy z glukozą jako syntezowanym węglowodanem zapisuje się:
Najczęściej substratami fotosyntezy są dwutlenek węgla i woda, produktem – węglowodan i tlen (jako produkt uboczny), a źródłem światła – słońce. Zarówno bezpośrednie produkty fotosyntezy, jak i niektóre ich pochodne (np. skrobia i sacharoza) określane są jako asymilaty.
W komórkach eukariotycznych proces fotosyntezy zachodzi w wyspecjalizowanych organellach – chloroplastach, zawierających barwniki fotosyntetyczne. U roślin organami zawierającymi komórki z chloroplastami są głównie liście, będące podstawowymi organami asymilacyjnymi. Pewne ilości chloroplastów zawierają także komórki niezdrewniałych łodyg oraz kwiatów i owoców. Ze względu na rozkład wody i wydzielanie tlenu sinice i fotosyntetyzujące eukarionty zalicza się do organizmów o oksygenicznym typie fotosyntezy, z wydzieleniem tlenu. Wśród bakterii jedynie sinice przeprowadzają fotosyntezę w sposób opisany powyżej. Pozostałe jako donorów elektronów używają związków siarki lub prostych związków organicznych. Tlen w takim przypadku nie jest wydzielany, a proces określa się jako anoksygeniczny typ fotosyntezy.
Organizmy wykorzystujące fotosyntezę.
Dawniej wszystkie organizmy wyposażone w chlorofil zaliczano do jednego królestwa – roślin. Badania ewolucjonistów wykazały, że zdolność do fotosyntezy nie jest dobrym kryterium oceny związków filogenetycznych. Fotosynteza tlenowa (z uwolnieniem tlenu) wykształciła się pierwotnie jedynie u sinic, natomiast aparat fotosyntetyczny organizmów eukariotycznych (chloroplast) jest wynikiem endosymbiozy.
Organizmy uzyskujące energię metaboliczną na drodze fotosyntezy to:
W przypadku protistów i bakterii zdolnych do przeprowadzania fotosyntezy część gatunków może korzystać zarówno z energii światła, gdy jest dostępne, jak i wykorzystywać związki organiczne jako źródło energii, gdy światło nie jest dostępne. Organizmy takie określa się jako miksotrofy.
Ze względu na istnienie ścisłych powiązań symbiotycznych, z efektów fotosyntezy niemal bezpośrednio mogą korzystać porosty, a także organizmy zasadniczo cudzożywne posiadające zoochlorelle, zooksantelle i cyjanelle.
Niektóre ślimaki morskie, np. "Elysia chlorotica", przyswajają chloroplasty z przyjmowanego pokarmu i przechowują w swoich ciele, gdzie nadal zachodzi fotosynteza. Niektóre geny jądrowe komórek roślinnych zostały na drodze poziomego przepływu genów przeniesione do genomu ślimaków, dzięki czemu chloroplasty są zaopatrywane w białka, które pozwalają im przetrwać.
U większości zwierząt i protistów będących gospodarzami dla tzw. kleptochloroplastów symbioza ta nie jest aż tak zaawansowana, gdyż chloroplasty pozbawione produktów genomu jądrowego skonsumowanych glonów są w stanie przeżyć w ciele nowego gospodarza najwyżej 2–3 miesiące.
Stosunkowo często zwierzęta i protisty są gospodarzami dla fotosyntetyzujących jednokomórkowych glonów, a nie tylko chloroplastów. Glony takie nazywa się endofitami, a w zależności od barwy i przynależności taksonomicznej wyróżnia się wśród nich zoochlorelle, zooksantelle i cyjanelle. Stopień współzależności bywa różny – niektóre endosymbiotyczne glony są silnie uwstecznione, zachowując pewną autonomię tak, że istnieje niemal płynna granica między uznawaniem za endosymbionty lub za autonomiczne chloroplasty (np. prochlorofity oraz zawierające nukleomorf chloroplasty kryptomonad i chlorarachniofitów).
Fotosynteza eukariotów.
Na proces fotosyntezy składają się dwa etapy:
Faza jasna.
Zamiana energii światła na energię wiązań chemicznych jest możliwa dzięki absorpcji kwantów światła (fotonów) przez chlorofil. Cząsteczka tego barwnika może absorbować zarówno kwant światła czerwonego (λmaks = 634 nm), przechodząc ze stanu podstawowego (trypletowego) do pierwszego stanu wzbudzonego (pierwszego stanu singletowego), jak i kwant światła niebieskiego, przechodząc ze stanu podstawowego do drugiego stanu wzbudzonego (drugiego stanu singletowego). Drugi stan wzbudzenia jest wyjątkowo nietrwały, cząsteczka chlorofilu szybko przechodzi do pierwszego stanu wzbudzenia, rozpraszając różnicę energii między stanami. Energia pierwszego stanu wzbudzenia może być przekazana poprzez kolejne cząsteczki chlorofilu układu antenowego do centrum reakcji fotoukładu, wybijając z niego elektron. Energia wzbudzonego chlorofilu może być też wyemitowana jako kwant światła, co obserwuje się jako czerwone promieniowanie fluorescencyjne w postaci ciepła. Jego część może być ponownie wykorzystana do fotosyntezy.
Niezależnie od tego czy zostanie pochłonięty kwant światła niebieskiego, czy kwant światła czerwonego, do wybicia elektronu z centrum reakcji używana jest jedynie energia pierwszego stanu wzbudzonego. W absorpcji światła biorą także udział barwniki należące do karotenoidów. Ich maksima absorpcji (400–500 nm) częściowo się pokrywają z maksimum absorpcji chlorofili (barwa niebieska), ale zakres obejmuje również część widma w niewielkim stopniu absorbowaną przez chlorofile (barwa niebieskozielona i zielona). W związku z tym udział karotenoidów zwiększa wykorzystane światła o część pasma słabo absorbowaną przez chlorofile. Cząsteczki karotenoidów znajdujące się w antenach fotosyntetycznych przekazują energię wzbudzenia do centrum reakcji za pośrednictwem chlorofilu. Barwniki pomocnicze znajdują się w kompleksach zbierających światło (ang. "light-harvesting complex" LHC) nazywanych także antenami pomocniczymi. Anteny pomocnicze to kompleksy barwnikowo-białkowe otaczające centrum reakcji, do którego przekazują energię wzbudzenia barwników w nich występujących. LHC II towarzyszący fotoukładowi II składa się z białka o masie 26 kDa, siedmiu cząsteczek chlorofilu a, sześciu cząsteczek chlorofilu b i dwóch cząsteczek karotenoidów. LHC II do pewnego stopnia wykorzystuje też energię cieplną, dzięki czemu możliwe jest wykorzystanie światła o niższej energii. Poza pełnieniem funkcji barwników pomocniczych karotenoidy chronią aparat fotosyntetyczny przed uszkodzeniem w przypadku nadmiaru światła. 
Zakres światła wychwytywany przez chlorofil, tj. 380–710 nm, określany jest jako promieniowanie czynne fotosyntetycznie (ang. "photosynthetically active radiation" PAR). Jest to pewne uproszczenie, gdyż co prawda większość fotoautotrofów wykorzystuje właśnie światło o tej długości fali, mimo to istnieją organizmy, których zakres długości fal światła używanego do fotosyntezy wykracza poza te wartości. Światło słoneczne niesie promieniowanie o różnych długościach fali, a PAR na poziomie morza stanowi ok. 44% jego całej energii. Ponadto dzięki wykorzystywaniu energii cieplnej w fotoukładzie II, może być wykorzystana energia bliskiej podczerwieni. Również PAR jest nierównomiernie wykorzystywane przez różne grupy organizmów, zwłaszcza wodnych. Czerwona i niebieska cześć widma absorbowana przez chlorofile jest w dużym stopniu pochłaniana przez wodę. Sinice i krasnorosty absorbują światło z udziałem kompleksów barwnikowo-białkowych o nazwie fikobilisomy, w których oprócz białek występują barwniki należące do grupy fikobilin. Fikobilisomy podobnie jak układy antenowe LHC przekazują energię wzbudzenia do centrum reakcji zawierającego chlorofil. Pojedynczy fikobilisom zawiera setki fikobilin i wykazuje maksimum absorpcji w zakresie 470–650 nm. Badania "in vitro" na glonach wykazały, że β-karoten przekazuje chlorofilowi 10% energii świetlnej, luteina – 60%, a fukoksantyna – 100%. Badania "in vivo" wykazały, że u zielenic na chlorofil przekazywane jest około połowy energii pochłanianej przez karotenoidy, u sinic – 10–15%, natomiast u glonów zawierających fukoksantynę (stramenopile takie jak okrzemki i brunatnice) – 70–80%. Taką samą wydajność uzyskują glony, u których energia słoneczna wychwytywana jest przez fikobiliny, tj. sinice i krasnorosty. W doświadczeniach na glonach "Chlorella" wykazano, że na 2500 wzbudzonych cząsteczek chlorofilu powstawała jedna cząsteczka O2.
Fosforylacja niecykliczna.
Energia kwantów światła przekazana do centrum reakcji fotoukładu II powoduje wybicie elektronu. Elektron jest przekazywany przez cząsteczkę feofityny, a następnie poprzez cząsteczki plastochinonu połączone z białkami na wolny plastochinol. Powstały wskutek redukcji plastochinonu, plastochinol przemieszcza się w błonie tylakoidu, na drodze dyfuzji, do kompleksu cytochromowego b6f. W obrębie kompleksu cytochromowego b6f zachodzi cykl Q, w wyniku którego dodatkowe protony H+ przemieszczane są ze stromy chloroplastów do wnętrza tylakoidów. Kompleks cytochromowy b6f przekazuje elektron na niewielkie białko zawierające miedź – plastocyjaninę. Odbiorcą elektronów od plastocyjaniny jest fotoukład I, po uprzednim wybiciu elektronów z centrum reakcji. Wybicie elektronu z centrum reakcji fotoukładu I odbywa się poprzez wzbudzenie cząsteczki chlorofilu. Elektron wybity z centrum reakcji fotoukładu I przekazywany jest na cząsteczkę NADP+, która staje się formą zredukowaną NADPH. W przekazaniu elektronu na cząsteczkę NADP+ bierze udział kilka przekaźników, między innymi cząsteczka witaminy K (filochinon) oraz ferredoksyna. Miejsce po elektronie oderwanym z centrum reakcji fotoukładu II zapełniane jest przez elektron oderwany z wody. Reakcja ta jest przeprowadzana przez kompleks rozkładający wodę. Po oderwaniu 4 elektronów następuje rozszczepienie 2 cząsteczek wody na 4 protony i cząsteczkę tlenu. W wyniku uwalniania protonów z rozkładu wody wewnątrz tylakoidu (lumenie), pobierania protonów podczas redukcji NADP+ w stromie chloroplastu oraz transportu protonów w cyklu Q, ze stromy do wnętrza tylakoidu, powstaje gradient protonowy – różnica stężeń protonów na zewnątrz i wewnątrz tylakoidu. Gradient protonowy jest wykorzystywany przez kompleks syntazy ATP do wytwarzania drugiego produktu fazy jasnej – ATP. Opisany szlak wędrówki elektronów z cząsteczki wody na cząsteczkę NADP+ określa się jako fosforylację niecykliczną.
Fosforylacja cykliczna.
W okresie zwiększonego zapotrzebowania na ATP elektron z ferredoksyny może zostać przeniesiony nie na NADP+, lecz na kompleks cytochromowy b6f, by następnie poprzez plastocyjaninę powrócić do centrum reakcji fotoukładu I. Takiemu cyklicznemu transportowi elektronów towarzyszy przenoszenie protonów przez błonę tylakoidu, wytwarzanie gradientu stężeń protonów i syntaza ATP, nie powstaje jednak NADPH. Opisany szlak wędrówki elektronu nosi nazwę fosforylacji cyklicznej.
Faza ciemna.
Związki będące produktami fazy ciemnej fotosyntezy zostały szczegółowo poznane dzięki badaniom Melvina Calvina i Andrew Bensona, za co w 1961 roku Melvin Calvin otrzymał nagrodę Nobla w dziedzinie chemii. Badania te wykazały, że izotop węgla 14C podawany organizmom fotosyntetyzującym pojawia się najpierw w związku trójwęglowym – kwasie 3-fosfoglicerynowym. Z tego powodu rośliny, u których pierwszym produktem asymilacji CO2 jest związek trójwęglowy, określa się jako "rośliny typu C3".
Faza karboksylacji.
Dwutlenek węgla przyłączany jest do 1,5-bisfosforybulozy. Enzymem katalizującym przyłączenie cząsteczki CO2 jest karboksylaza 1,5-bisfosforybulozy określana też jako karboksydysmutaza lub RuBisCO ( "ribulose bisphosphate carboxylase-oxygenase"). Enzym ten jest jednym z najbardziej rozpowszechnionych białek w przyrodzie. W wyniku przyłączenia cząsteczki CO2 do 1,5-bisfosforybulozy powstaje nietrwały związek sześciowęglowy – 1,5-bisfosfo-2-karboksy-3-ketoarabitol, który niemal natychmiast rozpada się na dwie cząsteczki kwasu 3-fosfoglicerynowego.
Faza redukcji.
Kwas 3-fosfoglicerynowy jest fosforylowany ze zużyciem ATP powstającego w fazie jasnej do kwasu 1,3-bisfosfoglicerynowego. Drugi produkt fazy jasnej jest z kolei zużywany w reakcji redukcji kwasu 1,3-bisfosfoglicerynowego do aldehydu 3-fosfoglicerynowego.
Faza regeneracji.
Z aldehydu 3-fosfoglicerynowego oraz pozostającego w stanie równowagi izomeru – fosfodihydroksyacetonu w cyklu reakcji (patrz schemat) z udziałem enzymów przenoszących części łańcuchów węglowych odtwarzany jest akceptor CO2 1,5-bisfosforybuloza. Po związaniu 6 cząsteczek CO2 z cyklu może zostać wyprowadzona 1 cząsteczka heksozy.
Reakcje te zachodzą w stromie chloroplastów i są określane jako cykl Calvina-Bensona. Jest to tzw. faza bezpośrednio niezależna od światła fotosyntezy. Należy jednak podkreślić, że światło stymuluje również niektóre enzymy cyklu Calvina-Bensona poprzez utrzymywanie w stanie zredukowanym ich grup sulfhydrylowych.
Modyfikacje fotosyntezy.
Fotosynteza C4 (cykl Hatcha-Slacka).
Fotosynteza C4 to proces wiązania dwutlenku węgla u roślin określanych nazwą rośliny C4. Rośliny te wykształciły mechanizmy anatomiczne i fizjologiczne pozwalające na zwiększenie stężenia CO2 w komórkach, w których zachodzi cykl Calvina-Bensona.
Przystosowania anatomiczne polegają na zróżnicowaniu komórek zaangażowanych w wiązanie CO2 na komórki mezofilowe oraz komórki pochew okołowiązkowych. Komórki pochew okołowiązkowych posiadają grubą ścianę komórkową, zwykle wysyconą suberyną, dzięki czemu ściana komórkowa jest w bardzo małym stopniu przepuszczalna dla gazów. Proces wiązania CO2 przebiega w komórkach mezofilu, gdzie dwutlenek węgla przyłączany jest do fosfoenolopirogronianu. W reakcji tej powstaje związek czterowęglowy – kwas szczawiooctowy. Jest on w zależności od gatunku rośliny przekształcany do asparaginianu lub jabłczanu i w tej postaci przenoszony do komórek pochew okołowiązkowych. Tam zachodzi reakcja dekarboksylacji i wydzielenie CO2, który jest włączany do cyklu Calvina-Bensona. Cykl ten zachodzi tylko w komórkach pochew okołowiązkowych, gdzie stężenie CO2 przekracza 10–20 razy stężenie CO2 w komórkach mezofilu. Fotosynteza C4 jest zatem sposobem zagęszczania CO2 w tych komórkach, gdzie zachodzi cykl Calvina-Bensona (w komórkach pochew okołowiązkowych). Przy zwiększonym stężeniu CO2 druga reakcja katalizowana przez enzym RuBisCO – przyłączanie do 1,5-bisfosforybulozy tlenu – rozpoczynająca szlak metaboliczny o nazwie fotooddychanie praktycznie nie zachodzi. Proces fotosyntezy u roślin C4 przebiega wydajniej. CO2 nie jest tracony w procesie fotooddychania, jednak nakład energetyczny na związanie jednej cząsteczki CO2 jest większy niż u roślin C3.
Rośliny C4 podzielono na trzy podtypy:
Kryterium podziału jest rodzaj enzymu odpowiedzialnego za przeprowadzenie reakcji dekarboksylacji w komórkach pochew okołowiązkowych. Jest to odpowiednio: enzym jabłczanowy (ME) zależny od NADP, enzym jabłczanowy zależny od NAD i karboksykinaza fosfoenolopirogronianu (PEP-CK). Do roślin C4 należą gatunki z wielu rodzin, np.: kukurydza, trzcina cukrowa, proso zwyczajne, sorgo, występujących w klimacie zwrotnikowym. Wiele z nich występuje jednak w warunkach klimatu umiarkowanego (w Europie ponad 100 gatunków w stanie naturalnym).
Fotosynteza C3-C4.
Fotosynteza C3-C4 zachodzi u roślin, u których pierwszym produktem asymilacji CO2 jest związek czterowęglowy, lecz reakcje cyklu Calvina-Bensona zachodzą zarówno w komórkach mezofilu, jak i komórkach pochew okołowiązkowych. Rośliny o fotosyntezie pośredniej między C3 a C4 posiadają anatomiczne zróżnicowanie komórek na komórki mezofilowe i komórki pochew okołowiązkowych.
Wśród glonów występują gatunki zdolne zarówno do asymilacji dwutlenku węgla przez RuDP, jak i PEP, a więc przechodzące w zależności od warunków z jednego typu do drugiego, takie jak zielenica chlorella zwyczajna ("Chlorella vulgaris") lub "Porphyridium cruentum" (krasnorost).
Fotosynteza CAM.
U roślin z rodziny gruboszowatych ("Crassulaceae"), wykryto po raz pierwszy specyficzny przebieg fotosyntezy, nazwany fotosyntezą CAM (" Crassulacean Acid Metabolism") – kwasowy metabolizm węgla gruboszowatych. Proces ten zachodzi także np. u ananasów i licznych sukulentów z różnych rodzin botanicznych, a także roślin zasiedlających kwaśne wody, np. jeziora lobeliowe. Rośliny te w dzień zamykają szparki, przez co wymiana gazowa jest ograniczona, a woda zatrzymywana w tkankach, natomiast w nocy otwierają je, a pochłonięty CO2 jest przyłączany do fosfoenolopirogronianu (podobnie jak u roślin C4), w wyniku czego tworzy się jabłczan, który magazynowany jest w wakuoli. W ciągu dnia, gdy rośliny mogą wykorzystywać energię światła słonecznego w fazie jasnej fotosyntezy, pochodzący z rozkładu jabłczanu CO2 zasila cykl Calvina. Przez to roślina może prowadzić fotosyntezę przy zamkniętych aparatach szparkowych.
Pseudocykliczny transport elektronów.
Przy znacznym natężeniu światła w chloroplastach zużywany jest nie tylko CO2, ale także O2. Tlen może być redukowany do nadtlenku wodoru H2O2 z udziałem fotoukładu I. Przeniesienie elektronów na O2 zamiast na NADP+ nosi nazwę reakcji Mehlera. Tworzenie reaktywnych form tlenu O2− oraz H2O2 ma miejsce przede wszystkim w chloroplastach ze zbyt małą ilością NADP+. Zbyt mała ilość NADP+ występuje głównie podczas oświetlania roślin natężeniem światła, kiedy fotosyntetyczny transport elektronów przebiega bardzo wydajnie, niemal cała pula NADP+ obecna w chloroplastach pozostaje w stanie zredukowanym. W takiej sytuacji elektrony mogą być przekazywane na tlen, co prowadzi do powstawania gradientu protonów i syntazy ATP, zapobiegając jednocześnie uszkodzeniu fotoukładów. Ze względu na syntazę ATP przy braku syntezy NADPH taki transport elektronów jest określany jako pseudocykliczny transport elektronów. Badania wykazały, że poza ochroną fotoukładów transport elektronów na O2 reguluje interakcje pomiędzy cyklicznym a niecyklicznym transportem elektronów. Chociaż stężenia tlenu nie wymienia się zwykle jako czynnika wpływającego na natężenie fotosyntezy, to w sytuacji intensywnego oświetlenia w wyniku fotolizy wody stężenie tlenu w komórkach jest na tyle duże, że reakcja Mehlera ma znaczący udział w wydajności całego procesu fotosyntezy.
Wydajność energetyczna fotosyntezy.
Całkowite utlenienie glukozy do CO2 i H2O w reakcji:
C6H12O6 + 6O2 → 6CO2 + 6H2O
prowadzi do zmiany energii swobodnej ΔG°'= 2796 kJ mol −1.
Wytworzenie jednego mola glukozy w reakcjach cyklu Calvina zgodnie z danymi przedstawionymi na schemacie wymaga 12 moli NADPH i 18 moli ATP. Utlenienie NADPH do NADP+ odpowiada ΔG°'=220 kJ mol −1. Dla reakcji hydroliza ATP do ADP i Pi ΔG°'=31 kJ mol −1. Zatem 12 moli NADPH stanowi (12 × 220 kJ) 2640 kJ, a 18 moli ATP (18 × 31 kJ) 558 kJ. W sumie do wytworzenia jednego mola glukozy zostaje zużyte 3198 kJ. Wydajność energetyczna cyklu Calvina-Bensona wynosi więc 87%. Do związania jednej cząsteczki CO2 konieczna jest absorpcja minimum 8 kwantów światła (4 kwanty na każdy z fotoukładów). Powstanie jednej cząsteczki glukozy wymaga związania 6 cząsteczek dwutlenku węgla, potrzebne jest zatem 48 kwantów światła. Jeden mol kwantów światła czerwonego to 176 kJ. Na wytworzenie 1 mola glukozy potrzebne jest więc (48 × 176 kJ × mol−1) 8448 kJ. Wydajność energetyczna całego procesu fotosyntezy dla światła czerwonego wynosi więc 33%. Chlorofil może absorbować zarówno kwanty światła czerwonego, jak i niebieskiego, którego mol kwantów niesie energię 268 kJ. W tym przypadku na wytworzenie glukozy potrzebne jest (48 × 268 kJ × mol−1) 12864 kJ energii, a wydajność procesu wynosi 22%. Dla światła fioletowego teoretyczna wydajność zmiany energii światła na energię wiązań chemicznych wynosi ok. 19%. W komórce w absorpcji światła biorą udział chlorofile oraz inne barwniki asymilacyjne przekazujące energię do centrów reakcji z różną efektywnością, zależną od warunków, dlatego dokładne określenie wydajności całego procesu nie jest możliwe. Efektywność zamiany energii docierającej do roślin jako światło na energię zgromadzoną w biomasie wynosi od 3 do 6%, w zależności od typu fotosyntezy.
Fotosynteza u prokariontów.
Do organizmów prokariotycznych zdolnych do korzystania ze światła jako źródła energii należą: sinice, bakterie zielone, bakterie purpurowe, heliobakterie oraz należące do odrębnej domeny wykorzystujące światło w odmienny sposób niż pozostałe organizmy fotosyntetyzujące halobakterie.
Oksygeniczny (tlenorodny) typ fotosyntezy, z wydzieleniem tlenu, występuje jedynie u sinic. Przebieg fotosyntezy u tych bakterii nie różni się znacząco od przebiegu fotosyntezy u roślin. Charakterystyczną cechą sinic są układy antenowe zawierające jako barwnik pomocniczy fikobiliny i związane z nimi fikobiliproteiny takie jak fikocyjanina lub fikoerytryna. Układy antenowe sinic określane są jako fikobilisomy. Niektóre z sinic w określonych warunkach mogą także przeprowadzać anoksygeniczą fotosyntezę z wykorzystaniem jako donora elektronów H2S .
Pozostałe bakterie wykazują wyłącznie anoksygeniczny typ fotosyntezy, bez wydzielania tlenu. Łańcuch transportu elektronów może przypominać albo fotoukład I – tak jest w przypadku bakterii zielonych siarkowych i heliobakterii, albo fotoukład II – tak jest w przypadku bakterii purpurowych i bakterii zielonych nitkowatych.
Bakterie zielone i heliobakterie.
Bakterie zielone siarkowe mogą wykorzystywać jako źródło elektronów wodór, siarkowodór, tiosiarczan, a nawet pierwiastkową siarkę. Kompleksy antenowe tych bakterii zawierają bakteriochlorofile a, c, d i e. W skład centrum jedynego fotoukładu wchodzi bakteriochlorofil a. Elektron wybity przez światło z fotoukładu przenoszony jest kolejno na bakteriochlorofil 663, centrum żelazo-siarkowe, ferredoksynę, mononukleotyd flawinowy i ostatecznie redukuje nukleotyd nikotynoadeninowy (NAD+). Transport elektronów w tym procesie ma charakter linearny, a układ przenośników obecnych w fotoukładzie jest zbliżony do fotoukładu I obecnego u roślin.
Istnieje także możliwość powrotu elektronu do centrum reakcji poprzez menachinon (witamina K2), kompleks cytochromowy bc i cytochrom c. Podczas takiego cyklicznego transportu elektronów następuje przenoszenie protonów w poprzek błony plazmatycznej i wytworzenie siły protonomotorycznej, koniecznej do syntazy ATP. Taka droga odpowiadałby cyklicznemu transportowi elektronów w fosforylacji cyklicznej roślin.
Podobny łańcuch transportu elektronów posiadają heliobakterie. Są one jedynymi bakteriami posiadającymi bakteriochlorofil g. Nie posiadają struktur porządkujących kompleksy biorące udział w fotosyntezie, a barwniki asymilacyjne znajdują się wprost w błonie komórkowej. Centrum reakcji z bakteriochlorofilem g wykazuje maksimum absorpcji przy 798 nm. Heliobakterie są względnymi autotrofami. W środowisku bez dostępu do światła przechodzą na metabolizm heterotroficzny, wykorzystując do wzrostu mleczan i pirogronian.
Strategia metaboliczna niektórych przedstawicieli heliobakterii, bakterii purpurowych i bakterii zielonych polegająca na wykorzystywaniu energii świetlnej określana jest jako fototrofia z różnymi odmianami – fotoorganoheterotrofia, gdy źródłem elektronów (reduktorem) są związki organiczne, fotolitoheterotrofia, gdy są to substancje nieorganiczne (związki siarki, wodór), a w obu przypadkach źródłem węgla są związki organiczne, natomiast gdy źródłem węgla jest dwutlenek węgla – fotoorganoautotrofia, gdy źródłem elektronów są związki organiczne i fotolitoautotrofia, gdy są to substancje nieorganiczne. Zatem tylko te dwa ostatnie przypadki fotoredukcji są właściwą fotosyntezą.
Bakterie purpurowe.
Bakterie purpurowe jako źródło elektronów mogą wykorzystywać związki siarki, siarkę pierwiastkową, wodór oraz proste związki organiczne (np. jabłczan, bursztynian). Kompleksy antenowe tych bakterii zawierają bakteriochlorofile a lub b. Fotoukład bakterii purpurowych przypomina fotoukład II występujący u roślin. Zawiera trzy polipeptydy o masie 21, 24, 32 kDa, 4 cząsteczki bakteriochlorofilu, 2 cząsteczki bakteriofeofityny, i cząsteczką karotenoidu, dwa chinony i atom żelaza. Maksimum absorpcji przypada na długość fali 870 nm, stąd określenie fotoukładu P-870.
Elektron wybity przez światło z fotoukładu przenoszony jest kolejno na bakteriofeofitynę (bakteriochlorofil pozbawiony magnezu), następnie poprzez chinon A przenoszony jest na cząsteczkę chinonu swobodnie przemieszczającą się w błonie. Ze zredukowanego chinonu elektron trafia na kompleks cytochromowy bc1, a z niego na cytochrom c2, który przenosi elektron do centrum reakcji fotoukładu. W przedstawionym cyklicznym, transporcie elektronów wytwarzane jest jedynie ATP. NADH2 lub NADPH2 wytwarzany jest prawdopodobnie bez udziału światła w odwrotnym łańcuchu transportu elektronów. Elektrony odrywane od związku będącego donorem (H2S, H2, S2O32−, związki organiczne) przenoszone są na cytochrom c2. Następnie trafiają na cytochrom b, co wymaga nakładu energii w postaci ATP. Z cytochromu b elektrony przenoszone są na chinon, a z niego na cząsteczkę NAD+ lub NADP+. Transport elektronów z chinonu na NAD+ także odbywa się wbrew potencjałowi i wymaga zużycia ATP. W efekcie zachodzenia odwrotnego łańcucha transportu elektronów wytwarzana jest siła redukcyjna w postaci NADH2 (NADPH2) niezbędna do redukcji CO2. Związki, które posłużyły jako donory elektronów, przekształcane są w siarkę, wodę lub kwasy organiczne.
NADH oraz ATP wytworzone przy okazji transportu elektronów zużywane są w cyklu Calvina-Bensona, odwrotnym cyklu Krebsa. W redukcji CO2, poza NADH, może także uczestniczyć zredukowana ferredoksyna. Związkiem, z którym łączy się CO2, może być acetylo-CoA lub bursztynylo-CoA, co prowadzi do powstania odpowiednio pirogronianu lub 2-oksoglutaranu.
Halobakterie.
W odmienny sposób energię świetlną wykorzystują halobakterie, w których błonie komórkowej znajduje się specyficzne białko połączone z barwnikiem – bakteriorodopsyna. Kompleks ten może pochłaniać kwanty światła, przechodząc w stan wzbudzenia. Powrót do stanu podstawowego umożliwia przeniesienie przez błonę protonu. Przeniesienie protonów na zewnątrz komórki prowadzi do powstania gradientu elektrochemicznego wykorzystywanego następnie przez syntazę ATP zlokalizowaną w błonie komórkowej do syntazy ATP.
Ewolucja fotosyntezy.
Fotosynteza jest ewolucyjnie bardzo starym procesem. Dowody chemiczne oraz znalezione skamieniałości wskazują, że fotosyntezujące sinice podobne do współcześnie występujących "Chroococcales" istniały 2,5–2,6 miliarda lat temu. Poprzedzały je z pewnością różne formy fotosyntetyzujących bakterii anoksygenicznych. Dane uzyskane przez badanie izotopów węgla sugerują, że asymilacja węgla przez organizmy autotroficzne zachodziła co najmniej miliard lat wcześniej. Natura najwcześniejszych organizmów fotosyntetyzujących nie jest dobrze znana. Główne elementy aparatu fotosyntetycznego to centra reakcji, kompleksy antenowe, kompleksy transferu elektronów i asymilacji węgla. Elementy te nie mają wspólnej historii ewolucji we wszystkich organizmach, dlatego na aparat fotosyntetyczny najlepiej patrzeć jako na mozaikę elementów, z których każdy ma swoją własną historię ewolucji. Istnieją dwie szkoły badania pochodzenia centrów reakcji i fotosyntezy. Pierwsza z nich widzi początek rozwoju centrów reakcji jeszcze w fazie prebiotycznej, druga szkoła widzi centra reakcji rozwijające się od cytochromu b w bakteriach.
Przedstawiane są dwa modele kolejnego etapu rozwoju centrów reakcji w bakteriach purpurowych, bakteriach zielonych i sinicach.
W modelu selektywnej utraty wspólny przodek zawierał zarówno centrum reakcji PS I, jak centrum reakcji PS II. Ewolucja centrów reakcji w bakteriach purpurowych i bakteriach zielonych nitkowatych doprowadziła do utraty centrum reakcji PS I. Z kolei rozwój bakterii zielonych siarkowych oraz heliobakterii doprowadził do utraty centrum reakcji PS II. Oba centra reakcji PS I i PS II pozostały u sinic.
W modelu połączenia wspólny przodek występował w dwóch liniach, jeden zawierał centrum reakcji PS I, a drugi zawierał centrum reakcji PS II. Linia pierwsza dała początek zielonym bakteriom siarkowym oraz heliobakterii, linia druga zapoczątkowała bakterie purpurowe i zielone bakterie nitkowate. Dwa centra reakcji sinic byłyby skutkiem połączenia organizmu zawierającego centrum reakcji PS I i organizmu zawierającego centrum reakcji PS II .
Historia ewolucji różnych kompleksów antenowych – LHC wydaje się całkiem niezależna. Przejście z anoksygenicznej do tlenowej fotosyntezy miało miejsce, kiedy sinice nauczyły się używać wody jako dawcy elektronów do wytwarzania siły redukcyjnej używanej do redukcji CO2. Przed tym przejściowym dawcą elektronów mógł być nadtlenek wodoru, a jeszcze wcześniej źródłem siły redukcyjnej mogłyby być jony żelaza.
Komórki eukariotyczne zdolność do fotosyntezy posiadły około 1,6 mld lat temu poprzez wchłonięcie na drodze endocytozy bakterii fotosyntetyzujących o typie fotosyntezy występującej u sinic. Dowodem takiego pochodzenia chloroplastów jest posiadanie własnego materiału genetycznego przez chloroplasty oraz całkowitego aparatu transkrypcji i translacji potrzebnego do wytworzenia białek zapisanych w genach chloroplastowych. Geny chloroplastu wykazują znaczne podobieństwo do genów sinic, niemniej jednak znaczna część białek chloroplastowych jest obecnie kodowana przez geny jądrowe i muszą być importowane do chloroplastów z cytozolu.
Jednorazowa endosymbioza nie wyjaśnia pochodzenia wszystkich organizmów eukariotycznych posiadających chloroplasty. Dlatego przyjmuje się, że doszło do przynajmniej dwóch, a prawdopodobnie do wielu endosymbioz, w wyniku których powstały współczesne komórki fotosyntetyzujących protistów. Wtórna endosymbioza umożliwia fotosyntezę u organizmów takich jak: kryptomonady, chlorarachniofity, eugleniny, okrzemki, brunatnice, złotowiciowce i innych. W przypadku dwóch pierwszych grup chloroplasty zawierają nukleomorfy, tj. resztki jądra komórkowego endosymbionta, w przypadku kryptomonad – pierwotnego krasnorostu, w przypadku chlorarachniofitów – pierwotnej zielenicy.
Najstarsza skamieniałość trawy "Tomlinsonia", o anatomii liścia typowej dla roślin C4 pochodzi sprzed 12–13 mln lat. Rozsądne granice powstania fotosyntezy C4 u traw to okres pomiędzy 30 a 15 mln lat temu. Około 5–8 mln lat temu, gdy stężenie CO2 w atmosferze spadło do około 200 μbar przy 20 mbar O2, rośliny wykorzystujące fotosyntezę tego typu stały się na ekologicznie dominujące na lądach. W czasach współczesnych fotosynteza C4 odpowiada za 20–30% asymilacji węgla na lądach.
Czynniki wpływające na natężenie fotosyntezy.
Natężenie światła.
W sprzyjających warunkach rośliny mogą zużytkować w procesie fotosyntezy około 5% energii docierającego do liści światła. Ze względu na zakres absorpcji poszczególnych długości fali przez barwniki fotosyntetyczne na wydajność fotosyntezy wpływa jedynie natężenie światła w zakresie 400–700 nm. Natężenie światła biorącego udział w fotosyntezie określa się najczęściej jako gęstość przepływu fotonów fotosyntetycznych (PPFD – "photosynthetic photon flux density") wyrażaną w µmol (fotonów z zakresu 400–700 nm) m−2 s-1. Przy pełnym nasłonecznieniu PPFD przekracza 1500 µmol m−2 s−1. Zależność fotosyntezy od natężenia światła obrazuje tak zwana krzywa świetlna. W przypadku braku oświetlania rośliny wydzielają CO2 produkowany podczas oddychania komórkowego. Przy bardzo małym natężeniu światła, proces wydzielania CO2 w oddychaniu komórkowym, przeważą nad fotosyntetycznym wiązaniem CO2 i roślina nadal wydziela dwutlenek węgla. Przy pewnym natężeniu światła specyficznym dla gatunku rośliny i panujących warunków (np. temperatury) dochodzi do zrównania pobierania CO2 w procesie fotosyntezy i wydzielania CO2 w procesie oddychania komórkowego, punkt ten nazywany jest świetlnym punktem kompensacyjnym. Rośliny wykorzystujące fotosyntezę C3 mają wartość tego punktu niższą niż rośliny o fotosyntezie C4. Przy natężeniu światła powyżej świetlnego punktu kompensacyjnego następuje dalszy wzrost natężenia fotosyntezy. Przy pewnym natężeniu światła, charakterystycznym dla każdej z roślin, dochodzi do wysycenia procesu fotosyntezy. Punkt ten nazywa się świetlnym punktem wysycenia. Jest on wyższy dla roślin o fotosyntezie C4 niż dla roślin o fotosyntezie C3. Również u roślin określanych jako światłolubne (heliofity) świetlny punkt wysycenia jest wyższy niż dla roślin określane jako cieniolubne (skiofity) lub cienioznośne. Długie działanie światła o dużym natężeniu prowadzi do uszkodzenia aparatu fotosyntetycznego i obniżenia wiązania CO2 przez roślinę. Zjawisko hamowania fotosyntezy przez duże natężenie światła nosi nazwę fotoinhibicji i jest głównie efektem uszkodzenia fotoukładu II.
W wodzie światło ulega pochłanianiu, dlatego jego dostępność jest znacznie bardziej ograniczona niż na lądzie. Organizmy przeprowadzające fotosyntezę pod wodą muszą wykorzystywać dostępne światło bardziej efektywnie. W czasie maksymalnego rozwoju w jeziorach, fitoplankton może pochłaniać 70% docierającego światła. Im głębiej, tym więcej światła ulega rozproszeniu i pochłonięciu. Ilość światła, przy której następuje kompensacja świetlna, dla różnych gatunków ma specyficzną wartość i występuje na różnych głębokościach. Głębokość, na której oddychanie (rozkład materii organicznej) równoważy fotosyntezę (tworzenie materii organicznej) w ekosystemie jako całości nazywa się poziomem kompensacyjnym. Umownie jest to głębokość, do której dociera 1% światła fotosyntetycznie czynnego padającego na powierzchnię. W wodach mętnych jest to kilka metrów, w wodach przejrzystych do 150 m. Naświetloną strefę przypowierzchniową, w której fotosynteza przeważa nad oddychaniem, nazywa się strefą eufotyczną lub trofogeniczną, a strefę poniżej poziomu kompensacyjnego – afotyczną lub trofolityczną. Makrofity są w stanie rozwijać się przy natężeniu co najmniej 2% światła padającego na powierzchnię. Rekordowe głębokości zasięgu makrofitów odnotowano w jeziorze Titicaca – rośliny naczyniowe ("Potamogeton strictus") do 11 m, ramienice do 14 m, mchy do 29 m i jeziorze Tahoe – rośliny naczyniowe do 6,5 m, a ramienice i mchy do 75 m. W tym jeziorze produkcja pierwotna glonów osiąga mierzalne wartości na głębokości 105 m, a rozwój zielenicy "Gongrosira" zaobserwowano na głębokości 160 m. Z kolei przy samej powierzchni natężenie światła wywołuje fotoinhibicję. Występuje ona, gdy promieniowanie przekracza 200–800 μE • m −2 • s −1, a całkowicie wyklucza fotosyntezę przy wartościach powyżej 1400 μE • m −2 • s −1. Fotoinhibicja może zachodzić na głębokości 0–0,5 m w wodach humusowych, a w jeziorach przezroczystych w słoneczne dni nawet do 30 m. Optymalne natężenie światła dla glonów zwykle jest rzędu tysięcy lub dziesiątek tysięcy luksów. Zależne jest od różnych czynników, takich jak temperatura czy stężenie dwutlenku węgla, a także jest różne u różnych organizmów. Przykładowo, u okrzemki z rodzaju "Asterionella" w warunkach naturalnych optymalne stężenie wynosiło 5000 lx w temperaturze 5 °C, a 12 500 lx w temperaturze 17 °C. W innych badaniach algologicznych fotoinhibicja dała się zauważyć przy natężeniu 10 000 lx, a fotosynteza ustawała całkowicie przy 100 000 lx. Rośliny i glony przystosowują się do małych natężeń światła zwiększaniem ilości chlorofilu (np. zielenica "Chlorella") albo reorganizując aparat fotosyntetyczny, co przyspiesza tempo fotosyntezy (np. okrzemka "Cyclotella"). Czas potrzebny na adaptację do zmiany oświetlenia może wynosić od kilkunastu minut do kilkunastu dni.
U glonów stosunek białek do węglowodanów wśród ostatecznych produktów fotosyntezy jest odwrotnie proporcjonalny do natężenia światła. Przy silnym natężeniu światła biosynteza białek jest ograniczona, m.in. przez brak przyswajalnego azotu.
Barwa światła.
U roślin lądowych w największym stopniu wykorzystywana jest energia światła pochłanianego przez chlorofil (głównie niebieskie, w mniejszym, stopniu fioletowe, czerwone, pomarańczowe i żółte), a w mniejszym przez karotenoidy. U fotoautotrofów wodnych bywa odmiennie. U zielenic jest podobnie jak u roślin lądowych. U sinic najefektywniej wykorzystywane jest światło żółte pochłaniane przez niebieską fikocyjaninę, u wielu krasnorostów – światło żółte i zielone (pochłaniane przez czerwoną fikoerytrynę), okrzemek i brunatnic (glonów z grupy stramenopili) – światło niebieskie i zielone pochłaniane przez karotenoidy. Optymalna barwa światła (długość fali świetlnej) dla organizmów zajmujących różne siedliska wodne związana jest z różnicami w rozpraszaniu poszczególnych długości fal przez wodę. Barwa organizmu fotosyntetyzującego odpowiada pasmu widma, które jest w najmniejszym stopniu pochłaniane przez barwniki fotosyntetyczne. Glony występujące głęboko (np. w metalimnionie) muszą zarówno mieć odpowiednią barwę (czerwoną), jak i wykazywać właściwości skiofityczne. Przykładem są czerwone sinice z gatunków "Planktothrix rubescens" i "Planktothrix aghardii" var. "isothrix" lub kryptomonady: "Rhodomonas minuta" osiągające optimum na głębokości, do której dociera 50% światła padającego na powierzchnię i "Rhodomonas lens", dla której jest to głębokość otrzymująca 10%. Niektóre glony zmieniają barwę w zależności od zasiedlanej głębokości, np. sinice "Chamaesiphon subglobosus" i "Lingbya purpurescens" przechodzą od barwy zielonej do krwistoczerwonej. Zmienność wykazują też inne sinice, krasnorosty i liczne glony morskie.
Dwutlenek węgla.
Stężenie dwutlenku węgla w powietrzu wynoszące około 0,036% jest znacznie niższe niż optymalne dla procesu fotosyntezy przy sprzyjających warunkach świetlnych i odpowiedniej temperaturze. W optymalnych warunkach natężenie fotosyntezy wzrasta aż do stężenia CO2 około 0,1%. Przy wyjątkowo niskich stężeniach CO2 procesy oddychania i fotooddychania wytwarzają więcej CO2 niż jest asymilowane w fotosyntezie. Stężenie CO2, przy którym jego wydzielanie równoważy się z fotosyntetycznym pobieraniem, nosi nazwę punkt kompensacyjny stężenia dwutlenku węgla. Dla roślin o fotosyntezie C4 jest on bliski zeru, a dla roślin o fotosyntezie C3 zależnie od gatunku i temperatury leży on w przedziale 0,009–0,018% CO2. Natężenie fotosyntezy dla roślin C4 jest przy niskich stężeniach dwutlenku węgla wyższe niż dla roślin C3. Przy wartościach bliskich stężeniu optymalnemu rośliny C3 uzyskują niewielką przewagę w intensywności wiązania CO2, co wykorzystuje się to w uprawach szklarniowych poprzez „nawożenie” roślin CO2 w sprzyjających warunkach temperaturowych i świetlnych. Stężenia CO2 powyżej 1% są dla roślin toksyczne i powodują zahamowanie procesy fotosyntezy. Badania na zielenicy "Chlorella vulgaris" nie wykazały, aby stężenie do 4% wpływało na aktywność RuBisCO, choć wpływało na niektóre inne enzymy biorące udział w metabolizmie węgla (jak anhydraza węglanowa czy karboksylaza PEP).
Bardziej skomplikowane są stosunki między stężeniem CO2 a fotosyntezą w wodzie (fototrofy wodne). Dwutlenek węgla rozpuszczony w wodzie może występować w fazie cząsteczkowej lub jako jony HCO3− i CO32−. Proporcje występowania tych form zależą od odczynu wody i zawartości substancji buforujących, np. jonów wapnia tworzących bufor wodorowęglanowy. Przy wysokim pH przeważającą formą jest HCO3− i CO32−, przy niskim pH wzrasta zawartość CO2 w postaci rozpuszczonego gazu. Postacie rozpuszczonego węgla nieorganicznego ( "DIC", "dissolved inorganic carbon") są w różnym stopniu dostępne dla organizmów fotosyntetyzujących. Większość wykorzystuje niemal wyłącznie cząsteczkowy dwutlenek węgla; z kolei do przyswajania jonów wodorowęglanowych przystosowane jest mniej gatunków (np. zielenica "Hydrodictyon africanum", niektóre okrzemki, niektóre rośliny naczyniowe). Zdolność do wiązania węgla w postaci jonów wodorowęglanowych daje organizmom przewagę konkurencyjną, gdyż zwiększa wydajność pozyskiwania węgla. W niektórych badaniach organizmy zdolne do asymilacji HCO3− wykorzystywały większość całego dostępnego węgla (glony z rodzajów "Anabaena", "Microcystis" i "Scenedesmus" do 90%, makrofity takie jak wywłócznik kłosowy i rdestnica przeszyta do 70%), podczas gdy organizmy do tego niezdolne – zaledwie do 5%. Poza tym organizmy zdolne do asymilacji jonów wodorowęglanowych mogą żyć w wodach o odczynie pH = 11, w których rozpuszczony dwutlenek węgla już nie występuje. Często glony najpierw zużywają jony wodorowęglanowe, a po ich wyczerpaniu natężenie fotosyntezy spada. Ten efekt jest szczególnie silny w wodach zasadowych, gdzie wodorowęglany i węglany to główna postać węgla nieorganicznego. Z drugiej strony, zanurzone rośliny wodne niepotrafiące wykorzystywać HCO3− i dla których niedostępny jest atmosferyczny dwutlenek węgla, mają punkt kompensacyjny pozwalający na wykorzystywanie mniejszych stężeń rozpuszczonego CO2 (2–12μM), niż rośliny zdolne do zdobywania nieorganicznego węgla z innych źródeł (60–110 μM). Jony wodorowęglanowe nie są bezpośrednio włączane do cyklu Calvina, lecz po ich uprzedniej dehydratacji przy użyciu powszechnie występującego u różnych organizmów enzymu – anhydrazy węglanowej, a więc jako dwutlenek węgla. W tej postaci są wiązane przez RuBisCO. Aktywność anhydrazy węglanowej jest uzależniona m.in. od stężenia dwutlenku węgla – u "Chlorella vulgaris" przy jego zwiększonej koncentracji spada i spowalnia fotosyntezę. Przy normalnych lub niskich stężeniach CO2 – wzrasta i wzmaga natężenie fotosyntezy. Jest to wynik wpływu odczynu (zależnego od stężenia dwutlenku węgla) na właściwości tego enzymu.
Temperatura.
Jak dla wszystkich procesów przeprowadzanych z użyciem enzymów, tak i dla fotosyntezy temperatura jest czynnikiem ograniczającym. Rośliny są w stanie przeprowadzać fotosyntezę w temperaturach niewiele poniżej zera (rośliny górskie) aż do temperatur zbliżających się do 50 °C (rośliny pustyń). Nieco szerszy zakres tolerancji wykazują glony, zwłaszcza sinice. Eukariotyczne glony są w stanie żyć i fotosyntetyzować do temperatury 55–60 °C, choć dla większości gatunków granica ta sięga zaledwie do 40 °C. Termofilne gatunki sinic przeprowadzają fotosyntezę do ok. 75 °C w wodach obojętnych lub zasadowych i ok. 56 °C w wodach kwaśnych. Przy optymalnych warunkach świetlnych optimum temperaturowe wynosi około 30 °C i jest niższe dla roślin C3 niż dla roślin C4. Przy wyższych temperaturach natężenie fotosyntezy spada, co jest związane przede wszystkim ze wzrastającą intensywnością reakcji oddychania i fotooddychania w temperaturach powyżej 40 °C. W temperaturze powyżej 40 °C spada powinowactwo enzymu RuBisCO do CO2, przez co większego znaczenia nabiera druga funkcja tego enzymy – oksydacja 1,5-bisfosforybulozy. W temperaturze 60–70 °C dochodzi do denaturacji kompleksów chlorofilowo-białkowych, co prowadzi do całkowitego zaniku aktywności fotosyntetycznej rośliny. Zmiana właściwości enzymów biorących udział w reakcjach fotosyntezy to niejedyna przyczyna zmian w natężeniu fotosyntezy. Wraz ze wzrostem temperatury zwiększa się płynność błon komórkowych, co prowadzi do wycieku jonów, między innymi protonów, z wnętrza tylakoidów. W efekcie pomimo transportu elektronów przez przenośniki nie jest wytwarzany gradient protonowy niezbędny do syntazy ATP. W niskich temperaturach płynność błon komórkowych ulega zmniejszeniu, co ogranicza tempo dyfuzji ruchliwych przenośników elektronów, głównie plastochinonu i plastocyjaniny, a w konsekwencji obniża wydajność fazy jasnej fotosyntezy. Wrażliwość na fotooksydację wzrasta wraz z temperaturą, przez co postuluje się, że duże natlenienie wód po letnich zakwitach jest wraz z wysoką temperaturą przyczyną śmierci skądinąd ciepłolubnych gatunków sinic. Dla niektórych termofilnych sinic optymalna temperatura fotosyntezy wynosi 68–72 °C, podczas gdy dla gatunków umiarkowanie termofilnych – 45–50 °C. Z kolei niektóre kriofilne szczepy zielenicy zawłotni śnieżnej "Chlamydomonas nivalis" optymalnie fotosyntetyzują przy temperaturze 3 °C (choć dla innych szczepów tego gatunku to może być nawet 20 °C). U niektórych organizmów temperatura może modyfikować dominujący typ fotosyntezy. "Chlorella vulgaris" w temperaturze ok. 25 °C przeprowadza cykl Hatcha-Slacka, natomiast w temperaturze niższej lub wyższej – typ C3.
Woda.
Woda zwykle nie stanowi czynnika bezpośrednio wpływającego na wydajność fotosyntezy, jednak gospodarka wodna rośliny ma dla tego procesu pewne znaczenie. Zamknięcie aparatów szparkowych w sytuacji niedoboru wody w roślinie prowadzi do ograniczenia wnikania CO2 do wnętrza liścia, a to z kolei powoduje zahamowanie fotosyntezy. Powstanie mechanizmów zapobiegających ograniczaniu fotosyntezy w warunkach niedoboru wody spowodowało wykształcenie roślin CAM i C4 prowadzących dużo bardziej oszczędną gospodarkę wodą niż rośliny C3.
Tlen.
Stężenie tlenu w atmosferze jest stosunkowo stałe, jednak w komórce, do której tlen dociera przez aparaty szparkowe oraz jest wytwarzany w fazie jasnej fotosyntezy, lokalne stężenie tlenu może ulegać znaczącym wahaniom. Stężenie tlenu w cytoplazmie wzrasta przy intensywnym zachodzeniu fazy jasnej fotosyntezy. Jeśli aparaty szparkowe pozostają otwarte, O2 zostaje usunięty z organów asymilacyjnych, jeśli jednak aparaty szparkowe pozostają zamknięte lub przymknięte, stężenie tlenu jest znacznie wyższe niż w roztworze, w którym rozpuszczone są składniki powietrza atmosferycznego. Tlen wpływa przede wszystkim na zachodzenie procesu utleniania 1,5-bisfosforybulozy, katalizowanego przez enzym RuBisCO. Zjawisko zwiększonego wydzielania CO2 podczas oświetlania roślin oraz przez krótki okres po oświetlaniu nazywane jest fotooddychaniem, a jego bezpośrednią przyczyną jest dwufunkcyjność enzymu RuBisCO. Ze względu na konkurowanie o miejsce katalityczne karboksylazy 1,5-bisfosforybulozy przez CO2 i O2 wzrost stężenia tlenu, następujący podczas intensywnego zachodzenia fazy jasnej, prowadzi do zahamowania natężenia fotosyntezy. Jednocześnie intensywne zachodzenie fotosyntezy prowadzi do obniżenia stężenia CO2 w komórce, co zwiększa udział reakcji rozpoczynającej fotooddychanie. Do zwiększenia stężenia tlenu dochodzi szczególnie podczas stresu, gdy w wyniku suszy, zasolenia lub wysokiej temperatury dochodzi do zamknięcia aparatów szparkowych, tlen nie może zostać usunięty z liści, a CO2 dostarczany jest w niewystarczających ilościach. W tej sytuacji dochodzi do zwiększenia udziału fotooddychania. Prawdopodobnie fotooddychanie chroni rośliny rosnące w zmieniających się warunkach przed uszkodzeniem aparatu fotosyntetycznego poprzez wytwarzanie CO2 i podtrzymywanie reakcji fazy ciemnej w warunkach niewystarczającego dostarczania CO2 z atmosfery. Rośliny przeprowadzające fotosyntezę C4 nie wykazują fotooddychania i nie obserwuje się u nich wpływu stężenia tlenu na natężenie fotosyntezy, w stężeniach zbliżonych do stężenia tlenu w powietrzu atmosferycznym. Wykazują jednak większą wrażliwość na niskie temperatury.
W stężeniach tlenu wyższych od stężenia atmosferycznego także u roślin C4 obserwuje się spadek natężenia fotosyntezy, co prawdopodobnie związane jest ze wzrostem natężenia reakcji Mehlera.
Wysokie stężenie tlenu powstającego przy intensywnej fotosyntezie może być przyczyną powstawania reaktywnych form tlenu i prowadzić do fotooksydacji chlorofilu i degradacji fotoukładów.
Bakterie fotosyntetyzujące mogą być fakultatywnymi lub obligatoryjnymi anaerobami. Zielone bakterie z rodziny "Chlorobiaceae" giną w obecności tlenu. Cześć bakterii purpurowych na świetle jest anaerobami, a w ciemności stają się heterotroficznymi aerobami.
Wysokie stężenie tlenu jest toksyczne zwłaszcza dla sinic. Fotooksydacja zachodzi zwykle przy dłuższym działaniu światła i zwiększana jest przez wysoką temperaturę i niskie stężenie CO2 i mineralnych substancji pokarmowych. Prawdopodobnie łagodzona jest przez karotenoidy.
Znaczenie.
Fotosynteza jest jednym z podstawowych procesów biologicznych. Warunkuje ona istnienie absolutnej większości organizmów żywych na Ziemi. Dzięki reakcjom fotosyntezy możliwa jest przemiana materii nieorganicznej (CO2) w organiczną stanowiącą źródło energii dla organizmów heterotroficznych.
Jedynym alternatywnym źródłem związków organicznych jest proces chemosyntezy przeprowadzany przez niektóre bakterie. Ilość materii organicznej powstałej w wyniku chemosyntezy jest znikoma. Istnieją jednakże całe ekosystemy wokół hydrotermalnych kominów oceanicznych, w których produkcją związków organicznych zajmują się bakterie chemosyntetyzujące. Jednakże nawet w ekosystemach położonych na dnie oceanów poniżej zasięgu światła słonecznego odkryto bakterie zawierające bakteriochlorofil, co sugeruje, że zachodzi tam fotosynteza, w której źródłem energii są niewielkie ilości światła geotermalnego. Pomimo produkcji substancji organicznych głównie na drodze chemosyntezy nawet te odległe od światła słonecznego ekosystemy pozostają zależne od procesu fotosyntezy zachodzącej na powierzchni kontynentów i oceanów. Liczne organizmy z wyższych poziomów troficznych (konsumenci) żyjące w strefie afotycznej korzystają z tlenu będącego ubocznym produktem fotosyntezy i posiadają znaczne ilości hemoglobiny służącej do jego magazynowania .
To właśnie uboczny produkt fotosyntezy, tlen, jest niezbędny do życia wszystkich organizmów heterotroficznych z wyjątkiem bakterii beztlenowych. Z drugiej strony, tlen jako pierwiastek reaktywny może prowadzić do niszczenia związków organicznych i być toksyczny dla organizmów, nie tylko bezwzględnych anaerobów, dlatego niemal wszystkie obecnie żyjące na Ziemi organizmy wykształciły mniej lub bardziej skuteczne mechanizmy ochrony przed toksycznym działaniem tlenu. Paradoksalnie istnieją organizmy, które są jednocześnie zdolne do tlenorodnej fotosyntezy i ściśle beztlenowych innych procesów metabolicznych. Tak jest w przypadku niektórych sinic, które mogą anaerobowo wiązać wolny azot wyłącznie w oddzielonych od reszty kolonii heterocytach. Prawdopodobnie jednak we wczesnej historii życia tlen uwalniany przez pierwotne organizmy fotosyntetyzujące oksygenicznie był przyczyną swoistej klęski ekologicznej i wymarcia znacznej części ówczesnych gatunków znanej jako katastrofa tlenowa. Obecny w atmosferze Ziemi tlen jest skutkiem fotosyntezy. Proces fotosyntezy jest jednym z czynników ustalających poziom CO2 w atmosferze ziemskiej. Jego obecna zawartość jest czynnikiem ograniczającym natężenie fotosyntezy. W wyniku fotosyntezy w ciągu roku na Ziemi gromadzone jest 1010 ton węgla w postaci związków organicznych, co odpowiada energii około 4,2 × 1017 kJ.
Cała energia zawarta w paliwach kopalnych (np. węgiel, torf, ropa, gaz ziemny) pochodzi z procesu fotosyntezy, który zachodził w roślinach przez miliony lat. Wtórne produkty fotosyntezy wykorzystywane są jako różnego rodzaju produkty roślinne. Stanowią źródło pokarmu, drewna, surowców dla przemysłu chemicznego, farmaceutyki i przetwórstwa.

</doc>
<doc id="1699" url="https://pl.wikipedia.org/wiki?curid=1699" title="Fluorowodór">
Fluorowodór

Fluorowodór, HF – nieorganiczny związek chemiczny zbudowany z fluoru i wodoru. Jest to bezbarwna ciecz lub gaz o ostrym zapachu. Jego temperatura wrzenia jest niewiele niższa od temperatury pokojowej, co wyróżnia go spośród innych fluorowcowodorów (np. temp. wrz. HCl wynosi –85 °C). Spowodowane jest to występowaniem pomiędzy cząsteczkami tego związku silnych wiązań wodorowych.
Jego wodny roztwór nazywany jest kwasem fluorowodorowym.
Budowa cząsteczki.
Fluorowodór w stanie stałym tworzy zygzakowate łańcuchy cząsteczek – asocjat HF. Cząsteczki HF o wewnętrznych wiązaniach H-F o długości 0,95 Å, są połączone wiązaniami wodorowymi H···F o długości 1,55 Å
Ciekły HF również tworzy zygzakowate łańcuchy, tyle że krótsze, zawierające średnio pięć do sześciu połączonych cząsteczek. Wyższa temperatura topnienia HF niż innych fluorowcowodorów jest spowodowana silnymi wiązaniami wodorowymi pomiędzy cząsteczkami. HF posiada moment dipolowy.
Otrzymywanie.
Fluorowodór otrzymuje się działając kwasem siarkowym na fluorek wapnia:
Zastosowanie.
HF jest stosowany do fluorowania węglowodorów. Wykorzystuje się go przy przeróbce ropy naftowej, wytrawianiu znaków i napisów na szkle, wytwarzaniu glinu, produkcji aluminiowych puszek, oczyszczaniu kwarcu. Jest także stosowany do wyrobu heksafluorku uranu UF6 używanego do wzbogacania uranu.
Fluorowodór bywa spotykany w odrdzewiaczach do stali, preparatach do oczyszczania mosiądzu, trawienia szkła. Preparaty zawierające HF zostały wycofane z użytku ze względu na jego toksyczność. HF jest najczęściej sprzedawany jako: ciekły bezwodny fluorowodór, 70% roztwór w wodzie lub 40% roztwór w wodzie.
Wpływ fluorowodoru na zdrowie.
Posiada ostry zapach, drażni drogi oddechowe. Ciekły HF oraz jego stężone roztwory powodują trudno gojące się rany. Rozcieńczone roztwory również są bardzo niebezpieczne, gdyż przenikają bez uczucia bólu przez skórę i tkanki miękkie, atakując bezpośrednio chrząstki i kości.
Fluorowodór jest toksyczny, a zatrucie nim może być fatalne w skutkach po spożyciu lub absorpcji przez skórę. Oparzenia fluorowodorem wymagają niezwłocznego leczenia i uwagi od razu, gdy zostaną zauważone. Oparzenia kwasem fluorowodorowym nie są podobne do oparzeń innymi kwasami. Jednorazowe skutki oparzeń zależą od stężenia roztworu kwasu. Efekt oparzenia kwasem 50% jest natychmiast widoczny, stężenia 20-50% roztworu skutkują objawami po jednej do ośmiu godzin. Stężenie niższe niż 20% skutkuje widocznymi objawami w ciągu 24 godzin od czasu oparzenia.
HF może oddziaływać na ciało przez wdychanie, połknięcie, kontakt ze skórą. Po spożyciu, lub oparzeniu skóry lub oczu najważniejszym objawem jest obniżenie zawartości wapnia w osoczu krwi i wytrącanie nierozpuszczalnego w wodzie fluorku wapnia CaF2, co może skutkować silnym bólem, zmianami metabolizmu i śmiercią.
W postaci cieczy przechowywany w butelkach plastikowych, butlach stalowych, miedzianych lub srebrnych kanistrach. Nie przechowuje się go w opakowaniach szklanych z uwagi na to, że reaguje ze szkłem.

</doc>
<doc id="1700" url="https://pl.wikipedia.org/wiki?curid=1700" title="Fosforowodór">
Fosforowodór

Fosforowodór, fosforiak, fosfina, – nieorganiczny związek chemiczny, którego cząsteczka zbudowana jest z fosforu i wodoru. Jest to bezbarwny, palny i bardzo toksyczny gaz.
Występowanie.
Na Ziemi powstaje naturalnie jako produkt redukcji fosforanów przez różne gatunki bakterii w warunkach beztlenowych. Najczęściej wytwarzają go bakterie ściśle beztlenowe, takie, jak ', rzadziej ', ', ', ' czy '. Obecności samozapalnego fosforiaku w palnych gazach wydobywających się z bagien przypisano zjawisko błędnych ogników.
Pierwsze doniesienia postulujące jego biogenezę pochodzą z 1897 r. W 1923 r., a następnie w 1927 r. opisane zostały bakterie zdolne do jego produkcji, co jednak przez długi czas budziło wiele kontrowersji, z powodu problemów z odtworzeniem odkrycia przez innych badaczy i obliczeniami wskazującymi, że proces biologicznej redukcji fosforanu do wydawał się niemożliwy termodynamicznie. Dopiero w 1959 r. stwierdzono, że powstawania fosforiaku nie można wykluczyć na gruncie termodynamiki. Trudności z wykryciem tego związku związane są w znacznej mierze z łatwością, z jaką ulega on utlenieniu. Po raz pierwszy jego obecność w gazach pochodzenia biologicznego stwierdzono w gazach wydobywających się z osadów w oczyszczalniach ścieków i płytkich jezior na terenie Węgier za pomocą spektrometrii mas w 1988 r., a następnie w wielu lokalizacjach na świecie. Zaobserwowane stężenia są o kilka rzędów wielkości niższe od stężenia szkodliwego dla człowieka.
Poza Ziemią obecność fosforowodoru stwierdzono na planetach-olbrzymach – Jowiszu i Saturnie – oraz gwiazdach węglowych, w warunkach bardzo wysokich ciśnień i temperatur. Natomiast nie są znane procesy abiologiczne, podczas których mógłby powstawać w znaczących ilościach na planetach skalistych. Dlatego w astrobiologii obecność fosforowodoru na ciele niebieskim postulowana jest jako znacznik obecności życia. Obecność gazu nie jest jednak dowodem jednoznacznym – nie można wykluczyć, że gaz ten może powstawać w wyniku nieodkrytych dotąd procesów foto- lub geochemicznych. W 2020 r. Jane Greaves z walijskiego Cardiff University i współpr. (współautorem odkrycia jest m.in. Janusz Pętkowski z zespołu Sary Seager z Massachusetts Institute of Technology) donieśli o zarejestrowaniu fosforowodoru w atmosferze Wenus (w ilości ok. 20 ppb). Wykorzystano w tym celu spektroskopię rotacyjną, korzystając z aparatury zainstalowanej na teleskopie Jamesa Clerka Maxwella oraz sieci radioteleskopów Atacama Large Millimeter Array (ALMA) w Chile, rejestrując promieniowanie mikrofalowe o , charakterystyczne dla przejścia rotacyjnego . Publikacja ta wzbudziła duże zainteresowanie, gdyż może oznaczać, że na Wenus występuje życie. Hipotezę tę poparła Clara Sousa-Silva z MIT, której zespół przez wiele lat badał możliwości niebiogennego powstawania fosforowodoru na planetach skalistych i nie zidentyfikował żadnego takiego procesu (co jednak nie oznacza, że nie ma takiej możliwości). Niektórzy astrobiolodzy i astrofizycy odnieśli się jednak krytycznie do tego odkrycia, sugerując błędną interpretację widma. Zastrzeżenia wzbudziło m.in. wykorzystanie danych z radioteleskopów ALMA – nawet Anita Richards, współautorka publikacji o odkryciu na Wenus, przyznałą, że sieć ALMA nie została stworzona do wykrywania subtelnych efektów w bardzo jasnych obiektach, takich jak Wenus. Uwagi do wykrycia fosfiny zgłosił m.in. zespół badaczy z Centrum Lotów Kosmicznych imienia Roberta H. Goddarda pod kierownictwem Geronima Villanuevy, który stwierdził, że za obserwowane wyniki spektralne odpowiada i wezwał Greaves do wycofania artykułu. Po ponownym przeanalizowaniu danych, zespół Sary Seager podtrzymał opinię, że na Wenus został wykryty , jednak w mniejszej ilości, niż podano pierwotnie – ok. 1 ppb, przy wartości maksymalnej do 5 ppb.
Otrzymywanie.
Fosforowodór otrzymać można wieloma metodami:
Surowy produkt oczyszcza się przez wymrożenie wody i difosfiny suchym lodem (ok. −78 °C), a następnie skroplenie fosforowodoru w płuczce zanurzonej w ciekłym azocie.
Właściwości.
Właściwości fizyczne.
W czystej postaci, w temperaturze pokojowej, jest to bezbarwny, pozbawiony zapachu gaz; w produktach komercyjnych zapach czosnku lub zgniłych ryb pochodzi od zanieczyszczeń powstałych w procesie produkcji. Skrapla się pod normalnym ciśnieniem w temperaturze −87,4 do −87,9 °C (według różnych źródeł), tworząc bezbarwną ciecz. W stan stały przechodzi w −133,5 °C i tworzy kilka form, prawdopodobnie związanych z ograniczeniem rotacji cząsteczek.
W NMR przesunięcie chemiczne jąder wodoru fosforiaku rozpuszczonego w ciekłym amoniaku wynosi , a w NMR . Stała sprzężenia 1"J" wynosi ok. 180–190 Hz, a z powodu sprzężenia jądra fosforu z trzema jądrami wodoru na widmie NMR obserwuje się kwartet o stosunku sygnałów 1:3:3:1. Przesunięcie chemiczne jest wyjątkowo duże, co wynika z faktu, że wiązania w fosforiaku mają charakter czystego wiązania pojedynczego "p"σ(P)"s"σ(H).
Silnie ujemne przesunięcie chemiczne w NMR i bardzo małe kąty wiązań (ok. 93°; dla porównania, w wynoszą one 98°) wskazują, że wolna para elektronowa cząsteczki znajduje się głównie na orbitalu 3s.
Właściwości chemiczne.
Fosforowodór, w porównaniu do swoich fosforoorganicznych pochodnych, jest bardzo bierny chemicznie i z trudem reaguje ze związkami elektrofilowymi.
Właściwości biologiczne.
Jest silnie trujący, przy dłuższym narażeniu śmiertelne może być stężenie 10 ppm w powietrzu. Pomimo słabej rozpuszczalności w wodzie (22,8 ml gazu w 17 °C/100 ml), wykazuje ostrą toksyczność dla organizmów wodnych.
Zastosowania.
Stosuje się go jako insektycyd do gazowania składów ziaren zbóż oraz konstrukcji drewnianych w budynkach, pokarmu dla zwierząt i liści tytoniu oraz owoców i warzyw (w tym zastosowaniu zastępuje bromometan), choć wymaga to zastosowania podwyższonej temperatury lub długiego, do 10 dni, okresu inkubacji. Stosuje się go także jako środek do zwalczania gryzoni. Ponadto wykorzystywany jest do domieszkowania w technologiach produkcji półprzewodników.
Pochodne fosforowodoru.
Formalnie solami fosforowodoru są fosforki. Fosforek cynku () i fosforek glinu (AlP) stosowane są do zwalczania szkodników (jako insektycydy i rodentycydy). Fosforki metali III grupy układu okresowego (fosforek glinu, fosforek galu, fosforek indu i fosforek talu) wykazują właściwości półprzewodnikowe i są stosowane w elementach elektronicznych.
Każdy atom wodoru w fosforowodorze może zostać zastąpiony grupą organiczną. Powstają wówczas związki fosforoorganiczne nazywane fosfinami, np. trifenylofosfina stosowana w reakcji Mitsunobu.

</doc>
<doc id="1701" url="https://pl.wikipedia.org/wiki?curid=1701" title="Fosforiak">
Fosforiak



</doc>
<doc id="1705" url="https://pl.wikipedia.org/wiki?curid=1705" title="Gastroenterologia">
Gastroenterologia

Gastroenterologia () – dziedzina medycyny zajmująca się czynnościami i schorzeniami układu pokarmowego; przełyku, żołądka, jelit, odbytu i gruczołów trawiennych (wątroba, trzustka) oraz dróg żółciowych. Badaniem samego żołądka zajmuje się gastrologia. W Polsce lekarze gastroenterolodzy są szkoleni w ramach dwóch specjalizacji: gastroenterologia i gastroenterologia dziecięca. Konsultantem krajowym gastroenterologii od 7 czerwca 2019 jest prof. dr hab. Jarosław Reguła, a gastroenterologii dziecięcej prof. dr hab. Mieczysława Czerwionka-Szaflarska (od 21 czerwca 2018).

</doc>
<doc id="1707" url="https://pl.wikipedia.org/wiki?curid=1707" title="Ginekologia">
Ginekologia

Ginekologia – dziedzina medycyny zajmująca się profilaktyką i leczeniem chorób żeńskiego układu płciowego. Najczęstsze problemy, jakimi zajmują się ginekolodzy, to: zaburzenia miesiączkowania, stany zapalne pochwy, antykoncepcja, niepłodność, nowotwory narządów rodnych. Ściśle związana z położnictwem. 
Badaniu ginekologicznemu powinna poddać się każda kobieta, która ukończyła 16. rok życia lub rozpoczęła współżycie. Takie badanie powinno składać się z następujących elementów:
W Polsce wchodzi w zakres specjalizacji lekarskiej położnictwo i ginekologia, której konsultantem krajowym od 1 sierpnia 2017 jest prof. dr hab. Krzysztof Czajkowski.

</doc>
<doc id="1708" url="https://pl.wikipedia.org/wiki?curid=1708" title="GNU General Public License">
GNU General Public License

GNU General Public License (GPL) – licencja wolnego i otwartego oprogramowania stworzona w 1989 roku przez Richarda Stallmana i Ebena Moglena na potrzeby Projektu GNU, zatwierdzona przez Open Source Initiative. Pierwowzorem licencji była licencja "Emacs General Public License". Druga wersja licencji GNU GPL została wydana w roku 1991, a trzecia – 29 czerwca 2007.
Założenia.
Celem licencji GNU GPL jest przekazanie użytkownikom czterech podstawowych wolności:
Tylko jeżeli program spełnia wszystkie cztery wolności jednocześnie, wówczas, według FSF, może być uznany za wolne oprogramowanie. Wystarczy, że nie spełnia dowolnej z nich, a nie może być tak kategoryzowany (jest oprogramowaniem zamkniętym).
Historia.
Pierwsza wersja licencji powstała w lutym 1989 roku.
Dwa lata później, w czerwcu 1991, pojawiła się wersja druga.
29 czerwca 2007, po 18 miesiącach prac (w czasie których wydano 4 szkice licencji oraz przeprowadzono kampanię zachęcającą do przejścia na nową wersję za około pół miliona dolarów), wydano trzecią wersję licencji.
Zmiany w wersji 3. są umiarkowane i mają na celu głównie dostosowanie ochrony licencyjnej do współczesnego stanu informatyki. GPLv3 bierze pod uwagę między innymi systemy prawne poza USA, kwestie patentów na oprogramowanie, ochronę DRM, proceder tiwoizacji oraz problem istnienia wielu niezgodnych ze sobą licencji.
Kwestie dyskusyjne.
Jedną z kluczowych kwestii związanych z GPL jest problem, czy oprogramowanie na innej licencji może być dynamicznie linkowane z bibliotekami GPL. Sama licencja wyraźnie mówi, że wszystkie pochodne prace bazujące na kodzie GPL muszą same opierać się na GPL. Jednak nie jest jasne, czy plik wykonywalny, który jest dynamicznie linkowany z biblioteką, może być uważany za pracę pochodną.
Kompatybilność z innymi licencjami.
Większość licencji wolnego oprogramowania, jak na przykład licencja X11, licencja BSD i LGPL jest kompatybilnych z GPL. Znaczy to, że kod źródłowy oparty na nich może być włączony bez problemu do programu na GPL (całość będzie wtedy objęta GNU GPL). Są jednak licencje open source, które nie są kompatybilne z GPL. Z tego powodu wiele osób odradza używanie takich licencji, ponieważ opartego na nich kodu trudno jest ponownie użyć w innych projektach.
Kod na licencji GNU GPL nie może być użyty w programach opartych o inne licencje.
Krytyka GPL.
Ze względu na to, że wszelkie prace bazujące na dziele objętym licencją GPL muszą również być oparte na tej licencji, licencja GPL jest licencją wirusową. Tak więc GPL oddziałuje na każdy program, który korzysta z kodu GPL. Krytyka tego przymusu najczęściej jest kierowana ze strony zwolenników mniej restrykcyjnych licencji, jak na przykład licencja BSD.

</doc>
<doc id="1709" url="https://pl.wikipedia.org/wiki?curid=1709" title="Gastrologia">
Gastrologia



</doc>
<doc id="1710" url="https://pl.wikipedia.org/wiki?curid=1710" title="Granica ciągu">
Granica ciągu

Granica ciągu – wartość, w której dowolnym otoczeniu znajdują się "prawie wszystkie" (tzn. wszystkie poza co najwyżej skończenie wieloma) wyrazy danego ciągu. Inaczej – wartość, "dowolnie blisko" której leżą wszystkie wyrazy ciągu o "dostatecznie dużych" wskaźnikach.
Dodatnia liczba całkowita formula_1 staje się coraz większa, wartość formula_2 staje się coraz bliższa formula_3 Mówimy, że granica ciągu formula_2 jest równa formula_3
Granica (właściwa) i zbieżność.
Niech formula_6 będzie nieskończonym ciągiem liczb rzeczywistych lub zespolonych. Liczbę formula_7 nazywa się granicą ciągu formula_8 jeżeli:
gdzie symbol formula_10 oznacza wartość bezwzględną liczby rzeczywistej, bądź moduł liczby zespolonej.
W interpretacji geometrycznej powyższa nierówność dla liczb zespolonych oznacza w istocie, że wybrane jw. wyrazy formula_11 leżą w kole formula_12 z kolei dla liczb rzeczywistych oznacza ona, że leżą one w przedziale formula_13 który jest odpowiednikiem koła dla osi liczbowej.
Powyższy formalny warunek można więc wysłowić następująco:
Granicę ciągu formula_6 oznacza się formula_22 i czyta się: „limes formula_23 przy formula_1 dążącym do nieskończoności” lub po prostu formula_25 i czyta się: „limes formula_26”, a fakt, że formula_7 jest granicą ciągu formula_8 niekiedy oznacza się formula_29 lub formula_30 i czyta się: „ciąg formula_11 "dąży do" formula_7” lub „ciąg formula_11 "jest zbieżny do" formula_7” (można dodać: „przy formula_1 dążącym do nieskończoności”).
Ciągi mające granice nazywa się zbieżnymi, a pozostałe – rozbieżnymi. Do badania ciągów rozbieżnych stosuje się pojęcie "granicy górnej i dolnej", czyli największej i najmniejszej spośród wszystkich granic jego podciągów zbieżnych. Ciąg liczb rzeczywistych jest zbieżny wtedy i tylko wtedy, gdy jego granice górna i dolna są sobie równe. Przydatne jest też pojęcie "punktu skupienia". Jest ono uogólnieniem pojęcia granicy, bowiem każda granica jest punktem skupienia, ale nie na odwrót.
Niekiedy, dla odróżnienia od "granicy niewłaściwej" opisanej w kolejnej sekcji, granicę ciągu zbieżnego do pewnej liczby rzeczywistej lub zespolonej (nazywanej wtedy „skończoną”, w przeciwieństwie do dwóch lub jednej „liczb nieskończonych”) nazywa się granicą właściwą.
Granice niewłaściwe.
Dla niektórych rozbieżnych ciągów nieskończonych wprowadza się pojęcie "granicy niewłaściwej". Chodzi o ciągi, których wyrazy rosną lub maleją nieograniczenie; o takich ciągach mówi się także, że dążą one do nieskończoności.
Jeżeli formula_6 jest ciągiem liczb rzeczywistych i wszystkie jego wyrazy o indeksach większych od odpowiednio dużego formula_17 są większe od dowolnej z góry wybranej liczby, to mówi się, że ciąg ma "granicę niewłaściwą" w formula_38 bądź że jest "rozbieżny" do formula_39
Jeżeli zaś są mniejsze od dowolnej z góry wybranej liczby, to mówi się, że ma on granicę niewłaściwą w formula_40 lub że jest rozbieżny do formula_41
Formalnie można to zapisać tak:
Jeżeli formula_6 jest ciągiem liczb zespolonych i wszystkie jego wyrazy o indeksach większych od odpowiednio dużego formula_17 są większe co do modułu od dowolnej z góry wybranej liczby rzeczywistej, to mówi się, że ciąg ma "granicę niewłaściwą" w formula_49 bądź że jest "rozbieżny" do formula_50
Formalnie:
Geometrycznie można to ująć w następujący sposób:
Wprowadzoną powyżej definicję rozbieżności ciągów zespolonych można bez zmian zastosować dla ciągów rzeczywistych, zastępując jedynie moduł liczby zespolonej formula_54 wartością bezwzględną liczby rzeczywistej. W praktyce jednak tej definicji nie stosuje się, bowiem traci się wówczas możliwość rozróżniania kierunku (zwrotu) rozbieżności ciągu.
Zbieżność w przestrzeniach metrycznych.
Pojęcie granicy ciągu można wprowadzić w dowolnej przestrzeni metrycznej. Wystarczy w definicji granicy zastąpić wartość bezwzględną (moduł) różnicy dwóch liczb odległością według metryki danej przestrzeni. Niech formula_128 będzie przestrzenią metryczną. Ciąg formula_6 elementów tej przestrzeni jest zbieżny do formula_130 jeśli:
Warunkiem równoważnym zbieżności ciągu formula_6 jest żądanie, by ciąg formula_133 gdzie formula_134 był zbieżny do zera.
Zbieżność w przestrzeni metrycznej można wyrazić:
Jeśli ciąg (w przestrzeni metrycznej) jest zbieżny, to jest ciągiem Cauchy’ego (w przypadku ciągów liczbowych rzeczywistych lub zespolonych zachodzi również twierdzenie odwrotne, to znaczy powyższe warunki są równoważne).
Przykłady
Zbieżność w przestrzeniach topologicznych.
Pojęcie granicy ciągu można wprowadzić w jeszcze ogólniejszych przestrzeniach topologicznych przez zastąpienie kul otoczeniami.
Niech formula_147 będzie przestrzenią topologiczną. Ciąg formula_148 elementów tej przestrzeni jest zbieżny do formula_149 jeśli
co można wyrazić:
lub inaczej:
W przestrzeniach Hausdorffa (którymi są m.in. przestrzenie liczb rzeczywistych lub zespolonych) każdy ciąg może być zbieżny do najwyżej jednego punktu.
Przykłady
Historia.
Zenon z Elei znany jest ze sformułowania paradoksów, które wykorzystują przejścia graniczne. Leucyp z Miletu, Demokryt z Abdery, Antyfont z Ramnus, Eudoksos z Knidos i Archimedes z Syrakuz wynaleźli metodę wyczerpywania, która wykorzystuje ciąg przybliżeń umożliwiający wyznaczenie powierzchni bądź objętości; ostatniemu z nich znane było również sumowanie, które dziś nazywane jest szeregiem geometrycznym.
Isaac Newton zajmował się szeregami w swoich dziełach dotyczących analizy szeregów nieskończonych ("Analysis with infinite series", napisane w 1669 roku, najpierw krążyło jako manuskrypt, opublikowano w 1711 roku), metodzie fluksji i szeregach nieskończonych ("Method of fluxions and infinite series", napisane w 1671 roku, wydane w tłumaczeniu angielskim w 1736 roku; oryginał łaciński wydano znacznie później) i traktacie o krzywych kwadratowych ("Tractatus de Quadratura Curvarum", napisane w 1693 roku, a opublikowane w 1704 roku jako dodatek do jego "Optiks"), później rozważał on rozwinięcie dwumienne formula_198 które linearyzuje, "biorąc granice", tzn. przyjmując formula_199
Osiemnastowiecznym matematykom, takim jak Leonhard Euler, udawało się zsumować pewne szeregi "rozbieżne" dzięki „zatrzymaniu się w odpowiednim momencie”; nie interesowali się oni nadto tym, czy granica istnieje, o ile tylko mogła być ona obliczona. Pod koniec XVIII wieku Joseph Louis Lagrange w swojej pracy "Théorie des fonctions analytiques" (1797) stwierdził, że brak rygoru przeszkadza w rozwoju analizy. Carl Friedrich Gauss w dziele o szeregach hipergeometrycznych (1813) po raz pierwszy zbadał w sposób rygorystyczny pod jakimi warunkami szereg zbiega do granicy.
Współczesną definicję granicy (dla każdego formula_14 istnieje taki wskaźnik formula_15 że…) niezależnie podali:

</doc>
<doc id="1711" url="https://pl.wikipedia.org/wiki?curid=1711" title="Grupa funkcyjna">
Grupa funkcyjna

Grupa funkcyjna – rodzaj podstawnika w związku organicznym, który zmienia jego właściwości chemiczne i decyduje o sposobie jego reagowania w danej reakcji i stanowiący podstawę klasyfikacji związku.
Cząsteczki zawierające określoną grupę funkcyjną reagują w podobny sposób w danych reakcjach chemicznych (reaktywność tę mogą jednak zmieniać inne grupy funkcyjne, zwłaszcza znajdujące się w bliskim sąsiedztwie grupy analizowanej). Znając ogólne zasady, według których reagują dane grupy funkcyjne, można przewidzieć własności związków je zawierających.
Wybrane grupy funkcyjne:

</doc>
<doc id="1712" url="https://pl.wikipedia.org/wiki?curid=1712" title="Grupa alkilowa">
Grupa alkilowa

Grupa alkilowa, potocznie alkil – fragment organicznego związku chemicznego, jednowartościowa grupa utworzona formalnie przez oderwanie jednego atomu wodoru od cząsteczki alkanu. Oznacza się ją często literą R (symbol „R”, od ang. "residue", nie jest jednak dla niej zarezerwowany i może symbolizować dowolną grupę funkcyjną), a jej wzór ogólny to CnH2n+1. 
Najprostszą z nich jest grupa metylowa (−CH3):
Podobnie jak alkany, grupy alkilowe mogą być liniowe, rozgałęzione oraz cykliczne. W przypadku alkilowych grup cyklicznych wzór ogólny przedstawia się następująco: CnH2n−1.
Grupy alkilowe występują powszechnie w ogromnej większości związków organicznych, stanowiąc ich podstawowy budulec.
Grupy alkilowe są niereaktywne w większości reakcji chemicznych, jednak ze względu na stosunkowo duże rozmiary ich obecność decyduje o konformacji cząsteczek, a także stereochemii i kinetyce reakcji chemicznych.

</doc>
<doc id="1713" url="https://pl.wikipedia.org/wiki?curid=1713" title="S/y Grażyna">
S/y Grażyna

Grażyna – jacht morski zbudowany w roku 1922 jako fiński „Turo”, przebudowany w 1934 r. i ochrzczony 29 lipca 1934 w Jastarni jako „Grażyna”.
Historia i rejsy.
Jacht do II wojny światowej służył morskiemu szkoleniu polskich harcerek. Jacht został ufundowany harcerkom z dobrowolnych składek, w czym swój udział miał ówczesny wojewoda śląski Michał Grażyński, od którego nazwiska ("nota bene" przybranego) pochodzi nazwa jednostki. Jacht był jolem podnoszącym cztery żagle: dwa sztaksle: fok i kliwer na bukszprycie, gaflowy grot oraz bermudzki bezan o łącznej powierzchni 76,5 m².
Pierwszym kapitanem Grażyny była Jadwiga Wolffowa-Neugebauerowa, pierwsza Polka, która otrzymała stopień kapitana jachtowego, a drugim (do roku 1939) kpt. Jadwiga Skąpska-Truscoe. Doprowadzenie do obecności na morskim żaglowcu załogi składającej się z samych kobiet, było w tamtych czasach rzeczą niezwykłą i osiągnięciem na miarę epoki (np. na harcerskiego Zawiszę, który podniósł banderę w czerwcu 1935 r., zgodnie z panującymi wówczas żeglarskimi obyczajami i przesądami dziewczęta nie miały wstępu).
W czasie jednego z pierwszych bałtyckich rejsów na „Grażynie” (1935) 12 dziewcząt przeżyło prawdziwy morski chrzest, napotykając sztorm. Jedna z uczestniczek tego rejsu – bosman Maria Bukarówna – ułożyła wówczas piosenkę „Pod żaglami Grażyny”, która – po „zaanektowaniu” przez harcerzy i niewielkich przeróbkach – zdobyła popularność jako „Pod żaglami Zawiszy”.

</doc>
<doc id="1714" url="https://pl.wikipedia.org/wiki?curid=1714" title="STS Generał Zaruski">
STS Generał Zaruski

STS Generał Zaruski – jeden z najstarszych czynnych polskich żaglowców, pływający cały rok po wodach Bałtyku, w sezonie głównie w rejsach szkoleniowych dla młodzieży, mający w swojej karierze wyprawy także na wody podbiegunowe. Jako pływający zabytek sztuki szkutniczej ma dość trudne warunki bytowe dla załogi i jest częściowo wyposażony w archaiczne już dziś urządzenia i jako taki jest ulubioną jednostką polskich żeglarzy pielęgnujących stare morskie tradycje. Do tych tradycji weszły już na stałe coroczne rejsy (właściwie wyprawy) zimowe, np. wielodniowe rejsy sylwestrowe.

</doc>
<doc id="1715" url="https://pl.wikipedia.org/wiki?curid=1715" title="Glicyna">
Glicyna

Glicyna (), skr. Gly, G – organiczny związek chemiczny, najprostszy spośród 20 standardowych aminokwasów białkowych, jedyny niebędący czynny optycznie. Za jej pojawienie się w łańcuchu polipeptydowym odpowiada obecność kodonów GGU, GGC, GGA lub GGG w łańcuchu mRNA.
Można ją otrzymać sztucznie w reakcji kwasu chlorooctowego z amoniakiem.
Struktura i właściwości.
Glicyna posiada najmniejszą resztę aminokwasową, z jednym tylko atomem wodoru w łańcuchu bocznym. Ze względu na to, że z atomem węgla α związane są dwa atomy wodoru, glicyna – w przeciwieństwie do innych aminokwasów – nie jest optycznie czynna. Glicyna zalicza się do grupy aminokwasów niepolarnych alifatycznych.
W trakcie ewolucji dywergentnej (rozbieżnej) reszty glicyny zmieniają się znacznie rzadziej niż pozostałych aminokwasów, a gdy już ulegają mutacji w białkach homologicznych, to na takie reszty jak alanina, seryna, kwas asparaginowy lub asparagina. Ta konserwatywność w występowaniu glicyny wiąże się z jej niewielkimi rozmiarami – zmiana tego aminokwasu na inny, z większym łańcuchem bocznym, mogłaby zaburzyć strukturę przestrzenną białka i pozbawić to białko jego funkcji biologicznej.
Glicyna stanowi średnio około 7,2% reszt aminokwasowych występujących w białkach. Wyjątkiem jest kolagen, w którym glicyna stanowi blisko jedną trzecią wszystkich budujących go aminokwasów.
Glicyna jest aminokwasem endogennym.
Objętość van der Waalsa 48 Å³.
Biosynteza.
Ludzki organizm potrafi syntetyzować glicynę, dlatego nazywa się ją aminokwasem endogennym. Glicyna może być produkowana: z glioksalanu i glutaminianu przez aminotransferazę glutaminianową; z alaniny przez aminotransferazę alaninową. Ważnym sposobem syntezy glicyny u ssaków jest także synteza z choliny oraz seryny.
Degradacja glicyny.
Glicyna ulega degradacji na drodze trzech szlaków metabolicznych.
Ponadto glicyna ulega licznym przemianom w inne metabolity, co zostało opisane w podrozdziale dotyczącym jej funkcji.
Choroby związane z przemianami glicyny.
Z metabolizmem glicyny związane są następujące schorzenia:
Zastosowanie w lecznictwie.
Glicyna jest wykorzystywana do zwiększania skuteczności leków przeciwpsychotycznych zawierających kwas glutaminowy. Choć sama w sobie nie ma działania psychotropowego, to jednak wzmacnia efekty działania glutaminianu w mózgu (zgodnie z hipotezą glutaminową). Przyłącza się do receptora NMDA wraz z glutaminianem i pokonuje barierę krew-mózg.
Występowanie glicyny w przestrzeni międzygwiazdowej.
W roku 1994 grupa badaczy z University of Illinois pod kierownictwem Lewisa Snydera ogłosiła wstępnie odkrycie glicyny w przestrzeni międzygwiazdowej, jednak późniejsze badania nie potwierdziły ich przypuszczeń.
Dziewięć lat później, w 2003 roku, Yi-Jehng Kuan z National Taiwan Normal University wraz ze Steve’em Charlneyem z NASA ponowili to doniesienie. Badacze monitorowali fale radiowe pod kątem obecności linii spektralnych charakterystycznych dla glicyny, których zarejestrowali w sumie 27. Kuan i Charlney wysunęli hipotezę, że glicyna międzygwiazdowa powstała z prostych cząsteczek organicznych uwięzionych w lodzie na skutek ekspozycji na nadfiolet.
W roku 2004 Snyder wraz ze współpracownikami opublikowali pracę, w której próbowali określić obiektywne kryterium, które powinny spełniać rejestrowane linie spektralne, aby można było mówić o potwierdzonym odkryciu glicyny w przestrzeni międzygwiazdowej. Uznali jednocześnie, że żaden z rezultatów uzyskanych przez grupę Kuana nie spełnia tego kryterium.
W roku 2009 dzięki misji NASA Stardust ostatecznie potwierdzono występowanie glicyny w przestrzeni kosmicznej pobierając próbki z komety Wild 2.
W 2016 roku ogłoszono, że sonda Rosetta odkryła glicynę w komie komety 67P/Czuriumow-Gierasimienko podczas pomiarów za pomocą spektrometru masowego ROSINA w latach 2014–2015.
Tzw. glicyna fotograficzna.
W fotografii terminem „glicyna” określa się typ wywoływacza fotograficznego zawierającego pochodną glicyny, "N"-(4-hydroksyfenylo)glicynę.

</doc>
<doc id="1716" url="https://pl.wikipedia.org/wiki?curid=1716" title="Gaz">
Gaz

Gaz – stan skupienia materii, w którym ciało fizyczne łatwo zmienia kształt i zajmuje całą dostępną mu przestrzeń. Właściwości te wynikają z własności cząsteczek, które w fazie gazowej mają pełną swobodę ruchu. Wszystkie one cały czas przemieszczają się w przestrzeni zajmowanej przez gaz i nigdy nie zatrzymują się w jednym miejscu. Między cząsteczkami nie występują żadne oddziaływania dalekozasięgowe, a jeśli, to bardzo słabe. Jedyny sposób, w jaki cząsteczki na siebie oddziałują, to zderzenia. Oprócz tego, jeśli gaz jest zamknięty w naczyniu, to jego cząsteczki stale zderzają się ze ściankami tego naczynia, wywierając na nie określone i stałe ciśnienie.
Termin wprowadzony przez flamandzkiego lekarza Johanna Helmonta w XVII wieku wzorem gr. "χάος" ‘cháos’.
Cząsteczki gazu przemieszczają się z różną szybkością, a rozkład tych szybkości ma charakter całkowicie statystyczny (rozkład Maxwella). Średnia szybkość poruszania się cząsteczek w gazie jest zależna wyłącznie od ich masy cząsteczkowej i temperatury. Podczas obniżania temperatury gazu maleje średnia szybkość cząsteczek, zaś zwiększanie ciśnienia powoduje zmniejszenie średniej odległości między nimi. Obniżanie temperatury lub zwiększanie ciśnienia prowadzi w końcu do skroplenia lub resublimacji gazu. Zamiana gazu w ciecz lub ciało stałe wynika z faktu, że w pewnym momencie energia oddziaływań międzycząsteczkowych (sił van der Waalsa, wiązań wodorowych itp.) staje się większa od energii kinetycznej cieplnego ruchu cząsteczek.
W fizyce przyjmuje się często prosty model gazu doskonałego, w którym cząsteczki gazu nie przyciągają się i nie mają objętości własnej. Teorie i zależności termodynamiczne wywiedzione z założeń gazu doskonałego sprawdzają się dość dobrze (na ogół) w przypadku niezbyt dużych ciśnień oraz niezbyt niskich temperatur. W innych przypadkach prawa te jednak zawodzą i wtedy stosuje się bardziej złożone modele gazów i tworzy dokładniejsze teorie i zależności (zob. gaz rzeczywisty, równanie van der Waalsa, wirialne równanie stanu).
Interesującą cechą gazu (a ściślej gazu doskonałego) jest to, że objętość przez niego zajmowana (w danej temperaturze i ciśnieniu) jest stała, niezależnie od rodzaju cząsteczek, jakie są w gazie, i zależy wyłącznie od liczby tych cząsteczek. Innymi słowy, jeśli weźmiemy np. 1 litr wodoru i 1 litr tlenu (oba przy tym samym ciśnieniu i temperaturze), to w obu objętościach będzie dokładnie taka sama liczba cząsteczek. Jest to tzw. prawo Avogadra.
Aby jednoznacznie określić stan gazu, poza składem chemicznym (ułamki wagowe lub molowe) i temperaturą należy podać gęstość gazu lub jego ciśnienie. Zamiast gęstości można podać równoważnie objętość molową lub stężenie gazu.
Dla dowolnego gazu:
gdzie: "m" – masa gazu, "V" – objętość gazu, "N" – liczba cząsteczek, "NA" – liczba Avogadra, "M" – masa molowa.
Dla gazu doskonałego:
gdzie: "R" – uniwersalna stała gazowa, "T" – temperatura.

</doc>
<doc id="1718" url="https://pl.wikipedia.org/wiki?curid=1718" title="Genotyp">
Genotyp

Genotyp (gr. "γένος" - ród, pochodzenie + "τύπος" - odbicie) – zespół genów danego osobnika warunkujących jego właściwości dziedziczne. Pojęcie genotypu odnosi się czasem także do genów wirusa. Jest to sparowany układ alleli (często myli się go z genomem, czyli składem genetycznym podstawowego (monoploidalnego) zestawu chromosomów). Można go wyrazić symbolicznie za pomocą oznaczeń "aa", "AA" lub "Aa", gdzie "aa" i "AA" oznaczają homozygotę pod względem tego genu, a "Aa" oznacza heterozygotę.
Przykład: kawie domowe o genotypach "BB" i "Bb" są podobne fenotypowo, to znaczy obie mają czarną sierść. Po skrzyżowaniu świnki czarnej "BB" ze świnką brązową (homozygotą "bb") otrzymuje się osobniki wyłącznie czarne (heterozygoty "Bb"). Jednakże, po skrzyżowaniu heterozygotycznej świnki czarnej "Bb" z homozygotą recesywną "bb" otrzymuje się osobniki dwóch kolorów: czarne ("Bb") i brązowe ("bb").
Metoda krzyżowania z homozygotą recesywną jest najprostszą metodą badania genotypu. Jest to tak zwane krzyżowanie testowe, stosowane od dawna w hodowli zwierząt i roślin użytkowych.

</doc>
<doc id="1720" url="https://pl.wikipedia.org/wiki?curid=1720" title="George Bernard Shaw">
George Bernard Shaw

George Bernard Shaw (ur. 26 lipca 1856 w Dublinie, zm. 2 listopada 1950 w Ayot St Lawrence) – irlandzki dramaturg i prozaik, przedstawiciel dramatu realistycznego. Jako filozof twórca koncepcji tzw. "siły życiowej" (Life Force) i "ewolucji twórczej" (Creative Evolution).
Indywidualność twórcza, outsider i "obcokrajowiec, irlandzki obserwator", żyjący "w niezgodzie ze społeczeństwem, które znaczyło dla niego mniej niż ludzie, których znał i lubił" (John Matthews). Znany zwłaszcza dzięki dramatowi "Pigmalion" (1913) oraz często przywoływanym aforyzmom.
George Bernard Shaw jest laureatem Nagrody Nobla w dziedzinie literatury za rok 1925. Z uzasadnienia Komitetu Noblowskiego: "za twórczość naznaczoną idealizmem i humanizmem, za przenikliwą satyrę, która często łączy się z wyjątkowym pięknem poetyckim". Jest także zdobywcą Oscara w roku 1938 za „najlepszy scenariusz adaptowany” do filmu "Pigmalion", co czyni go pierwszą osobą w historii uhonorowaną Oscarem i Nagrodą Nobla (drugą został Bob Dylan w 2016 r.).
Twórczość.
Początki pisarskie Shawa były dość trudne; na długi czas jego sztuki (utrzymane głównie w tendencjach naturalizmu i realizmu) nie spotykały się z zainteresowaniem krytyki ani czytelników. Autorowi zarzucano ponadto zbyt intelektualne podejście do relacji międzyludzkich, zarzucenie psychologizmu postaci na rzecz konfliktu idei i światopoglądów, zbyt archetypiczne przedstawienie bohaterów sztuk. Pierwszym dziełem Shawa, o którym zaczęto szeroko dyskutować, był dramat "Kandydy" wydany w 1903.
W 1904, głównie dzięki przyjaźni z wieloma reżyserami, stworzył tzw. "scenę idei" w londyńskim Royal Court Theatre, który przez następnych kilka lat stał się właściwie teatrem autorskim Shawa.
Po wybuchu I wojny światowej ogłosił drukiem obszerny artykuł "Wojna z punktu widzenia zdrowego rozsądku", w którym nakłaniał do pacyfizmu i z właściwym sobie ciętym dowcipem wyśmiewał ślepy patriotyzm zarówno Niemiec jak i Wielkiej Brytanii. Publikacja ta zyskała Shawowi wielu wrogów oraz spowodowała jego wykluczenie z Klubu Dramaturgów.
Ostatnie sztuki Shawa odrzuciły konwencje realistyczne i tematykę socjalną na rzecz zwrotu ku wzorcom antycznym.
Pieniądze otrzymane wraz z Nagrodą Nobla przeznaczył na angielsko-skandynawski fundusz skierowany głównie do tłumaczy dzieł Augusta Strindberga. Shaw był również gorącym zwolennikiem filozofii Friedricha Nietzschego i Henri Bergsona.
Koncepcja filozoficzna.
W tworzonej przez siebie filozofii autor "Pigmaliona" uwzględniał istnienie "siły życiowej", która miała być bodźcem do osiągnięcia stanu nadczłowieczeństwa tożsamego z życiem w harmonii. "Siła życiowa" była przez pisarza różnie rozumiana w ciągu jego całego życia; wpierw oznaczała ona "siłę woli", później pierwiastek biologicznie zapisany w człowieku, na końcu zaś element boski w jednostce ludzkiej.
Był zwolennikiem eugeniki. Cytat z "DNA: tajemnica życia" autorstwa Jamesa D. Watsona oraz Andrew Barry'ego: „Do grona jej gorących zwolenników należał też George Bernard Shaw, który napisał: «obecnie nie istnieje żadna rozsądna wymówka, by nie zaakceptować faktu, że nic—poza religią eugeniki—nie jest w stanie uratować naszej cywilizacji»”.
Kontrowersje.
Był jednym z dziennikarzy, którzy brali udział w tuszowaniu sprawy Wielkiego Głodu w mediach zachodnich.
Jedną z długotrwałych obsesji Shawa były masowe morderstwa przy użyciu trującego gazu. Podczas wykładu z 1910 r. przed "Eugenics Education Society" powiedział: „Powinniśmy być zaangażowani w zabijanie bardzo wielu ludzi, których teraz zostawiamy przy życiu (...) Część polityki eugenicznej ostatecznie doprowadziłaby nas do szerokiego wykorzystania komory śmiercionośnej. Bardzo wielu ludzi musiałoby zostać wyeliminowanych po prostu dlatego, że marnuje to czas innych ludzi, aby się nimi opiekować.”
W tygodniku BBC z 1933 r. Shaw wystosował apel do chemików „Aby odkryli humanitarny gaz, który zabija natychmiast i bezboleśnie. Śmiertelnie jak najbardziej, ale humanitarny nie okrutny…” Jego apel wkrótce doszedł do skutku w nazistowskich Niemczech. Jak zauważa Robert Jay Lifton w "The Nazi Doctors", „Użycie trującego gazu - najpierw tlenku węgla, a następnie Cyklonu B - było osiągnięciem technologicznym umożliwiającym „humanitarne zabijanie”.
Odniesienia w kulturze.
Liczne anegdoty dotyczące George'a Bernarda Shawa, a także bogata korespondencja, jaką pozostawił, spowodowały, że stał się bohaterem sztuk pisanych przez innych autorów:

</doc>
<doc id="1721" url="https://pl.wikipedia.org/wiki?curid=1721" title="Grazia Deledda">
Grazia Deledda

Grazia Maria Cosima Damiana Deledda (ur. 27 września 1871 w Nuoro, zm. 15 sierpnia 1936 w Rzymie) – włoska powieściopisarka i nowelistka, laureatka Nagrody Nobla w dziedzinie literatury za rok 1926.
Biografia.
Grazia Deledda urodziła się w Nuoro na Sardynii, jako czwarte dziecko z siedmiorga dzieci, zamożnego posiadacza ziemskiego. Jej ojciec, Giovanni Antonio Deledda, absolwent prawa, był zamożnym właścicielem ziemskim, zajmował się handlem i rolnictwem, interesował się poezją, sam komponował wiersze i założył drukarnię i wydawał czasopisma. W 1892 r. został burmistrzem Nuoro. Matka, Francesca Cambosu, zajmowała się prowadzeniem domu i wychowywaniem dzieci.
Ukończyła cztery klasy szkoły podstawowej, a następnie pobierała prywatne lekcje u profesora Pietro Ganga, który udzielał jej podstawowych lekcji w języku włoskim, łacinie i francuskim. Zachęcił ją także do publikowania prac pisanych na zadane przez niego tematy. W wieku 13 lat po raz pierwszy opublikowano jej opowiadania "Sangue sardo" i "Remigia Helder". Ukazały się one w 1888 r. w magazynie o modzie "„L’ultima moda”". W 1892 r. napisała pierwszą powieść "Fior di Sardegna".
W 1900 roku wyjechała w swoją pierwszą podróż do Cagliari, gdzie poznała swojego przyszłego męża Palmiro Madesaniego. Wraz z nim zamieszkała w Rzymie . Pierwszy większy sukces przyniosła jej powieść "Elias Portolú" (1903). W swoich powieściach i nowelach odtwarzała krajobraz i życie swojej rodzinnej Sardynii. Krajobraz Sardynii był przez nią często wykorzystywany jako metafora trudności w życiu bohaterów jej utworów.
Grazia Deledda jest laureatką Nagrody Nobla w dziedzinie literatury za rok 1926. Nagrodę otrzymała: „za poetyckie dzieła, w których z jaskrawą plastycznością opisuje życie jej ojczystej wyspy, a także za głębię w podejściu do ludzkich problemów w całości”.
Z licznych jej dzieł wymienić należy: "Anime oneste" (1896), "Elias Portolu" (1900), "Cenere" (1904), "Nostalgia" (1905), "Colombi e sparvieri" (1912), "Trzcina na wietrze" (1913), "Il segredo dell’ uomo solitario" (1921), "Annalena Bilsini" (1927).
W wielu późniejszych pracach Grazia Deledda łączyła wyobraźnię i autobiografię. Ta mieszanka jest widoczna szczególnie w powieści "Il paese del vento" (1931). Powieść "L’argine" (1934) odzwierciedla życie autorki jako wyrzeczenie się doczesnych rzeczy, w tym miłości, akceptując poświęcenie jako wyższy sposób życia i pojednanie z Bogiem. Wspólną cechą wszystkich jej późniejszych dzieł jest stała wiara w ludzi i w Boga.
W 1906 r. ukazała się w Polsce powieść "Popiół" ("Cenere") w tłumaczeniu Wilhelminy Zyndram-Kościałkowskiej. W 1934 r. wydano w Polsce powieść "Trzcina na wietrze" w tłumaczeniu Idy Ratinowowej.

</doc>
<doc id="1722" url="https://pl.wikipedia.org/wiki?curid=1722" title="Gustave Flaubert">
Gustave Flaubert

Gustave Flaubert [] (ur. 12 grudnia 1821 w Rouen, zm. 8 maja 1880 w Croisset) – powieściopisarz francuski, uważany za pierwszego przedstawiciela naturalizmu i jednego z największych mistrzów literatury francuskiej XIX wieku.
Jego "Pani Bovary" i "Salambo" zostały umieszczone w indeksie ksiąg zakazanych dekretem z 1864 r..
Życiorys.
Rodzicami pisarza byli zamieszkali w Rouen Achille-Cléophas Flaubert (1784–1846), naczelny lekarz (chirurg) szpitala miejskiego "Hôtel-Dieu", i Anne-Justine-Caroline z domu Fleuriot (1793–1872). Starszym bratem Gustave’a był Achille (ur. 1813), a młodszą siostrą – Caroline (ur. 1824). 
Flaubert próbował pisać bardzo wcześnie, bo już w wieku zaledwie 8 lat. Kształcił się początkowo w liceum im. P. Corneille'a w swoim rodzinnym mieście. Podczas letnich wakacji nad morzem w 1836 r. spotkał swą pierwszą miłość – znacznie starszą Élisę Schlesinger, długoletni ideał uczucia oraz źródło inspiracji twórczej, co znalazło odbicie w późniejszej "Szkole uczuć". Po uzyskaniu bakalaureatu (1840) podjął pod naciskiem ojca studia prawnicze (1841), które miały zapewnić mu karierę adwokacką. Studia w Paryżu traktował jednak z dystansem i przerwał je po wystąpieniu objawów epilepsji. Swoją chorobę, która była w XIX wieku bardzo źle postrzegana, utrzymywał do końca życia w tajemnicy i wiedzieli o niej tylko najbliżsi przyjaciele. Pod koniec 1840 wybrał się w podróż po Pirenejach i Korsyce. Pomimo zniechęcenia stolicą, zawarł w tym okresie szereg wartościowych znajomości, m.in. z rzeźbiarzem Jamesem Pradierem, młodym literatem Maximem Du Camp oraz z samym Wiktorem Hugo. 
W r. 1846 opuścił Paryż i ostatecznie porzucił studia prawnicze, poświęcając się bez reszty pisarstwu, co umożliwiał mu znaczny (pół miliona franków) spadek po ojcu, którym jednak z woli spadkodawcy zarządzała jego matka. Powrócił do Croisset w pobliżu Rouen i zamieszkał w domu matki spędzając tam resztę życia. Niekiedy tylko udawał się do Paryża i do Anglii. W latach 1846-1855 utrzymywał trwały związek ze sporo starszą, bo 34 letnią, bardzo znaną poetką-celebrytką Louise Colet; zdaniem biografów był to jedyny poważny związek miłosny w jego życiu. Odtąd bowiem ograniczał się do związków platonicznych i towarzyskich, zwłaszcza w środowisku literatów; korzystał też z usług prostytutek. Nigdy się nie ożenił. Utrzymywał jedynie bliski kontakt rodzinny ze swą siostrzenicą Caroline Commanville.
Do rewolucji 1848 r. miał stosunek krytyczny, czemu dał wyraz również w "Szkole uczuć". Rok wcześniej odbył podróż po Bretanii i Normandii w towarzystwie Maxime'a Du Camp. Sporo też podróżował w latach późniejszych. Śladami wielkich nazwisk literatury francuskiej (Chateaubrianda, Lamartine'a i Nervala) odbył wspólnie z M. Du Camp podróż na Bliski Wschód (1849-52), odwiedzając Grecję, kraje Lewantu (z Jerozolimą) i Egipt, zatrzymując się nieco dłużej w Stambule oraz we Włoszech w drodze powrotnej. Sam przyznawał, iż podczas tych wojaży zadawał się z prostytutkami płci obojga, czego wynikiem były choroby weneryczne (w Bejrucie zaraził się syfilisem), z których nie wyleczył się aż do śmierci. W 1858 zainteresowany ruinami Kartaginy odwiedził francuski Tunis, gdzie zbierał materiały do swej głośnej powieści historycznej "Salambo".
Słynął jako pisarz niestrudzony w pracy twórczej, którą traktował jak codzienny obowiązek, narzucając sobie w pisaniu wysokie wymagania wraz z niezwykłą precyzją oraz dyscypliną słowa; w listach do przyjaciół nierzadko uskarżał się na jej męczący charakter. Mimo to, dobrze widziany wśród socjety II Cesarstwa, okresowo odwiedzał paryskich przyjaciół z literackiego środowiska, do których należeli T. Gautier, Ch. Baudelaire, E. Zola, A. Daudet, a także I. Turgieniew i bracia Goncourt. Uczęszczał do literackich salonów paryskich, m.in. księżnej Matyldy (córki Hieronima Bonaparte i kuzynki Napoleona III) oraz George Sand, z którą przyjaźnił się i korespondował konsultując swoje pisarstwo. Na dworze przyjęty był przez cesarza Napoleona III, a w 1866 odznaczony Legią Honorową. 
Lata siedemdziesiąte były trudnym okresem dla Flauberta, któremu podczas wojny francusko-pruskiej (1870) dom zajęli Prusacy i który po śmierci matki (1872) popadł w trudności finansowe, powiększone jeszcze nietrafionymi inwestycjami Ernesta Commanville'a, męża siostrzenicy. Przygnębiała go również utrata przyjaciół: między rokiem 1869 a 1876 zmarli m.in. poeta Louis Bouilhet i przedsiębiorca i podróżnik Jules Duplan, z którymi był blisko związany, a także Jules de Goncourt, dawna wieloletnia kochanka Louise Colet i ważna dla niego w późniejszym okresie życia George Sand. 
W 1879 roku w towarzystwie zaproszonego Guy de Maupassanta spalił otrzymywaną przez całe życie korespondencję, w tym od matki, George Sand i Louise Colet. Kilka miesięcy później Flaubert zmarł w swoim domu w Croisset z powodu wylewu krwi do mózgu. 
Pochowany został w grobie rodzinnym na cmentarzu w Rouen; w pogrzebie uczestniczyli m.in. E. Zola, A. Daudet, G. de Maupassant i Edmond de Goncourt. W dawnym szpitalu "Hôtel-Dieu", gdzie poza muzeum historii medycyny umieszczono także jego muzeum, w 1890 odsłonięto poświęcony mu pomnik projektu Henri Chapu.
Twórczość.
Pisarz zadebiutował dwoma utworami młodzieńczymi o treści erotycznej: "Mémoires d’un fou" (1838) i "Novembre" (1842, przekł. Wilhelma Mitarskiego: "Listopad", 1920).
Pierwszym dojrzałym dziełem Flauberta była powieść "Madame Bovary" (1857, przekł. polski L. Kaczyńskiej: "Pani Bovary", 1878; nowy przekł. Alfreda Iwieńskiego, 1914). Autor ukazał smutne dzieje bohaterki z nieubłaganie naturalistyczną wiernością, rozmyślnym chłodem i ściśle poprawnym językiem. Powieść była krytykowana jako niemoralna, co spowodowało nawet próbę pociągnięcia autora do odpowiedzialności karnej.
Druga jego powieść – historyczna "Salammbô" (1862; pierwszy przekład polski Natalii Dygasińskiej pt. "Córka Hamilkara", 1876) przedstawia dramatyczny epizod z dziejów dawnej Kartaginy za czasów Hamilkara Barkasa. "L'éducation sentimentale" (1869; przekład pol. Tadeusza Jakubowicza, 1931: "Szkoła serca"; później jako "Szkoła uczuć") jest obrazem młodzieńczej miłości autora na tle francuskiej socjety lat 1840–1850.
Dialog "Tentation de saint Antoine" (1874; przekład pol. Antoniego Langego: "Kuszenie św. Antoniego", 1907) jest fantastycznym utworem filozoficznym i historyczno-kulturalnym.
Ostatnim ukończonym dziełem Flauberta były "Trois contes" (1877; przekład pol. Wacława Rogowicza: "Trzy opowieści", 1914; nowy przekład Renaty Lis i Jarosława Marka Rymkiewicza: "Trzy baśnie", 2009). Pierwsza opowieść przedstawia historię życia służącej, druga żywot św. Juliana Szpitalnika, a trzecia zdarzenia na dworze Heroda.
Pośmiertnie w roku 1881 wydano nieukończoną satyrę "Bouvard et Pécuchet" (przekł. Wacława Rogowicza "Bouvard i Pécuchet", 1950), skierowaną przeciwko ludzkiej głupocie i podejmującą m.in. problem pisarstwa historycznego.
Flaubert propagował hasło „sztuka dla sztuki” i dążył do najwyższej doskonałości formy przy możliwie największym obiektywizmie opisu.
Autorami najważniejszych studiów o Flaubercie byli E. Faguet (IV wyd. 1919) oraz A. Thibaudet (1922).

</doc>
<doc id="1724" url="https://pl.wikipedia.org/wiki?curid=1724" title="Gwara poznańska">
Gwara poznańska

Gwara poznańska – gwara języka polskiego charakterystyczna dla mieszkańców Poznania i części Poznańskiego. Elementy tożsame z gwarą poznańską, będące w istocie zapożyczeniami z języka niemieckiego, można spotkać na ziemiach byłego zaboru pruskiego (w granicach po 1815 roku).
Cechy gwary.
Charakterystyczne cechy wymowy w gwarze poznańskiej (charakterystyczne, poza ostatnim, dla jej formy literackiej oddającej stan z przełomu XIX i XX wieku; opisane w kontraście do standardowej polszczyzny):
Leksyka.
Odrębności leksykalne Poznania mają wiele źródeł; do głównych należą nierównomierne wycofywanie się słownictwa "archaicznego" na skutek rozbiorów Polski; znaczne wpływy języka niemieckiego w zaborze pruskim; przenikanie wpływów gwar ludowych do języka miejskiego (w przypadku Poznania przede wszystkim gwar ludowych Wielkopolski) razem z napływem ludności wiejskiej do miast; wreszcie kształtowanie się osobnego od ludowo-wielkopolskiego słownictwa w społeczno-kulturowych warunkach miasta, typowego dla gwary miejskiej.
Dlatego też nie wszystkie odrębności leksykalne Poznania typowe są wyłącznie dla niego, wiele z nich typowe jest dla całego zaboru pruskiego, z drugiej zaś strony wiele z nich ma charakter północnopolski (podobieństwa z dialektem mazowieckim) czy zachodniopolski, silne są też związki z dialektem małopolskim (zwłaszcza fonetyczne). Wśród słownictwa typowego dla Poznania można więc wyróżnić archaizmy, dialektyzmy, germanizmy i słownictwo miejskie. Pośród nich największe znaczenie mają dialektyzmy – klasyfikacja jednostek leksykalnych nie jest przy tym jednoznaczna, tak. np. archaizmy względem nieregionalnego języka ogólnego stanowić mogą jednocześnie bieżące słownictwo gwar wielkopolskich lub germanizmy.
Dialektyzmy, o największym (ok. 40% typowego i odrębnego od ogólnopolskiego słownictwa Poznania) stanowić mogą leksykę przejętą z gwar wielkopolskich, ale nie tylko, znaczna ich część typowa jest bowiem również dla innych dialektów i gwar języka polskiego, głównie Kujaw i Krajny (przez wiodące klasyfikacje także określane jako gwary dialektu wielkopolskiego) oraz innych gwar i dialektów polskich. Słownictwo przejęte z gwar wielkopolskich i dialektu wielkopolskiego w ogólności określić można jako słownictwo wąskoterytorialne, zaś słownictwo typowe także dla innych dialektów jako szerokoterytorialne.
Wśród leksyki wąskoterytorialnej (przez co rozumie się tu słownictwo typowe wyłącznie dla gwar Wielkopolski w sensie wąskim) wyróżnić można np.: "bręczeć" ("marudzić", "zrzędzić"), "bździągwa", "churchlać", "cyrać", "klejdry", "knajder", "koperytko", "labija", "leloszek", "obrzym", "pany", "ogigle", "opękać", "petronelka", "ryfa", "rzęchy", "szczapić", "szkieber", "szudrać się", "szuszwol", "szwagrocha", "ukrychnąć", "unorać", "wknaić się", "wyćpić"/"wyćpnąć", "wypiglać się", "zgrupić się", "żgak". Prócz gwar wielkopolskich w sensie wąskim także dla gwar Kujaw i Krajny charakterystyczne są np.: "chabas", "fleja", "głabnąć", "przesmradzać", "rupotać", "zgęziały".
Dla gwar Pomorza i regionów sąsiadujących charakterystyczne są także: "bachandryje", "boba", "brawęda", "chaps", "kramować", "lujnąć", "luntrus", "miągwa", "muk", "statory", "wąsiona", "westfalka". Dla Śląska – "biber", "giglać", "kalafa", "klekoty", "knajtek", "land", "modre", "przepękać", "smary", "tuleja". Dla Małopolski – "angryst", "bojączka", "glajda", "hycać", "makiełki", "nadrach", "nicpoty", "obachutać", "papcie", "puczyć się", "purtać", "ryska", "wykosierować". Dla Mazowsza i w pewnej mierze gwar Suwalszczyzny i Podlasia – "chorobny", "chrympać", "odtrzasnąć się", "przebrać", "resztówki", "szmaja", "taradeja". Dla wielu gwar terytorialnych Polski – "czępać"/"cząpać", "ćmok", "chichrać się", "glapa", "leżanka", "nyny", "ohajtnąć się", "paradzić się", "psiona", "tabula".
Drugą co do znaczenia (ok. 30%) grupą są archaizmy, wyrazowe (forma i znaczenie zanikły w języku ogólnym – w tym zapożyczenia) i semantyczne (forma obecna w języku ogólnym, ale przy zmienionym znaczeniu). Wśród archaizmów wyrazowych wyróżnić można np.: "deczka" i "deka", "grajcarek", "jaczka", "jadaczka", "kabatek", "kejter", "knyp", "korbol", "kopystka", "macoszka", "mączkować", "mrzygłód", "nieusłuchany", "skopowina", "szablak", "szpotawy", "sztyftować się", "tąpać", "westka", "węborek", "womitować". Wśród archaizmów semantycznych – "ból" ("rana", "wrzód"), "gapa" ("wrona"), "góra" ("strych"), "haczyk" ("pogrzebacz"), "mączka" ("krochmal"), "miałki" ("płytki"), "sklep" ("piwnica"), "skład" ("sklep"), "trafić" ("spotkać"), "wykład" ("wydatek").
Germanizmy stanowią ok. 30% słownictwa, przy czym podane tu przykłady uwzględnią prócz germanizmów właściwych także kalki słowotwórcze, frazeologiczne i semantyczne. Germanizmy właściwe to wyrazy, w których formę przejęto z języka niemieckiego razem ze znaczeniem, jak "ajnfach", "ajntop(f)", "bachać się", "badejki", "bauer", "blubrać", "bryle", "dracheta", "durch", "dyngs", "eka", "frechowny", "fyrtel", "glaca", "kipa", "kista", "laczki", "lajsnąć sobie", "lofer", "lumpy", "pana", "plindz"/"plendz", "przyzolić", "racha", "rajzefiber", "redyska", "rodle", "rojber", "rozkwirlać", "sosyska", "sportka"/"szportka", "szneka", "sznupa", "sznytloch", "sztender", "sztrykować", "szwaja", "szwamka", "tonkać", "zicherhajtka". Kalki słowotwórcze to wyrazy o zachowanej niemieckiej strukturze słowotwórczej, w których jednak morfemy niemieckie zastąpione zostały polskimi: "obkład", "odkluczyć", "przepisać się", "tudotąd", "zakluczyć". Podobnie kalki frazeologiczne to frazeologizmy o strukturze niemieckiej, ale polskich lub mieszanych co do pochodzenia słowach: "być na fleku" ("być w pełni sił"), "dostać kupić" ("móc kupić"), "mieć ambę" ("mieć głupie pomysły"), "mieć sztycha" ("być nadpsutym" – o mięsie), "przyjść komuś głupio", "przygadać", "dociąć komuś", "robić komuś hałas" ("robić komuś awanturę"), "spuścić się na kogoś" ("polegać na kimś"). Kalki semantyczne to wyrazy, w których zapożyczone z języka niemieckiego zostało jedynie znaczenie: "kij" ("piętro"), "pojedynczy" ("prosty", "zwyczajny"), "przypominać się" ("odbijać się"), "skrzydło" ("fortepian").
Niewielką część słownictwa tworzy natomiast słownictwo typowo miejskie, o charakterze neologizmów. Do neologizmów miejskich Poznania należą np.: "bimba" ("tramwaj"), "deska" ("długi blok mieszkalny"), "pestka" (Poznański Szybki Tramwaj), "okrąglak" (w odniesieniu do domu towarowego o okrągłym kształcie).
Stan badań i zagadnienia lokalności językowej.
Badania nad odrębnością językową miast, także Poznania, podjęte zostały w językoznawstwie polskim w okresie XX-lecia międzywojennego. Odrębności te były wtedy rozpatrywane często w kategoriach tzw. błędów językowych. Wyniki tych badań przedstawiali w formie artykułów Antoni Danysz ("Odrębności słownikarskie kulturalnego języka polskiego w Wielkopolsce w stosunku do kulturalnego języka w Galicyi"), Kazimierz Nitsch ("Odrębności słownikowe Poznania, Krakowa i Warszawy"), J. Biliński ("Błędy językowe"), E. Klich, P. Ciuła i W. Czarnecki ("Przyczynki do gwary uczniowskiej w Poznaniu i Trzemesznie"), A. Szyperski ("Błędy językowe w Wielkopolsce"; "Mowa zapomniana. O archaizmach w Wielkopolsce"), A. Tomaszewski ("Błędy językowe uczniów szkół poznańskich"; "Mowa ludu wielkopolskiego").
Gwarę badała prof. Monika Gruchmanowa, która opublikowała kilka książek, w tym słownik.
Gwarę poznańską propagowały słuchowiska z cyklu:
Ponadto poznański dom kultury Jubilat od lat 80. organizuje coroczny amatorski konkurs gwary "Godejcie po naszymu", w którym niemałe sukcesy odnosi nawet młodzież licealna. W 2015 Juliusz Kubel przełożył na gwarę poznańską "Małego Księcia", nadając mu tytuł "Książę Szaranek"
Poznański raper Peja (Ryszard Andrzejewski) w swoich tekstach także używa wiele elementów gwary poznańskiej, np. „winkiel”. Sam pseudonim artystyczny Andrzejewskiego oznacza „wesz”. Paluch (poznański raper) często używa w swoich utworach słowa „Tej”. Również poznański zespół rapowy Aifam używa wiele elementów gwary poznańskiej, np. „Wuchta wiary”, „szczon”.

</doc>
<doc id="1725" url="https://pl.wikipedia.org/wiki?curid=1725" title="Geografia Polski">
Geografia Polski

Polska jest krajem w większej części nizinnym, na południu występują również tereny wyżynne i pasma górskie. Polska jest położona w Europie Środkowej, nad brzegiem Morza Bałtyckiego.
Powierzchnia i punkty skrajne.
Powierzchnia Polski wynosi 322 575 km², w tym: obszar lądowy (wraz z wodami śródlądowymi) – 311 888 km², morskie wody wewnętrzne – 2005 km², morze terytorialne – 8682 km². W Europie większą powierzchnię mają: Rosja, Ukraina, Francja, Hiszpania, Szwecja, Niemcy, Finlandia i Norwegia.
Punkty skrajne.
Bez baz wojskowych, placówek dyplomatycznych i stacji naukowych:
Środek Polski.
Środek geometryczny Polski znajduje się w mieście Piątek, 15 km na wschód od Łęczycy, 19 km na południe od Kutna, a 33 km na północ od Łodzi.
Granice Polski.
Polska graniczy z siedmioma państwami (w nawiasie długość granicy z tym państwem)
Długość granicy lądowej wynosi 3071 km.
Długość granicy morskiej wynosi 440 km, przy czym długość wybrzeża morskiego Bałtyku (wraz z Zalewem Szczecińskim i Wiślanym) wynosi 775 km.
Całkowita długość granic wynosi 3511 km.
Przeszłość i budowa geologiczna Polski.
W początkach prekambru (w eonie archaicznym) rozpoczęło się formowanie skorupy ziemskiej. W eonie proterozoicznym kształtowaniu się litosfery towarzyszyły ruchy górotwórcze oraz zjawiska wulkaniczne. Uformował się szereg tzw. platform kontynentalnych, będących najstarszymi częściami kontynentu. Jedną z nich jest platforma wschodnioeuropejska, na której znajduje się wschodnia część obecnego obszaru Polski.
W początkach paleozoiku (w kambrze) część obecnego obszaru zajmowało morze. Powstały w nim grube pokłady wapieni (na terenie obecnych Sudetów) oraz piaskowców i łupków (na terenie obecnych Gór Świętokrzyskich). W ordowiku i sylurze miała miejsce orogeneza kaledońska, doprowadziła ona do częściowego sfałdowania Sudetów i Gór Świętokrzyskich. W dewonie klimat uległ ociepleniu. Pod koniec tego okresu rozpoczęła się orogeneza hercyńska – ponowne sfałdowanie Gór Świętokrzyskich i Sudetów, która miała miejsce także w karbonie. Podczas jej trwania powstały złoża węgla kamiennego oraz góry wulkaniczne. W permie środkowa i północna część Polski była zalana morzem. Osadzały się w nim margle i wapienie, tworzyły pokłady soli kamiennej i potasowej. Wtedy też powstały złoża ropy naftowej i miedzi.
W erze mezozoicznej tworzyły się skały osadowe: wapienie, dolomity, piaskowce. W erze tej rozpoczęły się kolejne ruchy górotwórcze – trwająca do dziś orogeneza alpejska.
Orogeneza alpejska doprowadziła w erze kenozoicznej, dokładnie w paleogenie do wydźwignięcia środkowej części obszaru Polski – powstał tam ląd. Natomiast w neogenie prawie cały obszar Polski został wzniesiony ponad poziom morza. Najwyżej wzniesiony został obszar Karpat. Pod koniec neogenu morze na terenie Polski całkowicie wycofało się. W pierwszym okresie czwartorzędu (plejstocen) nadeszła epoka lodowcowa, lód pokrył prawie całą Polskę.
Ukształtowanie terenu.
Ponieważ w Polsce zdecydowanie przeważają tereny nizinne (poniżej 200 m n.p.m.), zajmujące aż 75% powierzchni kraju, średnia wysokość wynosi tylko 173 m n.p.m., mediana 149 m n.p.m. Niziny występują na północy i w centrum, natomiast obszary górskie i wyżynne na południu. Polska jest jednym z kilku krajów Europy posiadające pojezierza i obok Niemiec posiada największe pradoliny.
Teren Polski pochyla się z południa ku północnemu zachodowi. Wysokość nie maleje jednak stopniowo, lecz skokowo, tzn. na przemian występują krainy niżej i wyżej wzniesione, które układają się pasami ciągnącymi się z zachodu na wschód. Zaczynając od południa wyróżnia się następujące pasy: gór, kotlin, wyżyn, nizin środkowopolskich, pojezierzy i pobrzeży.
Punkty najniższy i najwyższy znajdują się na przeciwległych krańcach kraju. Najniższy położony jest na wysokości 2,07 m p.p.m., w miejscowości Marzęcino na Żuławach Wiślanych. Najwyższym punktem są Rysy w Tatrach, których wierzchołek sięga 2499 m n.p.m. (w latach 1938-1939 najwyższym punktem był Lodowy Szczyt) W porównaniu z rekordami światowymi wielkości te nie są imponujące: najniżej położone miejsce na lądzie (tafla Morza Martwego) znajduje się o 415,9 m niżej, natomiast najwyższy szczyt (Mount Everest) wznosi się o 6349 m wyżej.
Pokrywa glebowa.
Pokrywa glebowa w Polsce, podobnie jak niektóre inne elementy środowiska, ma cechy przejściowe pomiędzy glebami charakterystycznymi dla Europy Zachodniej i Wschodniej. Powierzchniowo dominują gleby strefowe (powstałe pod wpływem klimatu), jednak tworzą one na terenie kraju mozaikę zależną głównie od podłoża geologicznego (skały macierzystej) oraz ukształtowania powierzchni. Około 52% powierzchni zajmują gleby płowe oraz gleby brunatnoziemne, zaś ok. 26% powierzchni zajmują, powstałe na utworach piaszczystych, gleby rdzawe, bielicowe i bielice. W dolinach rzecznych dominują mady (ok. 5% powierzchni), w miejscach podmokłych lub wilgotnych zaś można spotkać gleby organiczne (torfowe, murszowe), gleby glejowe i czarne ziemie. Wyraźne obszary zajmują również, powstające na skałach węglanowych – rędziny, najbardziej urodzajne na obszarze Polski – czarnoziemy, charakterystyczne dla obszarów górskich – gleby inicjalne i słabo ukształtowane, a także powstałe pod dominującym wpływem człowieka – gleby antropogeniczne.
W Polsce grunty rolne i leśne klasyfikuje się również pod kątem użytkowym. Według bonitacyjnej klasyfikacji gleb ornych i użytków zielonych w kraju powierzchniowo dominują gleby średnie (klasa IIIa-IVb – 63% gruntów ornych i klasa III i IV – 51,2% użytków zielonych), zaś najmniej jest gleb najlepszych (klasa I i II – 3,7% gruntów ornych i 1,7% użytków zielonych).
Rzeki i jeziora.
99,7% (312 683 km²) obszaru Polski leży w zlewisku Morza Bałtyckiego (53,9% dorzecze Wisły, 34,7% dorzecze Odry, 11% rzeki bezpośredniego dorzecza Bałtyku i 0,8% dorzecze Niemna), a poza tym w zlewisku Morza Czarnego (Orawa, Strwiąż) i Morza Północnego (Dzika Orlica, Izera).
Najdłuższe rzeki w Polsce to
Na terenie Polski znajduje się znaczna liczba jezior – według aktualnych danych 7081 o powierzchni powyżej 1 ha. Stanowią one jednak zaledwie 0,9% powierzchni kraju.
Klimat.
Zgodnie z klasyfikacją Köppena, obszar Polski leży w strefie wilgotnego klimatu kontynentalnego ("Dfb"). Jej klimat jest też określany jako przejściowy pomiędzy ciepłym i dżdżystym klimatem umiarkowanym, a śnieżno-leśnym klimatem borealnym. Nad obszarem Polski ścierają się różne masy powietrza, co jest wynikiem położenia w centrum Europy oraz równoleżnikowego układu krain geograficznych.
Przeważający obszar Polski znajduje się w strefie mrozoodporności "6": od "5a" na północnym wschodzie, poprzez "6a" na wschodzie i "6b" w centrum, po "7a" na samym zachodzie i nad samym morzem oraz "7b" na północno-zachodnim krańcu kraju.
Największy wpływ na klimat Polski mają masy powietrza polarno-morskiego i polarno-kontynentalnego, decydujące o przejściowości klimatu polskiego.
Nad Polskę napływają również masy powietrza, arktycznego, zwrotnikowego-morskiego i kontynentalnego, mające mniejszy wpływ na kształtowanie klimatu. Masy powietrza polarno-morskiego powodują latem zachmurzenie, ochłodzenie i wzrost wilgotności, zimą zaś przynoszą ocieplenie, odwilż i mgły.
Masy powietrza polarno-kontynentalnego latem przynoszą piękną, suchą i upalną pogodę, a zimą pogodę słoneczną, suchą i duże mrozy. Masy powietrza zwrotnikowo-morskiego znad Morza Śródziemnego i Azorów napływają nad Polskę rzadziej, przynosząc latem upały i częste burze, a zimą gwałtowne odwilże.
Masy powietrza zwrotnikowo-kontynentalnego napływają znad Azji Mniejszej i Bałkanów bardzo rzadko, przeważnie latem i wczesną jesienią. Przynoszą piękną, suchą pogodę („złota polska jesień”).
Masy powietrza arktycznego napływają nad Polskę:
Średnie opady ok. 500–600 mm rocznie. Rozkład opadów w ciągu roku jest nierównomierny, 2/3 opadów rocznych to opady półrocza letniego. Najmniejsze opady odnotowywane są we wschodniej Wielkopolsce, Kujawach i północno-zachodnim Mazowszu, gdzie wynoszą 450–500 mm rocznie, co jest skutkiem cienia opadowego Pojezierza Pomorskiego, gdzie spada 600–700 mm. Podobne ilości opadów otrzymują obszary Pojezierza Mazurskiego. Opady na wyżynach środkowopolskich wynoszą ok. 800 mm. Największe wartości opadowe przypadają na tereny wysokogórskie i osiągają od 1200 do 1500 mm rocznie (skrajnie do 1900 mm), z tym że w górach są to głównie opady śniegu. Polska leży w strefie wiatrów zmiennych z przewagą wiatrów zachodnich (północno-zachodnich i południowo-zachodnich), których udział stanowi ok. 60%. Wiatry wschodnie wieją głównie zimą, rzadsze są natomiast wiatry wiejące z południa i północy.
Klimatogramy wybranych miast w Polsce:
Ekstrema historyczne.
Zanotowane w historii ekstremalne wskaźniki pogodowe:
Regiony fizycznogeograficzne.
Podział ten stanowi fragment międzynarodowej klasyfikacji regionów fizycznogeograficznych Europy.
Obszary piaszczyste.
W Polsce są trzy obszary piaszczyste, potocznie zwane pustyniami

</doc>
<doc id="1728" url="https://pl.wikipedia.org/wiki?curid=1728" title="Glukoza">
Glukoza

Glukoza, -glukoza – organiczny związek chemiczny, monosacharyd (cukier prosty) z grupy aldoheksoz. Jest białym, drobnokrystalicznym ciałem stałym, z roztworów wodnych łatwo krystalizuje jako monohydrat. Jest bardzo dobrze rozpuszczalna w wodzie (nie zmienia pH roztworu). Ma słodki smak, nieco mniej intensywny od sacharozy.
Enancjomerem -glukozy jest -glukoza, niewystępująca w organizmach wyższych.
Występowanie.
Glukoza powstaje w organizmach roślinnych podczas fotosyntezy:
Występuje w znacznych ilościach w owocach (szczególnie w winogronach, stąd nazywana jest czasem "cukrem gronowym") i miodzie. W organizmach jest składowana w postaci dwóch polimerów: skrobi u roślin i glikogenu u zwierząt.
Budowa cząsteczki.
Krystaliczna glukoza ma budowę cykliczną. Sześcioczłonowy pierścień hemiacetalu powstaje w wyniku reakcji grupy aldehydowej przy atomie węgla C1 z grupą hydroksylową przy atomie węgla C5 w łańcuchu. Możliwe są zatem dwa anomery – α oraz β w zależności od położenia grupy OH przy pierwszym atomie węgla formy cyklicznej.
W roztworze wodnym oba anomery mogą ulegać reakcji otwarcia pierścienia i ponownego zamknięcia z utworzeniem ponownie takiego samego lub przeciwnego anomeru. Proces taki nazywa się mutarotacją. Forma otwartołańcuchowa występuje wyłącznie w roztworze wodnym, a jej termodynamiczna stabilność jest znacznie mniejsza od formy cyklicznej, dlatego równowaga procesu mutarotacji przesunięta jest silnie w kierunku formy cyklicznej, która występuje w przewadze.
W łańcuchowej formie glukozy znajdują się cztery asymetryczne atomy węgla (tj. C2, C3, C4 i C5), podczas gdy w formie cyklicznej występuje pięć takich atomów (dodatkowo asymetryczny jest węgiel anomeryczny, C1). W zależności od położenia grupy karbonylowej w łańcuchu i konfiguracji asymetrycznych atomów węgla, możliwych jest kilkadziesiąt izomerów glukozy (zob. heksozy). Jednym z często występujących naturalnie izomerów glukozy jest fruktoza, różniąca się położeniem grupy karbonylowej.
Właściwości.
Właściwości chemiczne.
Naturalna glukoza jest jednym z wielu izomerów optycznych tego związku – α--glukopiranozą (dekstrozą). Jest ona czynna optycznie – w temperaturze pokojowej skręca płaszczyznę spolaryzowanego światła białego w prawo. Skręcalność właściwa anomeru α wynosi +112°, natomiast anomeru β +19°. W roztworze wodnym w wyniku mutarotacji ustala się stan równowagi obu anomerów w stosunku 36:64, o skręcalności właściwej +52,6°.
Glukoza w przeciwieństwie do fruktozy odbarwia wodę bromową w obecności kwaśnego węglanu sodu (następuje utlenienie grupy aldehydowej do grupy karboksylowej, w wyniku czego powstaje kwas glukonowy):
Pod wpływem kwasu azotowego grupa aldehydowa oraz atom węgla C6 są utleniane do grup karboksylowych, w wyniku czego powstaje kwas glukarowy (który w gorącej wodzie przechodzi w lakton).
W temperaturze powyżej 150 °C topi się i brunatnieje, tworząc (podobnie jak sacharoza) karmel.
Właściwości biologiczne.
-Glukoza jest podstawowym związkiem energetycznym dla większości organizmów, przechowywanym pod postacią polimerów – skrobi i glikogenu. Stanowi cukier najłatwiej przyswajalny przez człowieka. W procesie glikolizy jest rozkładana na kwas pirogronowy. Jest także substratem wielu procesów zachodzących w komórce, a u roślin, jako celuloza, podstawowym budulcem ściany komórkowej.
Wykrywanie glukozy w roztworach.
Glukozę, podobnie jak inne cukry redukujące, można wykryć m.in. za pomocą prób Trommera i Tollensa. W obydwu próbach glukoza jest przeprowadzana w kwas glukonowy.
Próba Trommera.
Do świeżo strąconego osadu wodorotlenku miedzi(II) dodaje się badany roztwór i ogrzewa. W obecności glukozy niebieski osad Cu(OH)2 zmienia barwę na ceglastoczerwoną, wskutek redukcji do :
Próba Tollensa.
Do probówki napełnionej niewielką ilością roztworu azotanu srebra należy dodać kilka kropli stężonego roztworu wodorotlenku sodu (NaOH), a następnie dolewać kroplami roztworu amoniaku (), aż do rozpuszczenia się powstałego wcześniej osadu.
Tak przygotowaną probówkę należy umieścić w zlewce z gorącą wodą (na łaźni wodnej). Na koniec do probówki dodaje się badany roztwór. Podczas ogrzewania substancji o barwie brunatnej – (powstałej z reakcji z NaOH) – na ściankach probówki osadza się warstewka metalicznego srebra – lustro srebrowe (z tej przyczyny próba Tollensa zwana jest także "próbą lustra srebrnego").
Zastosowanie.
W medycynie (np. kroplówki), a także w przemyśle spożywczym (np. syrop glukozowy, syrop glukozowo-fruktozowy) oraz farmaceutycznym. Stosowana jest również jako dodatek o właściwościach nawilżających do kosmetyków, np. przeznaczonych do makijażu, pielęgnacji skóry i włosów.

</doc>
<doc id="1729" url="https://pl.wikipedia.org/wiki?curid=1729" title="Helowce">
Helowce

Helowce (gazy szlachetne) – pierwiastki chemiczne ostatniej, 18 (daw. 0 lub VIII głównej) grupy układu okresowego. Do pierwiastków tych zalicza się hel, neon, argon, krypton, ksenon i radon. Prawdopodobnie gazem szlachetnym jest również syntetyczny pierwiastek oganeson.
Właściwości.
Wszystkie helowce są bezbarwnymi, bezwonnymi, słabo rozpuszczającymi się w wodzie gazami. Mają niskie temperatury topnienia.
Pierwiastki te są wysoce niereaktywne. Wynika to z faktu, że nie zawierają one żadnych częściowo zapełnionych elektronami orbitali, które mogłyby uczestniczyć w tworzeniu wiązań chemicznych. W dół grupy powoli zwiększa się reaktywność tych pierwiastków. Spowodowane jest to tym, że zwiększa się ich rozmiar, a z nim odległość elektronów walencyjnych od środka jądra.
Pomimo bardzo niskiej reaktywności lżejszych helowców, znane są nietrwałe związki helu: cząsteczka He2, jon HeH+ i LiHe. Na przełomie 2016/2017 doniesiono o otrzymaniu pierwszego stałego związku helu, Na2He, który jest trwały termodynamicznie pod ciśnieniem powyżej 113 GPa (ok. 1,1 atm). Tworzy kryształy o strukturze fluorytu.
Znany jest też związek argonu – fluorowodorek argonu, otrzymany w temperaturze około 40 K.
Krypton i ksenon tworzą większą liczbę związków, lecz są one często trudne do uzyskania i niekiedy nietrwałe w temperaturze pokojowej. Działając ksenonem na heksafluorek platyny Neil Bartlett (opierając się na wcześniejszym odkryciu wraz z Lohmannem heksafluoroplatynianu dioksygenylu) otrzymał trwały czerwonopomarańczowy związek, heksafluoroplatynian ksenonu.
Stosunkowo najbardziej reaktywny jest radon. Istnieje trwały chemicznie związek – fluorek radonu RnF2. Powstaje on z mieszaniny fluoru i radonu w temperaturze ok. 400 °C. Cząstka ta rozpada się jednak ze względu na radioaktywność radonu.
Hel, neon, argon i ksenon występują w niewielkich ilościach w powietrzu i dlatego podstawowym sposobem ich otrzymywania jest destylacja frakcyjna powietrza. Krypton i radon są produktami rozpadu promieniotwórczego uranu i toru, towarzyszą zwykle złożom rud tych metali, dzięki czemu można te złoża stosunkowo łatwo wykrywać.
Gazy szlachetne w niskich temperaturach tworzą kryształy związane słabymi oddziaływaniami van der Waalsa. Hel przy zerowym ciśnieniu nie krystalizuje nawet w temperaturach bliskich 0 K; dominującą rolę odgrywa tu energia drgań zerowych, która destabilizuje kryształ. Drgania zerowe powodują też zmniejszenie wartości energii kohezji kryształów w porównaniu z sumaryczną energią oddziaływań van der Waalsa. Ze względu na zależność energii drgań zerowych od masy to zmniejszenie jest tym większe, im mniejsza jest masa atomu (np. dla neonu zmniejsza ją o wartość 28%, a dla ksenonu – o 4%). Skutkuje to również różną wartością stałej sieci dla kryształów różnych izotopów danego gazu szlachetnego. Gazy szlachetne, z wyjątkiem helu, krystalizują w strukturze regularnej gęstego upakowania (fcc).
Zastosowanie.
Gazy szlachetne wykorzystuje się wszędzie tam, gdzie potrzebna jest obojętna, beztlenowa atmosfera zapobiegająca reakcjom utleniania. Stosuje się je do napełniania żarówek, do prowadzenia reakcji wymagających obojętnych warunków i do spieniania tworzyw sztucznych.
Żaden z nich nie uczestniczy w procesach biologicznych, dlatego często używa się ich jako obojętnych gazów zapobiegających starzeniu się materiałów pochodzenia organicznego. Dla przykładu, oryginał Konstytucji oraz Deklaracji niepodległości Stanów Zjednoczonych przechowuje się w szczelnej gablocie wypełnionej helem.

</doc>
<doc id="1730" url="https://pl.wikipedia.org/wiki?curid=1730" title="Góra (ujednoznacznienie)">
Góra (ujednoznacznienie)

Miejscowości i ich części w Polsce.
W Polsce jest 268 miejscowości o nazwie Góra, w tym jedno miasto, 23 miejscowości podstawowe niebędące miastami. Wśród 244 integralnych części miejscowości 9 są częściami miast, a 235 częściami pozostałych miejscowości.

</doc>
<doc id="1731" url="https://pl.wikipedia.org/wiki?curid=1731" title="Geometria">
Geometria

Geometria (gr. γεωμετρία; "geo" – ziemia, "metria" – miara) – jedna z głównych dziedzin matematyki; tradycyjnie i nieformalnie definiowana jako nauka o przestrzeni i jej podzbiorach zwanych figurami. W znaczeniu precyzyjnym i ogólnym jest to nauka badająca dla wybranych przekształceń ich niezmienniki, zwłaszcza inne niż moc zbioru czy niezmienniki topologiczne. W zależności od rodzaju przestrzeni i przekształceń mówi się o różnych rodzajach geometrii.
Do XIX wieku geometria badała wyłącznie przestrzenie euklidesowe wymiaru nie większego niż trzy oraz odpowiadające im przestrzenie rzutowe. W takich przestrzeniach można zdefiniować relacje jak równoległość prostych, współliniowość punktów i wielkości jak odległość czy miara kąta, a przez to zachowujące je przekształcenia – odpowiednio afiniczne, rzutowe, izometrie i podobieństwa. Zależności te opisują geometrie afiniczna, rzutowa i euklidesowa. Ta ostatnia jest też historycznym źródłem innych pojęć jak krzywa i jej długość, a także wymiar, pole powierzchni, objętość czy krzywizna; ich uściślenie wymagało jednak metod topologii i analizy, zwłaszcza teorii miary. Tę geometrię przestrzeni euklidesowych niskich wymiarów – niezależnie od badanych niezmienników – tradycyjnie dzieli się też na planimetrię i stereometrię. Obie doczekały się własnych poddziedzin jak trygonometria, geometria sferyczna, wykreślna czy absolutna. Geometrię w tym historycznym znaczeniu można uprawiać zarówno w sposób syntetyczny („tradycyjny”), jak i powstały później analityczny – oparty na współrzędnych, zwykle kartezjańskich.
Geometria, tak jak arytmetyka, należy do najstarszych nauk i tak jak ona pozostaje wiecznie żywa. Już od swoich początków te dwie dziedziny wchodzą w nieustanne interakcje; oprócz tego geometria przyczyniła się do powstania innych dyscyplin jak algebra, analiza, teoria grafów czy topologia. Rozwinięta przez Euklidesa metoda aksjomatyczna była wzorcem dla różnych dziedzin, także fundamentalnych jak logika matematyczna i teoria mnogości. Geometria dostarczyła też problemów probabilistyce, którą finalnie oparto na pojęciu miary o geometrycznym rodowodzie. Te obszary „potomne” względem geometrii mocno wpłynęły na nią samą – jej formalizm, metody i zakres badań. Ten ostatni od czasów starożytnych bardzo się poszerzył; najpóźniej w XVII wieku oprócz ściśle rozumianych przestrzeni euklidesowych wprowadzono ich rzutowe odpowiedniki, a XIX wiek przyniósł prawdziwą eksplozję tematyki – przez rozważania wyższych wymiarów, geometrii nieuuklidesowych i obejmujących je przestrzeni Riemanna. Uogólnienia poszły jeszcze dalej, przez pojęcia rozmaitości i przestrzeni metrycznych, wykraczające poza geometrię. Wprowadzono także przestrzenie innego typu – skończone jak płaszczyzna Fana czy nawet bezpunktowe.
Geometria jest podstawą różnych nauk przyrodniczych i technicznych – między innymi fizyki z astronomią, pogranicza chemii fizycznej (krystalografia), geodezji z kartografią, budownictwa z architekturą czy inżynierii mechanicznej. Rola geometrii dosięga też innych dziedzin kultury jak sztuka – zwłaszcza sztuki wizualne – czy filozofia. Matematyk zajmujący się geometrią to geometra. Słowo to oznacza również – zwłaszcza historycznie – mierniczego związanego z geodezją, a „geometria” aż do XIX w. była synonimem całej matematyki. Geometrom "sensu stricto" wielokrotnie przyznawano najwyższe wyróżnienia dostępne matematykom jak Medal Fieldsa, Nagroda Abela czy – wręczany naukowcom różnych dyscyplin – Medal Copleya.
Historyczny rozwój.
Starożytność i średniowiecze.
Geometria – podobnie jak inne działy matematyki – wyewoluowała od badania obiektów znanych z życia codziennego, w jej wypadku kształtów. Zajmowali się nią już starożytni mieszkańcy Mezopotamii (III tysiąclecie p.n.e.) i Egiptu (II tysiąclecie p.n.e.). Znali oni podstawowe fakty z tej dziedziny jak twierdzenie Pitagorasa; już tam geometria dostarczyła tematów teorii liczb jak trójki pitagorejskie, a także przyczyniła się do prapoczątków algebry przez problem równań kwadratowych. Wtedy pojawiły się też zgrubne oszacowania liczby pi (π): 3+1/8 = 3,125 albo (4/3)4 ≈ 3,16.
Systematyczny i ściślejszy rozwój geometrii, oparty na definicjach i dowodach, nastąpił potem w starożytnej Grecji. Proces ten trwał prawie tysiąclecie, od okresu klasycznego do późnego Cesarstwa Rzymskiego. Postępy były wielorakie:
Równolegle rozwijano geometrię w Chinach; w III w. n.e. Liu Hui obliczył liczbę pi z dokładnością wyższą niż Archimedes.
XVI i XVII wiek.
W XVI wieku Adriaan van Roomen podał nowe rozwiązanie starożytnego problemu Apoloniusza, jednak jego metoda wykraczała poza konstrukcje klasyczne. To samo stulecie przyniosło też opis nowej krzywej sferycznej: loksodromy, a także odwzorowania walcowego Mercatora w kartografii.
Najpóźniej w XVII wieku spleciono geometrię z algebrą przez narodziny geometrii analitycznej. Wtedy też – między innymi na potrzeby geometryczne – narodziły się podstawy analizy, a za nią geometria różniczkowa. Nowa dyscyplina doprowadziła do:
XVII wiek to też początki właściwej geometrii rzutowej dzięki pracom Gérarda Desargues’a i Blaise’a Pascala. 
XVIII wiek.
W tamtym stuleciu geometria podążała głównie w kierunkach wyznaczonych wcześniej – rozwijano euklidesową planimetrię i stereometrię, stosując zarówno metody klasyczne (syntetyczne), jak i nowożytne techniki algebry oraz analizy. Przykładowo:
Badania Eulera nad wielościanami doprowadziły do powstania teorii grafów, a koncepcje geometryczne zastosowano w algebrze (płaszczyzna zespolona). Johann Heinrich Lambert udowodnił niewymierność liczby pi, co było pierwszym krokiem do wykazania w XIX wieku, że kwadratura koła i rektyfikacja okręgu nie są możliwe metodami klasycznymi.
XIX wiek.
XIX wiek przyniósł rewolucję – z jednej strony zaczęto rozważać przestrzenie euklidesowe wyższych wymiarów, a z drugiej pojawiły się badania geometrii nieeuklidesowych jak hiperboliczna (Łobaczewskiego) i eliptyczna. Te dwa kierunki uogólnień połączono przez dużo szersze pojęcie rozmaitości Riemanna. Doprecyzowanie starych pojęć geometrii, zwłaszcza różniczkowej, a także klasyfikacja ogromu nowo rozważanych przestrzeni stworzyły topologię. Pojęcie przestrzeni stało się przez to dużo szersze od pierwotnego znaczenia, abstrakcyjne, obejmujące też np. dowolne przestrzenie metryczne czy liniowe, a wymiar niektórych z nich sięgnął nieskończoności. Poszczególnymi rodzajami przestrzeni i ich niektórymi aspektami zajęły się nauki traktowane jako odrębne od geometrii jak topologia czy algebra liniowa. Przez to geometrię zdefiniowano na nowo, w ramach programu erlangeńskiego – właśnie jako teorię niezmienników, zwłaszcza innych niż te topologiczne. Analiza niezmienników jest też podstawą badania innych obiektów matematycznych (np. przestrzenie topologiczne czy struktury algebraiczne), a wśród niezmienników mogą być pojęcia bardzo ogólne i abstrakcyjne jak punkt stały.
Geometria XIX-wieczna wydała też inne owoce:
XX i XXI wiek.
Wiek XX przyniósł jeszcze nowsze, „egzotyczne” obszary badań jak geometria skończona i nieprzemienna. Rozwinięto też wcześniejsze kierunki jak geometria algebraiczna czy fraktalna; geometrię zastosowano również w teorii katastrof i topologii (hipoteza geometryzacyjna Thurstona).
Postępy nastąpiły również na drugim biegunie abstrakcji, w klasycznej planimetrii wielokątów i innych prostych figur:
Rozwiązano też problemy bliskie geometrii rozumianej klasycznie, choć klasyfikowane inaczej – przykładem jest zagadnienie czterech barw w teorii grafów planarnych.
Od początku XXI wieku udało się między innymi ostatecznie udowodnić postulat Keplera. Grigorij Perelman w 2003 roku udowodnił hipotezę Thurstona, a przez to wynikającą z niej hipotezę Poincarégo w pierwotnym, trójwymiarowym przypadku. Był to tryumf metod geometrycznych w topologii, uważanej za bardziej ogólną i w pewnym sensie bardziej fundamentalną od geometrii. Mimo to dalej bez odpowiedzi pozostają niektóre pytania „przyziemne” i „prozaiczne”, zadane elementarnie jak:
Aksjomaty Euklidesa.
Geometria powstała w starożytności. W swych początkach była zbiorem przepisów wykonywania pomiarów przedmiotów materialnych. Pierwsze próby formułowania twierdzeń geometrii pojawiły się w VI wieku p.n.e. w starożytnej Grecji (Tales z Miletu). Kompilacją poznanych do III wieku p.n.e. faktów jest dzieło Euklidesa "Elementy" (ok. 300 p.n.e.). Obejmuje ono teorię proporcji, arytmetykę oraz geometrię. Jest pierwszym dedukcyjnym wykładem geometrii w historii matematyki. Wszystkie twierdzenia są wyprowadzone zgodnie z tradycyjnymi regułami logiki na podstawie przyjętych pojęć pierwotnych i aksjomatów, których było pięć. Jest to również pierwsza aksjomatyczna teoria w historii matematyki. Aksjomatyzacja arytmetyki pojawiła się wiele wieków później.
Momentem przełomowym w rozwoju geometrii było opublikowanie w XVII w. przez matematyka francuskiego Kartezjusza pracy "La géométrie", (1637), co zapoczątkowało rozwój geometrii analitycznej. W pracy tej Kartezjusz wprowadził do geometrii metody algebraiczne. Niezależnie i nieco wcześniej uczynił to także Pierre de Fermat, który jednak nie opublikował swych wyników.
Geometrie nieeuklidesowe.
Pięć aksjomatów podanych przez Euklidesa przez dwa tysiąclecia stanowiło podstawę budowy geometrii. Dopiero w drugiej połowie XIX w. stwierdzono, że nie są one wystarczające. W roku 1882 matematyk niemiecki Moritz Pasch podał konieczne uzupełnienia. Pełny zestaw aksjomatów geometrii euklidesowej wraz z dowodem niesprzeczności tego systemu opublikował w 1899 matematyk niemiecki David Hilbert. Jednym z mniej oczywistych aksjomatów sformułowanych przez Euklidesa jest piąty (ostatni) aksjomat o równoległych, zwany często aksjomatem lub pewnikiem (również postulatem) Euklidesa. Jest on równoważny m.in. następującemu twierdzeniu: suma miar kątów wewnętrznych trójkąta jest równa mierze kąta półpełnego. Przez wiele wieków próbowano wyprowadzić ten aksjomat z pozostałych aksjomatów podanych przez Euklidesa. Próby te (które, jak dziś wiadomo, nie mogły przynieść sukcesu) przyczyniły się do rozwoju innych teorii, a także do powstania geometrii innych niż euklidesowa.
Geometrie te noszą nazwę geometrii nieeuklidesowych, a wspólną ich cechą jest to, że nie jest w nich spełniony piąty aksjomat Euklidesa (przykładami mogą tu być geometria hiperboliczna i geometria eliptyczna). Jedna z takich geometrii, geometria Riemanna, została zastosowana przy konstruowaniu ogólnej teorii względności. Teoria oparta na aksjomatach geometrii euklidesowej bez aksjomatu Euklidesa nazywa się geometrią absolutną. W geometrii absolutnej można wprowadzić na przykład odległość punktów i długość odcinka. Do geometrii absolutnej należą te twierdzenia, które są prawdziwe zarówno w geometrii euklidesowej, jak i w geometrii, w której prawdziwe jest zaprzeczenie piątego aksjomatu.
Powstanie rachunku różniczkowego i całkowego dało początek geometrii różniczkowej. Podwaliny geometrii różniczkowej stworzył szwajcarski matematyk i fizyk Leonhard Euler, a rozwinął ją w znacznym stopniu niemiecki matematyk i fizyk Carl Friedrich Gauss. Pod koniec XVIII wieku powstała geometria wykreślna obejmująca metody graficznego przedstawiania figur przestrzennych na płaszczyźnie. Jednocześnie skrystalizowała się geometria rzutowa, której pewne twierdzenia (na przykład twierdzenie Desargues’a) znane były już wcześniej. Do dalszego rozwoju geometrii duży wkład wniósł matematyk niemiecki Bernhard Riemann, który w 1854 roku dzięki użyciu metod geometrii różniczkowej ogłosił nową teorię. Zaproponował zastąpienie pojęcia płaszczyzny pojęciem powierzchni oraz pojęcia prostej pojęciem linii geodezyjnej, tj. takiej krzywej, leżącej na powierzchni, której łuk o końcach "P", "Q" jest najkrótszym z leżących na powierzchni łuków o końcach "P" i "Q" dla "P" i "Q" dostatecznie bliskich. Teorię powierzchni Riemanna uogólnia się na wyższe wymiary, co znajduje zastosowanie w fizyce teoretycznej.
Od ogłoszenia przez matematyka niemieckiego Felixa Kleina programu erlangeńskiego zaczęła się rozwijać geometria afiniczna.
Późniejsze kierunki.
Za pewnego rodzaju uogólnienie geometrii można uważać topologię. Coraz większego znaczenia zaczęła nabierać geometria algebraiczna. Geometria nie jest jednolitym działem; składa się z wielu różnorodnych dziedzin, w których specjaliści stosują odmienne metody.
Relatywnie nowym działem geometrii są "geometrie skończone", w których liczba punktów na prostej jest skończona. Najważniejsze przykłady skończonych geometrii afinicznych i rzutowych otrzymuje się korzystając z istnienia ciał skończonych Galois. Inne tego typu geometrie skończone nazywa się egzotycznymi. W ramach klasycznej geometrii wyodrębniła się też geometria zbiorów wypukłych oraz – często uważana za ogólniejszą – geometria kombinatoryczna, zajmująca się na przykład ekonomicznym pokryciem płaszczyzny lub ogólniej "n"-wymiarowej przestrzeni euklidesowej (kartezjańskiej) przez równoległe przesunięcia danego zbioru ograniczonego, wypukłego, domkniętego, o niepustym wnętrzu.
Wpływ poza matematykę.
Fizyka z astronomią.
Od starożytności rozwijane są optyka geometryczna i badania trajektorii ciał, w tym mechanika nieba. W tych dziedzinach odkrywano nieoczekiwane zastosowania dla geometrii, np. występowanie figur opisanych dużo wcześniej na potrzeby czysto matematyczne. Przykładowo:
W XX wieku geometria znowu wpłynęła na podstawy mechaniki i grawitacji:
Geometria przysłużyła się nie tylko fizyce fundamentalnej, ale i materii skondensowanej – parkietaż Penrose’a znalazł zastosowanie do opisu kwazikryształów. Istnieją całe czasopisma naukowe poświęcone związkom geometrii z fizyką.
Inne dyscypliny.
Geometria algebraiczna, zwłaszcza krzywych eliptycznych, w XX wieku została użyta w kryptologii.
Pojęcia geometryczne i sama natura tej nauki to istotne elementy doktryn pitagorejskich i platońskich. Przykładowo Platon próbował powiązać klasyczne żywioły z bryłami platońskimi – istnienie pięciu takich figur miało być racją stojącą za:
Tradycyjnie geometrię zaliczano do siedmiu sztuk wyzwolonych, a konkretniej do czterech bardziej zaawansowanych () – jako jednego z rozwinięć arytmetyki. Sposób wykładu geometrii przez Euklidesa był też inspiracją dla niektórych systemów filozoficznych jak ten Barucha Spinozy.
Niektóre koncepcje geometryczne bywają używane w sztuce, czasem jako jej główny temat. Klasycznym przykładem jest tu złoty podział, opisany złotą liczbą fi (φ) i powiązany ze „złotymi figurami” jak złoty trójkąt, trójkąt Keplera, złoty prostokąt, złota spirala itp. Od starożytności są one używane w architekturze (Partenon), typografii i innych sztukach plastycznych, a nawet muzyce. Motywy geometryczne pojawiają się też w klasyce malarstwa ("Melancholia I" Albrechta Dürera, "Corpus Hypercubus" Salvadora Dalego). Postępy w geometrii i topologii – np. opisanie płaszczyzny hiperbolicznej czy wstęgi Möbiusa – były też inspiracją dla wielu prac Mauritsa Cornelisa Eschera.
Geometria w Polsce.
Od czasów nowożytnych Polscy uczeni mieli pewne osiągnięcia w geometrii; w tej epoce pojawiła się też polskojęzyczna literatura na ten temat:
Geometrią wykreślną zajmował się polski premier Kazimierz Bartel, który wykładał ją na Politechnice Lwowskiej. Geometrię różniczkową badali m.in. Władysław Ślebodziński i Roman Sikorski; Michał Heller ze współpracownikami zastosował geometrię nieprzemienną do teorii względności, zwłaszcza do opisu osobliwości czasoprzestrzennych i do kwantowania grawitacji, w tym do kosmologii kwantowej.

</doc>
<doc id="1734" url="https://pl.wikipedia.org/wiki?curid=1734" title="Gameta">
Gameta

Gameta (komórka płciowa, komórka rozrodcza) – komórka służąca do rozmnażania płciowego.
Gameta u zwierząt powstaje na drodze mejozy (wyjątkowo na drodze mitozy u haploidalnych samców pszczół) w gametogenezie z gametocytów. Może uczestniczyć w procesie zapłodnienia, łącząc się z gametą innej płci i tworząc zygotę.
Gamety mogą:
Gdy gamety są ruchome, a równocześnie jedna z nich (komórka jajowa) jest dużo większa i nieruchoma, a druga (plemnik) – dużo mniejsza i ruchoma, wówczas mówimy o oogamii.
Niekiedy anizogamię i oogamię traktuje się łącznie i nazywa heterogamią. U nagonasiennych i okrytonasiennych mówi się o komórce plemnikowej. Gameta człowieka zawiera: 22 autosomy (chromosomy somalne) i 1 allosom (chromosom płci).

</doc>
<doc id="1736" url="https://pl.wikipedia.org/wiki?curid=1736" title="Grafika komputerowa">
Grafika komputerowa

Grafika komputerowa – dyscyplina zajmująca się cyfrową syntezą i manipulacją treści wizualnych. Ze względu na reprezentację danych dzieli się na grafikę rastrową i wektorową, a ze względu na charakter danych na grafikę dwuwymiarową, trójwymiarową i ruchomą. Obejmuje także przetwarzanie obrazów. Grafikę komputerową można także podzielić na teoretyczną skupiającą się i praktyczną, skupiającą się na manipulacji i tworzeniu obrazów i modelowaniu trójwymiarowym. (np. w programie Blender). Grafika komputerowa stanowi podstawę współczesnych gier, animacji, symulacji oraz wizualizacji. Grafika komputerowa jest działem informatyki i grafiki.
Rys historyczny.
Początki grafiki komputerowej sięgają lat pięćdziesiątych XX wieku, jednak – ze względu na duże koszty komputerów i urządzeń graficznych – aż do lat 80. grafika komputerowa była wąską specjalizacją, a na jej zastosowania praktyczne mogły pozwolić sobie tylko ośrodki badawcze, duże firmy oraz instytucje rządowe. Dopiero, gdy w latach dziewięćdziesiątych rozpowszechniły się komputery osobiste, grafika komputerowa stała się zjawiskiem powszechnym. Powstało też wiele programów dedykowanych grafice komputerowej, np. Gimp, a sama grafika komputerowa otrzymała też status dyscypliny artystycznej.
Klasyfikacja.
Ponieważ celem grafiki komputerowej jest generowanie obrazów, jednym z głównych kryteriów klasyfikacji jest technika ich tworzenia.
Identyczny podział istnieje, jeśli weźmie się pod uwagę reprezentację danych w programach komputerowych.
Przewagą reprezentacji wektorowej nad rastrową jest to, że zawsze istnieje dokładna informacja o tym, z jakich obiektów składa się obraz. W przypadku obrazów bitmapowych tego rodzaju informacja jest tracona, a jedyne, czego można bezpośrednio się dowiedzieć, to kolor piksela. Istnieją jednak metody, które pozwalają wydobyć z obrazów bitmapowych tekst i/lub krzywe.
W chwili obecnej dominują wyświetlacze rastrowe, więc programy wykorzystujące grafikę wektorową są zmuszone przedstawiać idealne figury geometryczne w skończonej rozdzielczości.
Kolejnym kryterium, według którego klasyfikuje się zastosowania grafiki, jest charakter danych:
Jeszcze jednym kryterium jest cykl generacji obrazu.

</doc>
<doc id="1737" url="https://pl.wikipedia.org/wiki?curid=1737" title="Grupa (matematyka)">
Grupa (matematyka)

Grupa – struktura algebraiczna definiowana jako zbiór z określonym na nim łącznym i odwracalnym dwuargumentowym działaniem wewnętrznym; szczególny przypadek monoidu, w którym każdy element ma element odwrotny (zob. "Podobne struktury"). Dział matematyki badający własności grup nazywa się teorią grup.
Motywacja.
W zbiorze liczb całkowitych formula_1 z ich dodawaniem można wyodrębnić następujące własności:
Niech formula_14 oznacza zbiór dodatnich liczb rzeczywistych wraz z działaniem mnożenia, które przejawia własności analogiczne do powyższych:
Rozpatrując zbiór formula_27 gdzie formula_28 jest liczbą naturalną, z działaniem dodawania modulo formula_28 (zob. arytmetyka modularna) okazuje się, że:
Niech formula_45 oznacza niepusty zbiór, zaś formula_46 jest zbiorem wszystkich wzajemnie jednoznacznych przekształceń zbioru formula_45 na siebie. Rozważając składanie przekształceń z formula_48 można zauważyć, że:
Wszystkie powyższe przykłady opisują grupy; w każdym przypadku dany jest niepusty zbiór, na którym określono działanie dwuargumentowe o szczególnych własnościach – tak niżej zostaną zdefiniowane grupy. Dlaczego bada się struktury, które spełniają powyższe/poniższe cztery własności, nie zaś inne; z jakiego powodu wybrano właśnie tę kombinację własności, a nie tylko ich część bądź jakąś dodatkową? Nie ma powodu, by wykluczać te, czy inne możliwości – w istocie rozpatruje się inne teorie i wiele ze wspomnianych kombinacji własności ma swoje nazwy (zob. "Podobne struktury"), jednakże są one dużo mniej ważne niż struktury spełniające wyróżnione cztery własności.
Teoria matematyczna, aby mogła być uznana za ważną, musi być dostatecznie ogólna, a zarazem mieć znaczenie informatywne. Teoria, której postulaty są w wielu przypadkach zbyt ograniczające, okaże się nieistotna w obszarach, w których nie sposób je zapewnić, co ostatecznie przełoży się na ograniczone nią zainteresowanie. Interesujące teorie są ogólne, jednakże ogólność ma cenę: treść. Umożliwiając spełnienie aksjomatów teorii w różnych obszarach i wielu kontekstach, należy zdawać sobie sprawę, że teoria dotyczyć będzie tylko tego, co jest w nich wspólne – może się wtedy okazać, że nie ma takich rzeczy. Istnieje więc niebezpieczeństwo, że teoria będzie się sprowadzać do listy nieciekawych parafraz postulatów pozbawionych głębi. Nakładanie ograniczeń zmniejsza zakres użycia i zainteresowanie teorią, znoszenie ograniczeń prowadzi do pustej teorii. Wyważenie między ogólnością a treścią jest trudnym zagadnieniem, a teoria grup jest jedną z tych, w których udało się osiągnąć równowagę – dzięki temu znajduje ona zastosowanie w matematyce czystej i stosowanej, fizyce teoretycznej oraz innych naukach przyrodniczych (zob. teoria grup). Ponadto pełna jest ona głębokich, interesujących i pięknych wyników. To właśnie wskazuje na to, że wybór czterech własności przedstawionych w definicji można uważać za rozsądny; zastosowania podobnych struktur nie okazały się tak owocne, jak grup.
Definicja.
Zbiór formula_62 z (dobrze) określonym na nim dwuargumentowym działaniem formula_63 nazywa się grupą, jeżeli ma on następujące własności (spełnia poniższe aksjomaty):
Grupa to para uporządkowana formula_80 a więc zbiór formula_81 nazywany "nośnikiem", z funkcją formula_82 daną wzorem formula_83 Dlatego grupy formula_84 oraz formula_85 są równe, o ile formula_86 oraz formula_87 jako funkcje (relacje) na tym zbiorze; na zbiorze formula_62 mogą istnieć dwa różne działania formula_49 oraz formula_90 ze względu na które formula_62 będzie tworzyć grupę, wtedy formula_84 oraz formula_93 są różnymi grupami.
Charakteryzacje.
Wprost z definicji można wywieść kilka trywialnych, choć ważnych obserwacji. Warunek łączności oznacza, że kolejność obliczania (nawiasowanie elementów) nie ma wpływu na ostateczny wynik; dzięki temu zapis formula_94 ma sens i może jednoznacznie wskazywać element formula_95. Postulat istnienia elementu neutralnego oznacza, że nośnik grupy nie może być zbiorem pustym.
W definicji nie zapewnia się nic ponad istnienie (co najmniej jednego) "prawostronnego elementu neutralnego", który służy zagwarantowaniu istnienia (co najmniej jednego) "prawostronnego elementu odwrotnego" do danego. Mimo to wynika z niej, że grupa formula_84 ma jeden i tylko jeden prawostronny element neutralny, który równocześnie jest jednoznacznie wyznaczonym lewostronnym elementem neutralnym; w związku z tym mówi się po prostu o "elemencie neutralnym" grupy. Podobnie dowolny formula_77 ma jednoznacznie wyznaczony prawostronny element odwrotny, który jest jednoznacznie wyznaczonym lewostronnym elementem odwrotnym do formula_98 dlatego nazywa się go "elementem odwrotnym" do formula_75 i wprowadza dla niego oznaczenie formula_100
W świetle tych obserwacji przyjmuje się często definicje:
Ich przyjęcie zwalnia z dowodzenia wyżej przedstawionych własności, jednak podejście to wymaga sprawdzenia dużo większej liczby warunków zawartych w definicji; uzasadnia to też definiowanie grupy jako uporządkowanej czwórki formula_108 której trzeci element oznacza (jednoargumentowe) działanie odwracania, a czwarty – (wyróżniony) element neutralny.
W definicji można zastąpić istnienie prawostronnych elementów neutralnych i odwrotnych na lewostronne, nie zmieniając jej sensu; okazuje się jednak, że zmiana musi dotyczyć obu rodzajów elementów jednocześnie: istnienie prawostronnego elementu neutralnego i lewostronnych elementów odwrotnych nie zawsze zapewnia istnienie struktury grupy w zbiorze (por. "Przykłady"), podobnie dotyczy to lewostronnego elementu neutralnego i prawostronnych elementów odwrotnych.
Przytoczona definicja nie jest jedyną, która wprowadza w zbiorze strukturę grupy. Poza istnieniem łącznego działania dwuargumentowego formula_49 można założyć dla każdego formula_77 istnienie elementu formula_111 spełniającego warunek formula_112 dla dowolnych formula_113; inną możliwością jest wprowadzenie obok działania formula_49 dwóch innych działań dwuargumentowych: formula_115 oraz formula_116, które dla dowolnych formula_113 spełniają formula_118.
Grupę formula_84 spełniającą piąty aksjomat:
nazywa się "grupą przemienną" (lub "abelową"); powyższy warunek dotyczy, ściśle rzecz ujmując, działania dwuargumentowego określonego na formula_81 które nazywa się "przemiennym" – grupa przemienna jest więc grupą z działaniem przemiennym. Warunek przemienności jest na tyle silny, iż umożliwił rozwój teorii grup przemiennych w oderwaniu od ogólnej teorii grup jako dość samodzielnego działu matematyki.
Konwencje zapisu.
Badanie grupy polega na dociekaniu, w jaki sposób formula_66 zależy od elementów formula_75 oraz formula_126 nie zaś od nazwy, czy znaku samego działania. Mając to na uwadze, przyjęło się pomijać znak działania, zastępując go zestawieniem: zamiast formula_66 pisze się formula_128 (czasami formula_129). Samo działanie nazywa się "mnożeniem", rozumianym w związku z tym w szerokim sensie. Może ono oznaczać mnożenie liczb, ale też złożenie odwzorowań, branie różnic symetrycznych zbiorów, czy też jakąkolwiek inną bardziej wymyślną definicję (por. "Przykłady"). Mówi się wtedy, że w grupie używa się zapisu "multiplikatywnego" bądź że jest ona "grupą multiplikatywną". Dlatego też, mówi się też o "iloczynie" formula_128 elementów formula_75 oraz formula_132 Ponadto element neutralny oznacza się często cyfrą formula_133 przy czym nie musi to być liczba 1: może to być odwzorowanie tożsamościowe, zbiór pusty, czy obiekt innego rodzaju. Nie stosuje się jednak zapisu formula_134 zamiast formula_135 dla elementu odwrotnego do formula_136 Opisany sposób zapisu będzie wykorzystywany w dalszej części artykułu (zachowane zostanie oznaczenie formula_73 dla elementu neutralnego).
Obok zapisu multiplikatywnego stosuje się również zapis "addytywny", w szczególności, gdy grupa jest przemienna. Działanie oznacza się w nim znakiem „+” i nazywa "dodawaniem", rozumianym – podobnie jak mnożenie – w szerokim sensie. Element formula_138 nazywa się "sumą" elementów formula_75 oraz formula_132 W "grupie addytywnej" element neutralny oznacza się cyfrą formula_141 przy czym znowu nie musi on oznaczać liczby 0. Ponadto element odwrotny do formula_75 zapisuje się formula_143 i nazywa "elementem przeciwnym" do formula_136
Zwyczajowo grupą nazywa się nie parę grupa–działanie, a sam nośnik – zbiór formula_62 – o ile nie prowadzi to do niejasności: jak wspomniano wcześniej, na zbiorze można często określić wiele grup; w takim przypadku sformułowania „grupa addytywna” i „grupa multiplikatywna” służą wyróżnieniu jednej z nich.
Własności.
Niech formula_62 będzie grupą i formula_147 Wówczas:
W definicji grupy określa się iloczyn dwóch elementów; wcześniej wprowadzony został jednoznaczny iloczyn trzech elementów; podobnie można wprowadzić iloczyn czterech elementów. W celu uproszczenia notacji w podobny sposób wprowadza się ogólny iloczyn formula_28 elementów formula_159 grupy formula_62 formula_161 definiowany poprzez formula_162-krotny iloczyn dwóch elementów; nawiasy można wstawić na wiele sposobów, jednak dzięki łączności wszystkie one dają ten sam wynik: formula_163 równy formula_164 Jeśli formula_159 formula_166 są wszystkie równe formula_167 to pisze się formula_168 w szczególności formula_169 a przy tym formula_170 Tę obserwację można wyrazić więc w postaci formula_171 (dla formula_77 i formula_173); ponadto formula_174 formula_175.
Własności te rozszerza się na wykładniki całkowite; przyjmuje się, że formula_176 (element neutralny) oraz formula_177 (element odwrotny do formula_178) dla formula_77 oraz formula_180 Ze względu na to, dla wszystkich formula_77 oraz formula_182
Dodatkowo dla formula_186 zachodzi formula_187; obserwacja ta dowodzi też formula_188 Jeżeli formula_113 są elementami, dla których formula_190 to formula_191, a stąd formula_192 dla wszystkich formula_193. Jeśli formula_194 dla dowolnego formula_167 to grupa formula_62 jest przemienna.
W przypadku grup addytywnych zamiast formula_197 pisze się formula_198 dla formula_199 i definiuje formula_200 oraz formula_201 dla formula_202 Określa to formula_198 dla formula_77 oraz formula_205 Poprzednie obserwacje zapisuje się wtedy odpowiednio: formula_206 formula_207 oraz formula_208 ponadto formula_209 (formula_210 w ostatniej tożsamości istotne jest założenie przemienności grupy).
Przykłady.
Najprostszą, a zarazem najmniejszą grupą jest grupa trywialna złożona z jednego elementu. Dalsze przykłady obejmują "grupę przekształceń" ustalonego zbioru (ostatni przykład w "Motywacja"); "grupę euklidesową", czyli grupę izometrii ustalonej przestrzeni euklidesowej; "grupę symetrii" danej figury przestrzeni euklidesowej, czyli grupę izometrii własnych tej figury (tzn. izometrii odwzorowujących figurę na siebie); "grupę diedralną", tzn. grupę symetrii wybranego wielokąta foremnego (wszystkie z działaniem składania przekształceń). Ze względu na możliwość reprezentacji elementów grupy jako macierzy, ważnym przykładem są różnorodne "grupy macierzy" (odwracalnych z działaniem ich mnożenia, np. wygodna reprezentacja macierzowa "grupy kwaternionów").
Pojęcia.
Wśród podanych wyżej przykładów grup niektóre z nich mają nośnik będący zbiorem skończonym, inne – zbiorem nieskończonym. Liczbę elementów grupy formula_81 a dokładniej jego moc zbioru formula_81 nazywa się "rzędem" tej grupy i oznacza symbolem formula_362 Jeżeli formula_363 jest skończony, to grupę formula_62 również nazywa się "skończoną", jeśli formula_363 jest nieskończony, to mówi się, że grupa formula_62 jest "nieskończona". Niekiedy rozróżnia się różne rodzaje nieskończoności, ale często przyjmuje się, że jeśli rząd grupy formula_62 jest nieskończony, to pisze się formula_368 gdzie symbol formula_369 reprezentuje wszystkie typy nieskończoności.
Grupa jako zbiór (z określonym na nim działaniem dwuargumentowym spełniającym pewne własności) ma podzbiory; spośród wszystkich podzbiorów bardziej interesujące są te podzbiory, które odzwierciedlają strukturę algebraiczną grupy, gdyż pomagają zrozumieć jej budowę. Wyróżnione miejsce zajmują pośród nich te, które same są grupami (ze względu na to samo działanie): nazywa się je "podgrupami" danej grupy. Wśród innych podzbiorów grupy istotne miejsce zajmują "warstwy" względem określonej podgrupy, które stanowią rozbicie nośnika na rozłączne podzbiory; liczbę warstw względem wybranej podgrupy nazywa się "indeksem" tej podgrupy w grupie (podobnie jak w przypadku rzędu można rozróżniać rodzaje nieskończoności, jednak częstokroć się tego nie czyni). Ponieważ warstwy danej grupy względem jej ustalonej podgrupy są równoliczne, to rząd grupy jest równy iloczynowi rzędu podgrupy oraz indeksu podgrupy w grupie; w szczególności jeśli grupa jest skończona, to rząd podgrupy jest dzielnikiem rzędu grupy – ta ważna obserwacja nazywana jest twierdzeniem Lagrange’a.
Dla grupy formula_62 oraz formula_77 zbiór formula_372 wszystkich potęg całkowitych elementu formula_75 jest niepusty, a ponadto tworzy podgrupę w formula_62 – nazywa się go "podgrupą cykliczną" grupy formula_62 "generowaną przez element" formula_136 Gdy formula_377 to formula_62 nazywa się "grupą cykliczną", a element formula_75 nazywa się "generatorem" tej grupy. Rząd formula_380 tej podgrupy nazywa się "rzędem" elementu formula_75 i oznacza formula_382 (jak wyżej, zwykło się przyjmować, że wartość ta jest liczbą naturalną albo nieskończonością). Jeżeli formula_62 jest skończona, to każdy element formula_77 ma skończony rząd, a dokładnie formula_385 na mocy twierdzenia Lagrange’a; w grupach nieskończonych mogą istnieć tak elementy rzędu skończonego, jak i nieskończonego. Definicję generowania podgrupy przez element rozszerza się na zbiory elementów: jeżeli formula_386 to formula_387 nazywa się "podgrupą generowaną" przez formula_45 i składa się ze wszystkich skończonych iloczynów elementów w formula_45 oraz ich odwrotności (przyjmuje się, że formula_390 jest trywialna; ponadto formula_391 a formula_392 oznacza się formula_393); jeżeli formula_394 oraz formula_395 to formula_45 nazywa się "zbiorem generatorów" grupy formula_81 a o grupie formula_62 mówi się, że jest "generowana" przez formula_399 jeśli grupa formula_62 ma skończony zbiór generatorów, to nazywa się ją "skończenie generowaną".
Zbiór warstw względem podgrupy szczególnego rodzaju, tzw. "podgrupy normalnej", można wyposażyć w naturalnie określone działanie, względem którego będzie on tworzyć grupę nazywaną "grupą ilorazową" (danej grupy przez wspomnianą podgrupę normalną). Oprócz tego, że mogą one służyć do tworzenia kolejnych, mniejszych grup (zachowując przy tym własności grupy wyjściowej, np. przemienność, czy cykliczność) umożliwiają one wniknięcie w budowę grupy za pomocą "homomorfizmów grup", tzn. przekształceń zachowujących strukturę algebraiczną grup; centralną rolę pełni tu twierdzenie o izomorfizmie (wraz z nieco ogólniejszym twierdzeniem o homomorfizmie). Podgrupy mogą być wkomponowane w grupę we względnie prosty bądź w dość złożony sposób, przedstawiając grupę w postaci iloczynów jej podgrup: ogólnego, półprostego, czy prostego (można je opisać za pomocą tzw. "iloczynu kompleksowego"). Ogólnie wszystkie wspomniane pojęcia, przede wszystkim grupy ilorazowe i podgrupy, można wykorzystać do opisu grupy za pomocą jej "prezentacji": dowolna grupa jest ilorazem "grupy wolnej" nad zbiorem generatorów danej grupy przez podgrupę relacji spełnianych w tej grupie.
"Automorfizmy" grupy to przekształcenia, które można uważać za uogólnienie izometrii własnych figur geometrycznych (por. "Przykłady"). Można wyróżnić wśród nich klasę automorfizmów nazywanych "wewnętrznymi", które wyznaczane są przez relację sprzężenia elementów (elementy sprzężone mają te same własności, np. ten sam rząd). Dwie podgrupy są sprzężone (jedna względem drugiej, wzajemnie), gdy jedna jest obrazem drugiej w pewnym automorfizmie wewnętrznym. Interpretując elementy sprzężone jako „takie same” można pokusić się o rozumienie automorfizmów wewnętrznych jako „zachowujących wygląd”, wtedy podgrupy sprzężone można rozumieć jako podgrupy „wyglądające tak samo”. Podgrupy „o unikatowym wyglądzie, jedyne w swoim rodzaju”, to "podgrupy normalne" (albo "samosprzężone"): takie, które wszystkie automorfizmy wewnętrzne przekształcają w siebie. Automorfizmy grupy formula_62 tworzą grupę formula_402 ze względu na składanie przekształceń, a automorfizmy wewnętrzne grupy formula_62 tworzą podgrupę (normalną) formula_404 we wspomnianej grupie automorfizmów (wśród wszystkich „symetrii” danej grupy przekształcenia „zachowujące wygląd” podgrup są „jedyne w swoim rodzaju”).
"Centrum" grupy formula_62 to podgrupa (normalna) formula_406 elementów przemiennych z dowolnym elementem grupy formula_81 jej rozmiar mówi więc o stopniu przemienności grupy; związek między centrum a automorfizmami wewnętrznymi ustala grupa ilorazowa formula_62 przez formula_409 która ma tę samą strukturę, co grupa formula_410 Innym pojęciem służącym określeniu stopnia przemienności, czy też raczej nieprzemienności, grupy jest "komutator" dwóch elementów; podgrupa generowana przez wszystkie komutatory, nazywana "pochodną" grupy (lub jej "komutantem"), jest trywialna, gdy grupa jest przemienna. Podgrupa ta umożliwia wskazanie przemiennych grup ilorazowych: są nimi te grupy ilorazowe, których pochodna zawiera się w podgrupie normalnej będącej dzielnikiem; pozostałe grupy ilorazowe są nieprzemienne. "Podgrupa charakterystyczna" (będąca przypadkiem szczególnym podgrupy normalnej) to podgrupa, która „wygląda symetrycznie” (strukturę pierwszych zachowują wszystkie automorfizmy grupy, podczas gdy drugich jedynie szczególna ich część – tylko wewnętrzne). Przykładami są m.in. wspomniane centrum, czy pochodna grupy.
W sekcji "Przykłady" zasygnalizowano istnienie grup funkcji, np. grupy przekształceń formula_46 danego zbioru formula_412 grupy izometrii przestrzeni euklidesowej, wyżej wspomniano również o grupie formula_402 funkcji zachowujących mnożenie w formula_414 Ogólnie, jeśli formula_45 jest zbiorem z określoną na nim pewną strukturą (algebraiczną, geometryczną, analityczną, topologiczną, czy inną), odwzorowania określone na formula_412 które zachowują tę strukturę, tworzą grupę. "Działanie grupy na zbiorze" pozwala na uchwycenie funkcyjnego charakteru elementów grupy, który mogą one przejawiać; o elementach grupy formula_62 można myśleć właśnie jako o funkcjach określonych na zbiorze formula_418 W gruncie rzeczy dowolne działanie grupy na zbiorze formula_45 można rozumieć jako homomorfizm grupy formula_62 w grupę formula_46 (tzw. "reprezentacja permutacyjna" grupy formula_62). Wykorzystując pojęcie działania grupy na zbiorze, można w czytelny sposób uzasadnić twierdzenie Cayleya: grupa formula_62 ma tę samą strukturę, co pewna podgrupa przekształceń (wzajemnie jednoznacznych) zbioru formula_414 Wiele informacji o grupie można pozyskać, rozważając działanie grupy formula_62 na zbiorze formula_62 poprzez sprzężenia (zob. "klasa sprzężoności").
Proste odwrócenie twierdzenia Lagrange’a jest fałszywe: jeśli formula_427 jest dzielnikiem rzędu formula_363 grupy skończonej formula_81 to formula_62 nie musi mieć podgrupy rzędu formula_431 nałożenie dodatkowego warunku na formula_432 by było potęgą liczby pierwszej (grupy o rzędzie wyrażającym się potęgą liczby pierwszej to tzw. "grupy pierwsze") i było względnie pierwsze z formula_433 sprawia, że teza twierdzenia staje się prawdziwa – jest to pierwsze z trzech twierdzeń Sylowa. Wspomniana podgrupa (pierwsza) rzędu formula_427 nazywana jest "podgrupą Sylowa"; drugie twierdzenie Sylowa mówi, że podgrupy Sylowa są sprzężone; trzecie opisuje liczbę możliwych podgrup Sylowa.
Grupy zawierające podgrupy normalne można rozłożyć na iloraz oraz wspomnianą podgrupę normalną. Nietrywialną grupę nazywa się "prostą", jeżeli nie ma ona nietrywialnych, właściwych podgrup normalnych – definicja ta przywodzi na myśl liczby pierwsze: podobnie jak liczby pierwsze są „budulcem” liczb całkowitych, tak grupy proste są „budulcem” pewnego rodzaju grup; analogii tej nie należy jednak posuwać zbyt daleko, gdyż różne grupy mogą składać się z tych samych elementów składowych – problem konstrukcji grupy znany jako "problem rozszerzenia" nadal oczekuje na rozwiązanie. Proste grupy przemienne to dokładnie grupy cykliczne o rzędzie będącym liczbą pierwszą (zob. klasyfikacja skończonych grup przemiennych); innym przykładem są grupy alternujące (grupa permutacji parzystych z działaniem ich składania) stopnia piątego i wyższych.
Jeżeli formula_435 jest podgrupą w formula_81 to skończony ciąg podgrup w formula_62 (zawierający formula_435 oraz formula_62) nazywa się "ciągiem" ("podnormalnym") od formula_435 do formula_81 gdy każda podgrupa ciągu jest podgrupą normalną kolejnej. Elementy ciągu nazywa się jego "wyrazami", a grupy ilorazowe kolejnych wyrazów – jego "ilorazami" (lub "faktorami"); ciąg od podgrupy trywialnej do formula_62 nazywa się krótko ciągiem formula_414 Jeśli każdy wyraz ciągu jest normalny/charakterystyczny w formula_81 to cały ciąg nazywa się "normalnym"/"charakterystycznym"; gdy ciąg nie zawiera powtórzeń (zawieranie właściwe podgrup), to ciąg nazywa się "właściwym". Ciąg (2) od formula_435 do formula_62 nazywa się "zagęszczeniem" ciągu (1) od formula_435 do formula_81 jeżeli każdy wyraz (1) jest również wyrazem (2); zagęszczenie ciągu (1) można więc uzyskać poprzez wstawienie dodatkowych grup – niekoniecznie różnych od wyrazów ciągu (1) – między kolejne wyrazy ciągu (1). Gdy jednak (2) jest zagęszczeniem (1) i co najmniej jeden wyraz (2) nie był wyrazem (1), to (2) nazywa się "zagęszczeniem właściwym" (1). Ciąg formula_62 nazywa się "ciągiem kompozycyjnym", jeśli jest ciągiem właściwym formula_62 i nie ma zagęszczenia właściwego (ilorazy ciągu kompozycyjnego to "ilorazy kompozycyjne"); ciąg kompozycyjny grupy formula_62 można scharakteryzować jako ciąg formula_81 w którym wszystkie ilorazy są proste. Dwa ciągi grupy formula_62 są "równoważne", gdy mają tę samą liczbę wyrazów i ilorazy pierwszego ciągu mają, w pewnym porządku, tę samą strukturę co ilorazy drugiego ciągu (a więc niekoniecznie odpowiadające sobie wyrazy ciągów). Twierdzenie Jordana-Höldera mówi, że dowolne dwa ciągi kompozycyjne danej grupy są równoważne (o ile tylko grupa ma ciąg kompozycyjny); w istocie prawdziwe jest dużo mocniejsze twierdzenie Schreiera, które zapewnia, że dowolne dwa ciągi grupy mają równoważne zagęszczenia (wniosek: każdy ciąg właściwy grupy ma zagęszczenie będące ciągiem kompozycyjnym). Przytoczone wyniki są elementem szerszej klasyfikacji skończonych grup prostych.
Ciąg od formula_435 do formula_62 nazywa się "abelowym", gdy wszystkie ilorazy są abelowe (przemienne). Grupę nazywa się "rozwiązalną", jeśli ma ciąg abelowy. Każda grupa przemienna jest rozwiązalna, choć istnieją rozwiązalne grupy nieprzemienne; ponadto podgrupy i grupy ilorazowe grup rozwiązalnych również są rozwiązalne, z drugiej strony jeśli rozwiązalna jest podgrupa normalna i iloraz grupy przez nią, to rozwiązalna jest i sama grupa. Przykładami grup nierozwiązalnych są znowu grupy alternujące stopnia piątego i wyższych, rozwiązalne są z kolei skończone grupy pierwsze. Ogólniej: ponieważ rozwiązalne grupy proste to grupy cykliczne rzędu będącego liczbą pierwszą, to skończone grupy rozwiązalne to grupy, w których każdy iloraz kompozycyjny ma rząd wyrażający się liczbą pierwszą. Wynika stąd, że grupy permutacji stopnia piątego i wyższych również są nierozwiązalne. Obserwacja ta pełni kluczową rolę w dowodzie tego, że równanie wielomianowe stopnia większego niż cztery nie może być rozwiązane za pomocą pierwiastników (tzn. czterech działań arytmetycznych i pierwiastkowania, tj. potęg i pierwiastków o wykładniku/stopniu naturalnym) – jest to tzw. twierdzenie Abela-Ruffiniego.
Zbiór elementów skończonego rzędu grupy przemiennej formula_62 tworzy podgrupę nazywaną "podgrupą torsyjną" formula_457 iloraz formula_62 przez formula_459 poza elementem neutralnym zawiera wyłącznie elementy nieskończonego rzędu. Ogólnie dowolną grupę formula_62 nazywa się "torsyjną", o ile tylko zawiera wyłącznie elementy skończonego rzędu; grupę, w której każdy element poza neutralnym ma rząd nieskończony nazywa się "beztorsyjną" (w ten sposób jedyną grupą jednocześnie torsyjną i beztorsyjną jest grupa trywialna; każda grupa skończona jest torsyjna, choć torsyjna jest również nieskończona grupa ilorazowa formula_461 przez formula_462 grupy, które nie są ani torsyjne, ani beztorsyjne nazywa się "mieszanymi"). Twierdzenie klasyfikacyjne są w matematyce bardzo pożądane, lecz niezmiernie rzadkie: nie mniej istnieje wyczerpująca klasyfikacja skończenie generowanych grup przemiennych (twierdzenie Frobeniusa–Stickelbergera). Wystarczy więc zbadać dwie klasy grup przemiennych: torsyjne i beztorsyjne, a następnie znaleźć sposób na skonstruowanie z nich grupy przemiennej. Nie obędzie się jednak bez dodatkowych warunków nałożonych na formula_463 jeśli przyjąć, że formula_62 jest skończenie generowana, to formula_459 jest skończona. Wtedy badanie skończonych grup przemiennych sprowadza się do badania skończonych, przemiennych grup pierwszych oraz beztorsyjnych grup przemiennych – wykorzystuje się do tego pojęcia "niezależności", "bazy" (niezależnego zbioru generującego grupę, o ile nie zawiera on elementu neutralnego) oraz "rangi grupy" (jednoznacznie wyznaczonej liczby elementów w bazie). Złączenie części torsyjnej i beztorsyjnej przebiega w najprostszy możliwy sposób: poprzez iloczyn prosty – struktura skończenie generowanej grupy przemiennej formula_62 wyznaczona jest w zupełności przez zbiór liczb całkowitych w jednoznaczny sposób.
Podobne struktury.
Niech formula_62 będzie dowolnym zbiorem z określonym na nim działaniem dwuargumentowym formula_468 Istnieje szereg podobnych struktur mających osobne nazwy, które spełniają aksjomaty podobne do aksjomatów grupy; struktura formula_93 jest:

</doc>
<doc id="1738" url="https://pl.wikipedia.org/wiki?curid=1738" title="GNU/Linux">
GNU/Linux

GNU/Linux – tym mianem określa się często uniksopodobny system GNU z jądrem Linux. Korzysta on z jądra Linux, gdyż jądro GNU Hurd tworzone przez GNU jest wciąż w fazie rozwojowej. Zazwyczaj jednak, w mowie fachowej i potocznej, wspomniany system jest określany tylko jednym wyrazem – Linux.
Kontrowersje związane z nazwą systemu operacyjnego opartego na jądrze Linux i m.in. oprogramowaniu GNU powstały, gdy Richard Stallman, założyciel Free Software Foundation i projektu GNU, zaczął promować nazwę „GNU/Linux”. Podkreśla ona duży wkład projektu GNU dla większości istniejących systemów z Linuksem (np. kompilatory, biblioteki, parsery).
Z kolei przeciwnicy tego podejścia przypominają, że w przeciętnej dystrybucji jest też wiele oprogramowania powstałego w innych, niezależnych od FSF, projektach (np. środowisko graficzne KDE). Proponują oni aby kompletny system operacyjny nazywać po prostu „Linux”, choć samego Linuksa w takiej dystrybucji jest jeszcze mniej niż pakietów GNU.
W odpowiedzi na powyższe Stallman wyjaśnia, że FSF zasługuje na wyróżnienie za powołanie do życia systemu operacyjnego GNU jako całości, nie zaś za każdy program z osobna będący jego częścią. Przypomina, że system operacyjny GNU powstał w roku 1984, na lata przed początkami Linuksa.
Dystrybucje GNU/Linuksa.
Nie istnieje jedna oficjalna wersja GNU/Linuksa, wydana np. dzięki porozumieniu członków projektu GNU oraz programistów Linuksa.
Istnieje wielu niezależnych producentów, którzy do wydanego jądra Linux dodają oprogramowanie GNU oraz inne programy (np. serwer Apache, przeglądarka Firefox). Takie połączenie bibliotek i programów nazwane jest dystrybucją GNU/Linuksa.
Poszczególne dystrybucje są w bardzo różnym stopniu zgodne z założeniami projektu GNU oraz FSF. Oficjalnie aprobowanych przez FSF jest 9 dystrybucji: Dragora, , gNewSense, Guix System Distribution, Musix, Parabola GNU/Linux, PureOS, Trisquel oraz Ututo.
Jądro Linux bez GNU.
Możliwe jest wydanie systemu operacyjnego używającego jądra Linux bez użycia komponentów systemu GNU. Bibliotekę C glibc można zastąpić inną – np. uClibc, a zamiast standardowych programów GNU (Coreutils) zastosować program BusyBox. Powstają w ten sposób zazwyczaj małe wbudowane systemy przeznaczone na firewalle i do innych zastosowań. Wszyscy – włącznie z FSF – zgadzają się, że nazwa „GNU/Linux” nie ma dla nich zastosowania. Prawie wszystkie systemy biurkowe i serwerowe używają jednak składników GNU, zwłaszcza glibc, coreutils i GCC.

</doc>
<doc id="1740" url="https://pl.wikipedia.org/wiki?curid=1740" title="Goszcz">
Goszcz

Goszcz (niem. "Goschütz") (1638-1742 miasto) – wieś w Polsce, położona w województwie dolnośląskim, w powiecie oleśnickim, w gminie Twardogóra, na wysokości 150 m n.p.m., około 50 km na północny wschód od Wrocławia i około 40 km na południowy zachód od Ostrowa Wielkopolskiego.
W latach 1954–1972 wieś należała i była siedzibą władz gromady Goszcz. W latach 1975–1998 wieś administracyjnie należała do województwa wrocławskiego.
Nazwa.
Niemiecki językoznawca Heinrich Adamy wywodził nazwę miejscowości od polskiej nazwy "„gościniec”" lub "„gospoda”" . W swoim dziele o nazwach miejscowych na Śląsku wydanym w 1888 roku we Wrocławiu jako najstarszą nazwę miejscowości wymienia "Gosz" podając jej znaczenie "„Wirtshaus”" czyli w języku polskim "„Gospoda, gościniec, oberża, karczma”". Nazwa wsi wiązała się prawdopodobnie ze znanym zajazdem lub karczmą przyjmującą gości i została później fonetycznie zgermanizowana na "Goschütz" tracąc swoje pierwotne znaczenie.
W księdze łacińskiej "Liber fundationis episcopatus Vratislaviensis" (pol. "Księga uposażeń biskupstwa wrocławskiego") spisanej za czasów biskupa Henryka z Wierzbna w latach 1295–1305 miejscowość wymieniona jest w zlatynizowanej formie "Gosche".
W alfabetycznym spisie miejscowości na terenie Śląska wydanym w 1830 roku we Wrocławiu przez Johanna Knie wieś występuje pod polską nazwą "Goszcza" oraz niemiecką "Goschütz". Polską nazwę miejscowości w obecnie obowiązującej formie "Goszcz" w książce „Krótki rys jeografii Szląska dla nauki początkowej” wydanej w Głogówku w 1847 wymienił śląski pisarz Józef Lompa.
Historia.
Najstarsze ślady człowieka na terenie dzisiejszego Goszcza pochodzą z neolitu. Dowodem na to są trzy toporki pochodzące z tego okresu znalezione w Goszczu i niedaleko Goszcza.
Pierwsze zapisy o Goszczu jako części dóbr biskupstwa wrocławskiego - pochodzą z bulli papieża Hadriana IV z roku 1156. Dobra goszczańskie były własnością Kościoła do XVI w., następnie stały się dobrami książęcymi. W roku 1686 Goszcz (zwany w tym czasie Gościec) otrzymał nawet prawa miejskie i herb. Był państwem stanowym.
W roku 1945 miejscowość włączono do Polski. Jej dotychczasową ludność wysiedlono do Niemiec.
Jan Jakub Kolski w latach 90. XX wieku kręcił w Goszczu swój film "Jańcio Wodnik".
Zabytki.
Do wojewódzkiego rejestru zabytków wpisane są:
Demografia.
Według Narodowego Spisu Powszechnego (III 2011 r.) liczył 1202 mieszkańców. Jest największą miejscowością gminy Twardogóra.

</doc>
<doc id="1741" url="https://pl.wikipedia.org/wiki?curid=1741" title="Góry Świętokrzyskie">
Góry Świętokrzyskie

Góry Świętokrzyskie (342.34) – niski łańcuch górski w południowo-wschodniej Polsce, w centralnej części Wyżyny Kieleckiej. Najwyższym szczytem jest Łysica (614 m n.p.m.) w paśmie Łysogór. Nazwa gór pochodzi od relikwii Krzyża Świętego przechowywanych w klasztorze na Łysej Górze.
Góry Świętokrzyskie, obok Sudetów, są jednym z najstarszych łańcuchów górskich w Europie. Kilkukrotnie ulegały wypiętrzaniu, niszczeniu i zalewaniu przez morza. Zostały wypiętrzone 500 mln lat temu w kambrze, później w czasie kaledońskich ruchów górotwórczych na granicy syluru i dewonu. Następnie odmłodziły je orogeneza hercyńska (dolny karbon) i ponownie orogeneza alpejska.
Charakterystyczne dla krajobrazu najwyższych partii Gór Świętokrzyskich są strome stoki, głęboko wcięte doliny, skałki ostańcowe i gołoborza. Góry Świętokrzyskie porośnięte są lasami jodłowymi (Puszcza Jodłowa) i bukowymi. Na ich terenie utworzono Świętokrzyski Park Narodowy.
Gospodarczym i turystycznym centrum regionu są Kielce. Do ważniejszych ośrodków turystycznych należą Święta Katarzyna i Nowa Słupia.
Nazewnictwo.
„"I mnie miłe Gór Świętokrzyskich knieje"
"Na których wiecznie list się zielenieje,"
"A z między inszych wyższa pięknym brakiem"
"Zbawiennym Pańskim uczczona jest znakiem.”"
Po raz pierwszy określenie Góry Świętokrzyskie pojawia się w wierszu z 1674 roku wieszcza Wespazjana Kochowskiego (nazwa szczytu Święty Krzyż – "Mons S. Crucis", "Mons Calvus" lub "Mons Lysecius" używana była już wcześniej – od średniowiecza). Przez następne 200 lat nazwa Góry Świętokrzyskie nie występowała praktycznie w literaturze naukowej i na mapach. Stosowano częściej pojęcie Łysogóry.
Stanisław Staszic stosował określenie Łysogóry dla pasma, które według niego rozciąga się od Pilicy, aż po ujście Nidy. "„Na długość od Gór Tarnawskich przechodzi w Polesie, i tam zupełnie zniżone ginie”"
Jerzy Bogumił Pusch w 1833 roku pisał: „"W części kraju, położonej między Wisłą a Pilicą, wznosi się, jak wyspa pośród płaskiej lub pagórkowatej przestrzeni, mały łańcuch gór, nie noszący u swoich żadnego miana, a który ja, ze względu na jego położenie, nazywam Sandomierskim lub Środkowopolskim łańcuchem górskim [Sandomirer oder polnische Mittelgebirge]”"
Przez cały XIX i początek XX wieku nazywano Góry Świętokrzyskie również: Góry Sandomierskie, Góry Kielecko-Sandomierskie, Góry Sandomiersko-Kieleckie, Góry Kieleckie, Góry Środkowopolskie.
W 1922 roku Zjazd Geografów w Krakowie uchwalił terminologię regionalną ziem polskich. Ustalono, że tereny pomiędzy Wisłą, a Pilicą to Średniogórze Polskie. Geolodzy Jan Czarnocki (1889-1951) i Jan Samsonowicz (1888-1959), konsekwentnie używali jednak określenia aktualnego – Góry Świętokrzyskie i to dzięki ich zabiegom, zostało ono upowszechnione i uznane za właściwe.
Geologia.
Góry Świętokrzyskie, z geologicznego punktu widzenia stanowią jeden z najważniejszych obszarów w Europie, w którym na powierzchni Ziemi występują skały osadowe reprezentujące wszystkie okresy geologiczne od prekambru do czwartorzędu. Obszar świętokrzyski jest jedną z najlepiej odsłoniętych jednostek geologicznych położoną w strefie potężnego rozłamu tektonicznego – tzw. szwu transeuropejskiego.
Prekambr i kambr.
Trzon Gór Świętokrzyskich, w skład którego wchodzą Łysogóry, zbudowany jest z kwarcytowych piaskowców prekambryjskich, podobnie jak dno Doliny Chęcińskiej oraz południowe pasmo wzniesień ciągnące się od Pasma Zgórskiego po Wygiełzowskie. Zbocza pokryte są gołoborzami powstałymi w czwartorzędzie (plejstocenie) w wyniku wietrzenia fizycznego.
Z początku kambru pochodzą najstarsze skały osadowe znalezione na powierzchni w Górach Świętokrzyskich, tzw. „łupki kotuszowskie”. Kambryjskie piaskowce kwarcytowe należą do najtwardszych i najbardziej zwięzłych skał, które biorą udział w budowie najwyższych pasm w Górach Świętokrzyskich: łysogórskiego i jeleniowskiego, tworząc m.in. gołoborza i skalne granie na Łysicy i Łysej Górze.
Ślady z okresu prekambru i kambru widoczne są w miejscowościach takich jak: Kotuszów, Ociesęki, Kamieniec k/Klimontowa, Jugoszów-Usarzów (ok. 430 okazów skamieniałości trylobitów), Biesak-Białogon, Widełki – Łapigrosz (skamieniały pancerz trylobita), Góra Słowiec, Brzechów, Wiśniówka Duża (mułowce w kamieniołomie), Ameliówka (naturalne odsłonięcia w dolinie Lubrzanki i jej dopływach), Łysa Góra.
Ordowik i sylur.
Z ordowiku oraz syluru pochodzą osady (m.in. łupki szarogłazowe), z których zbudowane jest m.in. dno Doliny Wilkowskiej. Profil skalny ordowiku w Górach Świętokrzyskich zawiera zapis pogłębienia, a następnie spłycenia morza w wyniku globalnego zlodowacenia, które miało miejsce pod koniec tego okresu geologicznego. W profilu skalnym ordowiku w Górach Świętokrzyskich zapisała się aktywność wulkaniczna, w postaci tzw. skał piroklastycznych. Globalne ochłodzenie i masowe wymieranie u schyłku ordowiku zapisały się również w świętokrzyskich profilach (m.in. w Mójczy – Odciski ramienionogów, wapienie ordowiku górnego).
Sylur w Górach Świętokrzyskich składa się niemal wyłącznie ze skał pochodzenia morskiego mało odpornych na procesy wietrzenia i erozji, co sprawia, że nie jest zbyt dobrze odsłonięty na powierzchni terenu. Ciemno zabarwione iłowce syluru występują w Górach Świętokrzyskich pod grubym nadkładem młodszych skał osadowych – miejscami zawierają one złoża gazu łupkowego. Pod koniec syluru skały z południowej części Gór Świętokrzyskich zostały wydźwignięte w pasmo górskie.
Ślady z okresu ordowiku i syluru widoczne są m.in. w miejscowościach takich jak: Biesak-Białogon (profil piaskowców, mułowców i bentonitów w ścianie nieczynnego kamieniołomu), Mójcza (nieczynny łomik), Bukówka, Bardo-Wąwóz Prągowiec (wychodnia iłowców, mułowców i szarogłazów syluru dolnego w dolinie potoku Kierdonka), Rzepin (tzw. warstwy rzepińskie)
Dewon.
Zapis dewonu świętokrzyskiego rozpoczyna się osadami lądowymi i płytkomorskimi (tzw. old red), w którym widoczne są odciski skamieniałości bezszczękowców, ryb oraz ramienionogów. W dolomitach dewonu środkowego z Zachełmia k/Kielc znaleziono najstarszy na świecie zapis wyjścia czworonogów z morza na ląd. Grube pokłady wapieni z liczną fauną gąbek, koralowców i innych bezkręgowców morskich, budujące m.in. Pasmo Kadzielniańskie, Zelejowskie i Chęcińskie, to pamiątki tropikalnego morza sprzed 375 milionów lat, w którym tworzyły się budowle zbliżone do współczesnych raf tropikalnych.
Ślady z okresu dewonu widoczne są m.in. w: Rezerwat Wietrznia, Bukowa Góra (ściana kamieniołomu), Bolechowice (wapień ze skamieniałymi szczątkami gąbek i koralowców wykorzystywany jako „marmur” techniczny typu „bolechowice”), Ostrówka (skamieniałe głowonogi zachowane w wapieniu bitumicznym).
Karbon.
W okresie karbonu rozpoczyna się nasilenie ruchów górotwórczych, morze ustępuje i powstają pasma górskie Gór Świętokrzyskich. Zapisami tego wydarzenia w profilach geologicznych Gór Świętokrzyskich są luki stratygraficzne oraz deformacje skał w postaci uskoków i fałdów (m.in. fałd ślichowicki). W czasie ruchów górotwórczych tworzyły się mniejsze i większe uskoki i dyslokacje, a także związane z nimi złoża minerałów (m.in. uskok łysogórski i związane z nim złoże pirytu w Rudkach k/Nowej Słupi) czy uskoki przecinające wapienie dewońskie i związane z nimi żyły kalcytu „różanki”. W tym samym czasie, gdy na obszarze dzisiejszych Gór Świętokrzyskich było wypiętrzanie i tworzył się ląd, na Górnym Śląsku w zapadliskach śródgórskich rozciągały się rozległe, zabagnione puszcze, których dzisiejszą pamiątką są pokłady węgla kamiennego.
Przykłady: Bieliny (rezerwat Kamień-Ławki – wychodnie skalne), Ostrówka (skamieniałe szczątki łodyg liliowców, skamieniała muszla ramienionoga), Kowala (wapienie i margle dewonu górnego i karbonu dolnego), Besówka (ciemnoszare oraz czarne wapienie przepełnione fauną: ramienionogi, ślimaki, małże, małżoraczki, trylobity, ryby pancerne ze świetnie zachowaną tkanką kostną, a przede wszystkim głowonogi)
Perm.
We wczesnym permie obszar ten pokrywał ląd, którego powierzchnię kształtowały procesy geologiczne (erozja, wietrzenie, procesy krasowe). Po wspomnianym lądzie nie zachowały się żadne pozostałości, poza osadami pochodzącymi z jego niszczenia, które gromadziły się u podnóży zboczy – geologicznymi zapisami tych osadów są brekcje i zlepieńce permskie (m.in. słynny zlepieniec zygmuntowski). Wapienie i margle odsłonięte w rejonie Gałęzic, Niewachlowa i Kajetanowa są geologiczną pamiątką morza, które w późnym permie wkroczyło na ląd rozciągający się na obszarze dzisiejszych Gór Świętokrzyskich. Pod koniec permu, kiedy klimat staje się bardziej gorący i suchy, powstały wielkie złoża gipsów, anhydrytów i soli (skały te rozpoznano również wierceniami na obrzeżu Gór Świętokrzyskich). Geologiczną pamiątką są piaskowce i mułowce z tropami kręgowców, określane jako tzw. facja pstrego piaskowca.
Ślady widoczne są m.in. w: Czerwona Góra (profil zlepieńców permu górnego i brekcje w nieczynnym kamieniołomie „Zygmuntówka”), Kowala (kamieniołom), Jaworznia, Gałęzice.
Trias.
Na lądzie i w strefie przybrzeżnej w warunkach gorącego i suchego klimatu gromadziły się czerwone osady, które dały początek czerwonym piaskowcom i mułowcom znanym z obrzeżenia Gór Świętokrzyskich. Słynne „piaskowce tumlińskie” to pamiątka wydm i okresowych rzek. Z piaskowców Gór Świętokrzyskich znane są najstarsze na świecie tropy pierwszych dinozaurów. Morze, które wkroczyło w środkowym triasie na obszar dzisiejszych Gór Świętokrzyskich zostawiło po sobie geologiczną pamiątkę w postaci wapieni przepełnionych skamieniałymi muszlami (m.in. ramienionogów, małży i ślimaków) – stąd wzięło się określenie „wapień muszlowy”. Z końcem triasu na obszarze ponownie zagościł ląd, którego geologicznymi pamiątkami są iły, mułowce i piaski odsłonięte w nielicznych profilach w północno-zachodnim obrzeżeniu Gór Świętokrzyskich.
Ślady widoczne w: Gostków k. Bliżyna (skamieniałe muszle i małże), Szkucin (cegielnia), Gostków, Pałęgi, Sosnowica
Jura i kreda.
Obszary lądowe z rzekami i bagniskami porośnięte przez roślinność tropikalną to typowy krajobraz Gór Świętokrzyskich we wczesnej jurze, około 200 mln lat temu (zapisami tych środowisk są piaskowce i mułowce ze szczątki flory i tropami dinozaurów oraz ciemne iły). W czasie środkowej jury następuje wkroczenie (transgresja) morza na obszar lądowy tzw. „potop” jurajski. W późnej jurze obszar świętokrzyski w dalszym ciągu stanowi fragment zbiornika morskiego, który stale ulega pogłębianiu (zapis w postaci śladów amonitów i belemnitów), następnie spłyceniu (zapis w postaci wapieni koralowcowych i oolitowych) i ponownemu pogłębieniu (muszlowce ostrygowe i osady ilasto-marlgiste). Przykłady: Rezerwat Gagaty Sołtykowskie (tropy dinozaura drapieżnego, profil piaskowców i iłów), Gnieździska, Małogoszcz (muszle ostryg).
Wapienie, margle i opoki kredy górnej odsłaniające się na obrzeżeniu Gór Świętokrzyskich zawierają bogatą faunę bezkręgowców morskich w postaci skamieniałych szczątków małży, amonitów czy jeżowców, oraz unikatowe szczątki ryb i dużych gadów morskich (rekinów, płaszczek, ichtiozaurów, pliozaurów). Na przełomie kredy i paleogenu obszar dzisiejszych Gór Świętokrzyskich ulega wydźwignięciu, a powstałe pasma górskie podlegają wietrzeniu i erozji.
Paleogen.
W czasie paleogenu na obszarze świętokrzyskim dominowały procesy erozji i denudacji, które doprowadziły do usunięcia grubej pokrywy osadów mezozoicznych (trias, jura, kreda) i odsłonięcia tzw. trzonu paleozoicznego Gór Świętokrzyskich, zbudowanego ze skał od kambru do permu. W warunkach klimatu gorącego skały budujące Góry Świętokrzyskie podlegają intensywnemu wietrzeniu chemicznemu (w tym krasowieniu), czego pamiątką są leje krasowe wypełnione osadami ilasto-piaszczystymi.
Na obszarze Gór Świętokrzyskich brak jest wzorcowych profili dokumentujących paleogen. Z tym okresem niewątpliwie związane są: jasne piaski i iły piaszczyste wypełniające niektóre leje krasowe np. na Kadzielni czy Czarnowie.
Neogen.
Ocean Tetyda przekształcił się w Morze Śródziemne. Na obszarze Polski w miejscu występowania bagien i lasów wytworzyły się złoża węgla brunatnego, a w miejscach płytkich, parujących mórz duże złoża gipsów, siarki i soli; wypiętrzaniu ulegają Alpy i Karpaty. Morze, które na początku neogenu (w miocenie) wkroczyło na obszar dzisiejszych Gór Świętokrzyskich od południa zostawia liczne skalne pamiątki w postaci wapieni detrytycznych, iłów, gipsów oraz soli np. w Borkowie. Jedną z geologicznych pamiątek mioceńskiego morza są tzw. „iły korytnickie” przepełnione skamieniałymi muszlami bezkręgowców morskich. W wapieniach z Pińczowa znaleziono unikatowe szczątki wieloryba, który od miejsca znalezienia zyskał nazwę gatunkową "Pinocetus polonius".
Czwartorzęd.
Zlodowacenia południowopolskie, które wkraczają na obszar Gór Świętokrzyskich powodują w wielu miejscach zmiany w rzeźbie terenu; po ich ustąpieniu kształtuje się współczesny rys rzeźby Gór Świętokrzyskich. Na przedpolu lądolodu w tzw. strefie peryglacjalnej tworzą się charakterystyczne formy – gołoborza i osady – lessy. W jaskiniach gromadzą się namuliska, jak również tworzą się nacieki.
Ślady widoczne w: Kadzielnia (leje krasowe i jaskinie), Jaskinia Raj, Gołoborze im. R. Kobendzy (rozległe rumowisko skalne złożone z bloków kambryjskich piaskowców kwarcytowych, które tworzyło się na przedpolu lądolodu), Wąwóz Królowej Jadwigi (odsłonięcie lessów związanych z okresami zlodowaceń w środkowym i późnym plejstocenie).
Klimat.
Klimat Gór Świętokrzyskich znacznie różni się od otaczających regionów. Średnia temperatura roczna jest o około niższa od temperatury w Warszawie i wynosi . Średnia suma opadów waha się od 650 do 900 mm rocznie. Najwyższe szczyty pokryte są śniegiem od listopada do kwietnia. Pokrywa śnieżna utrzymuje się średnio 50–90 dni w ciągu roku. Okres wegetacyjny najwyższych partii gór jest o dwa tygodnie krótszy niż w Warszawie i wynosi około 200 dni. Podobnie jak w wyższych górach można zaobserwować zjawisko inwersji temperatur – temperatura na nagrzanych stokach górskich może być nawet o wyższa niż na dnie dolin, położonych kilkadziesiąt metrów niżej.
Wody.
Teren Gór Świętokrzyskich jest działem wodnym dopływów Wisły. Największy obszar zajmują zlewnie: Kamiennej na północy i Nidy na południu. Część obszaru Gór odwadniana jest ponadto przez Opatówkę, Koprzywiankę, Czarną Staszowską oraz Czarną Konecką (dopływ Pilicy). Sieć wodna jest niezależna od przebiegu pasm oraz budowy geologicznej. Tylko Pasmo Jeleniowskie stanowi dział wód między dorzeczami Kamiennej i Nidy. Główny węzeł hydrograficzny ulokowany jest na północ od Pasma Klonowskiego.
Ochrona przyrody.
Celem ochrony unikatowych walorów Gór Świętokrzyskich utworzono na ich terenie park narodowy, 5 parków krajobrazowych, liczne rezerwaty, obszary chronionego krajobrazu i obszary Natura 2000.
Pasma górskie.
W skład Gór Świętokrzyskich wchodzi kilkanaście równoległych pasm, ciągnących się z zachodu na wschód. Główną oś stanowi ciąg pasm o długości około 70 km, rozpoczynający się w okolicach Dobrzeszowa na zachodzie. W jego skład wchodzą od zachodu: Pasmo Dobrzeszowskie, Pasmo Oblęgorskie, Wzgórza Tumlińskie, Pasmo Masłowskie, Łysogóry oraz Pasmo Jeleniowskie, które kończy się w okolicach Opatowa. Ciąg ten pocięty jest przełomami rzek: Łosośnej, Bobrzy, Lubrzanki i Słupianki. Na południe od Łysogór znajduje się niewielkie Pasmo Bielińskie.
Na południe od pasma głównego, oddzielony od niego Padołem Kielecko-Łagowskim, położony jest ciąg, w skład którego wchodzą pasma: Zgórskie, Posłowickie, Dymińskie, Daleszyckie, Cisowskie, Brzechowskie, Orłowińskie i Wygiełzowskie. Na północ od Pasma Wygiełzowskiego znajduje się Pasmo Iwaniskie. Na południe od Pasma Orłowińskiego położone jest Pasmo Ociesęckie.
Na południowy zachód od Pasma Dymińskiego rozciągają się pasma Zelejowskie i Chęcińskie, rozdzielone Doliną Chęcińską. Na zachód od Chęcin położone są dwa niewielkie pasma: Grzywy Korzeczkowskie i Grząby Bolmińskie. Na północny wschód od Chęcin znajduje się natomiast Pasmo Bolechowickie.
Na północ od pasma głównego rozciągają się Pasmo Klonowskie, Pasmo Bostowskie oraz Pasmo Pokrzywiańskie, a dalej na północ położone jest Pasmo Sieradowickie.
Na terenie Kielc oraz w ich okolicach znajdują się: Pasmo Kadzielniańskie oraz Wzgórza Szydłówkowskie.
Turystyka.
Historia turystyki.
Święty Krzyż od dawna przyciągał rzesze ludzi. Sława przechowywanych tu relikwii Drzewa Krzyża Świętego spowodowała, że od XIV w. na górze powstał ośrodek pątniczy o zasięgu ogólnokrajowym. Wielkie znaczenie dla propagowania i wielowiekowego utrzymywania się pielgrzymowania na Święty Krzyż miała pobożna postawa polskich władców (Władysław Jagiełło), oddziałująca przez wieki na ich rodzinę, otoczenie, duchowieństwo oraz zyskująca naśladowców w innych warstwach społecznych. Sława cudownej mocy Drzewa Krzyża Świętego i masowe pielgrzymki spowodowały, że peregrynacje religijne trwały nawet po kasacie opactwa Benedyktynów w 1819 r. W rozbudzaniu zainteresowania Górami Świętokrzyskimi przed 1795 r. niebagatelną rolę odgrywała także twórczość literacka. Już Jan Długosz w XV w., przypatrując się tutejszym krajobrazom, pozostawał pod urokiem Świętego Krzyża. W następnym stuleciu urodę okolic we fraszce "„Do gór i lasów”" opiewał Jan Kochanowski. Prawdziwie poetycką wizję Świętego Krzyża stworzył XVII-wieczny poeta – regionalista Wespazjan Kochowski.
Rozwijając swoje zainteresowania krajoznawcze wielu podróżników zwracało uwagę na różnorodne aspekty pozareligijne Łysogór. Podkreślano między innymi wątki przyrodnicze, literackie, artystyczne, historyczne, architektoniczne, etnograficzne czy społeczne. W Górach Świętokrzyskich przebywali wówczas i rozpowszechniali o nich wiadomości m.in. Adam Tadeusz Naruszewicz, Stanisław Staszic, Julian Ursyn Niemcewicz, Zorian Dołęga-Chodakowski, Kazimierz Władysław Wójcicki, Oskar Kolberg, Klementyna z Tańskich Hoffmanowa, Cyprian Kamil Norwid, Deotyma, Żegota Pauli, Wojciech Jastrzębowski, Wojciech Gerson, Napoleon Orda, Stefan Żeromski.
Od połowy XIX w. publikowano również pierwsze prace naukowe dotyczące świętokrzyskich ssaków, ptaków, ryb i mięczaków. Stopniowo zbierano i porządkowano również informacje o tutejszej florze.
w 1900 roku został wydany pierwszy przewodnik po Górach Świętokrzyskich pt. "„Wycieczki po kraju”" autorstwa Aleksandra Janowskiego.
Nagromadzenie różnorodnych walorów w jednym, osobliwym miejscu, spowodowało, że Góry Świętokrzyskie stały się idealnym celem podróży krajoznawczych dla zorganizowanego ruchu turystycznego, który na ziemiach Królestwa Polskiego powstawał od końca XIX w. Na przykładzie Łysogór prezentowano wszystko to, czym w dobie niewoli narodowej żyli Polacy: wspomnienia o wspaniałej przeszłości kraju, jednoczącą rolę wspólnej religii, problemy i tajemnice oczekujące na wyjaśnienie, zachwyt nad ojczystą przyrodą i krajobrazem. Wyraził to najlepiej Aleksander Janowski, jeden ze współtwórców utworzonego w 1906 r. Polskiego Towarzystwa Krajoznawczego:
"„Odcięci kordonem granicznym od Karpat i Tatr z Łysicy braliśmy miarę Gór. Przez całe życie pamiętam, syn mazowieckich równin, to wstrząsające wrażenie, jakie zrobił na mnie szczyt Łysicy ukryty w chmurach i ta droga przez pasmo ku Nowej Słupi (…) i te jodły obdziergane młodziutką wiosenną zielenią. I widziałem potem Alpy, Pireneje, Bałkan, Apeniny, ale to już nie było to! I widziałem Atlas, Kordyliery, Fudżi-yamę, ale to już nie było to”"
Dla popularyzacji terenu wielkie znaczenie miało w 1910 r. otwarcie w Świętej Katarzynie pierwszego schroniska turystycznego w Królestwie Polskim. Po raz pierwszy sprawę utworzenia w Górach Świętokrzyskich rezerwatu, omawiano w 1909 r. na XVI Zjeździe Przyrodników i Lekarzy w Moskwie. Na podkreślenie zasługuje fakt, iż w tym samym czasie rosyjski badacz S. Ganieszyn w pracy zawierającej opis geograficzny i botaniczny centralnej części pasma „kielecko-sandomierskiego” proponował objęcie grzbietu Łysogór oraz Bukowej Góry zakazem użytkowania i przeznaczenie ich dla celów naukowych.
Po odzyskaniu przez Polskę niepodległości, w 1918 r. PTK powołało Komisję Ochrony Zabytków Przyrody, która zajęła się m.in. ochroną Puszczy Jodłowej. Efektem działań towarzystwa było utworzenie w 1921 r. na Chełmowej Górze pierwszego rezerwatu. Uzyskał on status ścisłego rezerwatu przyrody, a głównym celem była ochrona modrzewia polskiego. Niedługo potem powołano kolejne rezerwaty: na Miejskiej Górze nad Bodzentynem oraz na Łysej Górze i Łysicy, w celu ochrony Puszczy Jodłowej- unikalnego w skali światowej drzewostanu jodłowego i jodłowo-bukowego oraz charakterystycznych gołoborzy (rumowisk skalnych) przed ingerencją człowieka (Park Narodowy, łączący istniejące rezerwaty, oficjalnie powołano 1 maja 1950 roku).
"„Puszcza królewska, książęca, biskupia, świętokrzyska, chłopska ma zostać na wieki wieków, jako las nietykalny, siedlisko bożyszcz starych, po których święty jeleń chodzi, – jako ucieczka anachoretów, wielki oddech ziemi i pieśń wieczności. Puszcza jest niczyja, nie moja, ani twoja, ani nasza, jeno, Boża, Święta”"
Sława pisarza i oddziaływanie poematu były tak wielkie, że zainteresowanie walorami przyrodniczymi Gór Świętokrzyskich ogarnęło ogromną część społeczeństwa, zaś odkrycia geologiczne na tym terenie spowodowały, że do dziś Góry Świętokrzyskie są wielką „mekką dla Geologów’.
W poł. lat 20. XX w. Oddział Kielecki PTK uruchomił kolejne schroniska turystyczne na terenie Łysogór – w Kielcach, dwa w rejonie Przełomu Lubrzanki i kolejne w Świętej Katarzynie w willi dziennikarza Alfonsa Dziaczkowskiego.
W roku 1926 przewodniczący sekcji PTK w Kielcach Edmund Massalski wraz z Kazimierzem Kaznowskim, wyznaczyli pierwszy szlak turystyczny w krainie świętokrzyskiej, prowadzący z Kielc do Nowej Słupi. Później został on przedłużony do Sandomierza. Znakiem szlaku był pasek czerwony pomiędzy dwoma białymi. W roku 1927 Oddział Ostrowiecki PTK wydał mapę "„Świętokrzyskie Szlaki Turystyczne”" w podziale 1:200 000, którą to opracował inż. T. Rekwirowicz. W drugim wydaniu wymienionej mapy z roku 1936 widać już szereg nowych szlaków wyznaczony w terenie. Znajdziemy tam szlaki z Kielc do Chęcin i dalej do przystanku kolejowego Rykoszyn oraz ze Św. Katarzyny przez Bodzentyn do Wąchocka. Szlaki turystyczne Gór Świętokrzyskich noszą w większości przypadków imiona ludzi, którzy włożyli ogromny wkład w propagowanie turystyki oraz krajoznawstwa regionu. Do grona „Wielkich Nauczycieli” zalicza się patronów szlaków: Edmunda Massalskiego, Edwarda Wołoszyna, Martę Hubicką, Edmunda Padechowicza, Sylwestra Kowalczewskiego, Juliusza Brauna, Henryka Orlińskiego, Stanisława Jeżewskiego, Jerzego Głowackiego, Stanisława Malanowicza, ks. Hieronima Konarskiego, Wacława Żelichowskiego.
Po II wojnie światowej powstał najdłuższy ciąg wycieczkowy pomiędzy Kuźniakami a Gołoszycami oznaczony w całości kolorem czerwonym (Główny Szlak Świętokrzyski). W późniejszym czasie dokonano jeszcze pewnych zmian w przebiegu szlaku ze względu na zaostrzenie przepisów ochrony przyrody (utworzenie Świętokrzyskiego Parku Narodowego) oraz rozwój sieci drogowej i kopalń, m.in. zlikwidowano przejście z Obożnej Drogi przez Górę Masłowską i Wiśniówkę, a wprowadzono wariant przez Białą Górę i Koszarkę. Zamknięto przejście przez Park Narodowy od kaplicy Św. Mikołaja do Przełęczy Huckiej, a wprowadzono obejście przez Kakonin, Bieliny (Podlesie) i Podłysicę.

</doc>
<doc id="1743" url="https://pl.wikipedia.org/wiki?curid=1743" title="Grecja">
Grecja

Grecja (gr. "Elláda", IPA: [] lub "Ellás", IPA: []), Republika Grecka ( "Ellinikí Dimokratía", IPA: []) – kraj położony w południowo-wschodniej części Europy, na południowym krańcu Półwyspu Bałkańskiego. Graniczy z czterema państwami: Albanią, Macedonią Północną i Bułgarią od północy oraz Turcją od wschodu. Ma dostęp do czterech mórz: Egejskiego i Kreteńskiego od wschodu, Jońskiego od zachodu oraz Śródziemnego od południa. Grecja ma dziesiątą pod względem długości linię brzegową na świecie, o długości 14 880 km. Poza częścią kontynentalną w skład Grecji wchodzi około 2500 wysp, w tym 165 zamieszkałych. Najważniejsze to Kreta, Dodekanez, Cyklady i Wyspy Jońskie. Najwyższym szczytem jest wysoki na 2918 m n.p.m. Mitikas w masywie Olimpu.
Grecja ma długą historię i bogate dziedzictwo kulturowe. Uważana jest za spadkobierczynię starożytnej Grecji. Jako taka, stanowi kolebkę całej cywilizacji zachodniej, miejsce narodzin demokracji, filozofii, igrzysk olimpijskich, sportu, wielu podstawowych twierdzeń naukowych, zachodniej literatury, historiografii, politologii oraz teatru, zarówno komedii, jak i dramatu. Świadectwem tej spuścizny jest 18 Obiektów Dziedzictwa Kulturowego UNESCO. Nowożytne państwo greckie zostało utworzone w wyniku zwycięskiego powstania przeciwko rządom osmańskim.
Współczesna Grecja jest rozwiniętym krajem, o wysokim wskaźniku rozwoju społecznego i innych wskaźnikach jakości życia. Od 2002 r. Grecja zrezygnowała z własnej waluty, przyjmując euro. W 2013 trafiła do grupy państw rozwijających się (pierwszy w historii przypadek degradacji kraju do tej grupy). Jest członkiem wielu organizacji międzynarodowych: Paktu Północnoatlantyckiego od 1952 roku, z przerwą w latach 1974–1980, Wspólnot Europejskich od 1981 r. oraz Europejskiej Agencji Kosmicznej od 2005 r. Także członkiem założycielem Organizacji Narodów Zjednoczonych, Organizacji Współpracy Gospodarczej i Rozwoju oraz Organizacji Współpracy Gospodarczej Państw Morza Czarnego.
Geografia.
Terytorium Grecji można podzielić na:
Prawie 1/5 powierzchni Grecji przypada na około 2500 wysp, z czego 165 jest zamieszkałych.
Największe z nich to:
Większość z wysp Grecji wchodzi w skład archipelagów, z których główne to:
Blisko 81% powierzchni Grecji zajmują pasma górskie (średnia wysokość 1200–1800 m n.p.m.), które mają przebieg południkowy. Obszary nizinne są niewielkie i występują w pobliżu wybrzeży (Niz. Salonicka, Tracka, Tesalska oraz Argolidzka). Półwysep Chalcydycki tworzy 3 wtórne półwyspy: Kassandra, Sithonia i Athos.
Klimat Grecji.
Grecja pozostaje pod wpływem klimatu śródziemnomorskiego. Cechuje go łagodna zima z suchym, gorącym latem. W najcieplejszym miesiącu średnia temperatura wynosi ponad 22 °C. Są co najmniej cztery miesiące ze średnią temperaturą ponad 10 °C, w zimie mogą zdarzać się przymrozki. Notuje się co najmniej trzy razy więcej opadów atmosferycznych w najwilgotniejszych miesiącach zimowych w porównaniu z suchym latem.
Regionalnie zróżnicowany klimat można jednak podzielić na trzy strefy: śródziemnomorski – obejmujący południową część kraju (szczególnie w rejonach ośrodków wypoczynkowych); alpejski – dominujący w rejonach górskich, takich jak Olympus i Pindos; umiarkowany, występujący w północnej części kraju.
Opady w Grecji również zależą od regionu. W zachodniej części Grecji z łagodną i często bardzo deszczową zimą, opady wahają się od 700 do 1500 mm rocznie. We wschodniej części Grecji jest bardziej sucho i opady wahają się od 400 do 500 mm rocznie.
Nazwa.
Polska nazwa "Grecja", podobnie jak nazwy w wielu innych językach, np. angielska "Greece", francuska "Grèce", pochodzi od łacińskiej nazwy "Graecia" stosowanej przez Rzymian i znaczącej „ziemia Greków”. Nazwa łacińska pochodzi zaś od greckiego "Γραικός", w starożytności nazwy własnej mieszkańców miejscowości Tanagra, w Beocji, grupy Hellenów, która jako pierwsza osiedliła się następnie w Italii.
Początkowo staroruska nazwa "Grieki", stosowana w nazwie drogi "iz Wariag w Grieki", i jej południowosłowiański odpowiednik "Grьkъ" (odnotowany w żywotach Metodego z IX w.) odnosiły się do całego Cesarstwa Bizantyńskiego, a nie tylko do ziem greckich. W tym samym znaczeniu u Długosza notowana jest nazwa "Grecia", będąca zlatynizowaną nazwą staroruską, a nie zniekształconym zapisem nazwy łacińskiej. Utożsamianie w tamtym czasie u Słowian nazwy "Grecja" z całym Cesarstwem Bizantyjskim, a nie z samymi ziemiami greckimi, wynika prawdopodobnie z używania w nim języka greckiego – był to na tym obszarze język państwowy, język religii, piśmiennictwa oraz codziennego porozumiewania się. Jeszcze w XVI w. u Stryjkowskiego ("Kronika Polska, Litewska, Żmudzka i wszystkiej Rusi") i Bielskiego ("Kronika, to jest historia świata") spotykana jest nazwa "Cesarstwo Greckie" w odniesieniu do nieistniejącego już wówczas Cesarstwa Bizantyńskiego. Od XVI w. w języku polskim nazwa "Grecyja" (później w zapisie "Grecja") odnoszona jest do ziem greckich, a następnie do niepodległego państwa greckiego. Jedynie sporadycznie w tym znaczeniu stosowane były inne nazwy – "Achaja" (odnotowywana w VI i XVII w.), "Liwadyja" (odnotowywana w XVIII w.) i "Hellas" (odnotowywana w XVIII i XIX w.).
Historia Grecji.
Silny wpływ na rozwój starożytnej cywilizacji greckiej wywarły warunki naturalne. Granice górskie podzieliły kraj na wiele niezależnych miast o różnorodnych formach władzy państwowej. Sąsiedztwo morza sprawiło, że Hellenowie byli odkrywcami i kupcami, dzięki czemu wymieniali towary i idee z innymi ludami świata śródziemnomorskiego. Bliski Wschód i Egipt wywarły wpływ na powstanie mitów greckich, które znalazły wyraz w poezji epickiej, a później w dramacie i sztukach plastycznych. Ziemia obfitowała w marmur i glinę, co umożliwiło wznoszenie świątyń oraz tworzenie rzeźb i ceramiki – głównych źródeł wyobrażeń o antycznej kulturze Grecji.
Greckie wybrzeża Morza Egejskiego były pierwszym miejscem w Europie, w którym wykształciły się zaawansowane cywilizacje – kultura cykladzka, kultura minojska oraz kultura mykeńska. Następnie na Peloponezie i w dzisiejszej Grecji Środkowej, a także na wyspach Morza Jońskiego i Egejskiego powstało wiele państw-miast ("poleis"). Położenie sprzyjało rozwojowi handlu i ekspansji kolonialnej na wybrzeża Azji Mniejszej, południowej Italii (Wielka Grecja) i Morza Czarnego. Mniejsze miasta sprzymierzały się z najsilniejszymi – Atenami i Spartą dla powstrzymania ekspansji Persów. Gdy odparto wroga, pojawiły się konflikty między "poleis", których kulminacją była wojna peloponeska. W ciągu stulecia po niej i po okresie hegemonii Teb, Grecja została zjednoczona pod rządami macedońskimi przez Filipa II. Po śmierci Filipa władzę przejął jego syn Aleksander, który podbił imperium perskie, jednocząc świat grecki z Bliskim Wschodem w jednym państwie. Po jego nagłej śmierci hellenistyczne imperium stało się obszarem walk pomiędzy jego wodzami.
W 146 r. p.n.e. Półwysep Bałkański i wyspy greckie zostały zajęte przez Rzymian. Grecja stała się prowincją rzymską, lecz nie przerwało to rozwoju greckiej kultury, którą nowi władcy przyjęli i rozprzestrzenili na ziemiach swojego imperium.
Po podziale cesarstwa rzymskiego Hellada znalazła się w Cesarstwie wschodniorzymskim, w którym dominował język (od VIII w. urzędowy) i kultura grecka. Przed zdobyciem Konstantynopola przez Turków wielu intelektualistów greckich wyemigrowało do Włoch i innych części Europy wolnych od ich panowania, przyczyniając się znacząco do wykształcenia kultury Renesansu i przeniesienia dorobku cywilizacji greckiej na zachód Europy. Ostatni cesarz, Konstantyn XI Dragazes, zginął 29 maja 1453. Osmański system milletów przyczynił się do tego, że Grecy zachowali odrębność kulturową w etnicznie posegregowanym imperium. Odegrało to ważną rolę w procesie odzyskiwania przez Grecję niepodległości.
17 marca 1821 wybuchło gwałtowne powstanie antytureckie. W okręgu Mani na półwyspie Peloponez zdobyto wszystkie trzy tureckie zamki garnizonowe. Następnie, 25 marca 1821, arcybiskup Patras, Germanos, wezwał wszystkich Greków do walki o wyzwolenie narodowe (25 marca jest oficjalnym świętem narodowym; drugim jest Dzień „Nie!”, 28 października, rocznica odrzucenia ultimatum Mussoliniego). Powstanie po krwawych walkach, z pomocą Anglii, Francji i Rosji, doprowadziło w 1830 do odzyskania niepodległości i powstania królestwa na terytorium Peloponezu i środkowej Grecji.
W wyniku wojny z imperium osmańskim w 1897 oraz wojen bałkańskich przyłączone zostały Kreta, Epir, oraz Macedonia z Salonikami. W czasie I wojny światowej Grecja długo była neutralna, a król Konstantyn I Glücksburg wykazywał silne sympatie proniemieckie. 27 czerwca 1917 Grecja włączyła się do wojny po stronie państw Ententy w atmosferze rządowego zamachu stanu, a król został zdetronizowany. Przegrana państw centralnych, w tym Imperium Osmańskiego, oraz pogromy i ludobójstwo ludności prawosławnej w Turcji, otworzyły przed rządem w Atenach szansę realizacji haseł Wielkiej Idei, czyli zjednoczenia Greków w ramach jednego państwa, nawiązującego do tradycji monarchii bizantyńskiej, ze stolicą w Konstantynopolu. Pod wpływem tej idei rządy Grecji podjęły kolejne decyzje o zajęciu obszaru dawnej Jonii, a następnie o kontynuacji grecko-tureckiej wojny. Z upoważnienia państw Ententy wojska greckie zajęły wtedy okręg Smyrny, o którego przyszłości zdecydować miało referendum. Turcja nie uznała zmiany granic, kontynuując działania wojenne. Grecy zdecydowali wtedy o ofensywie w głąb Anatolii, zakończonej klęską i kolejnymi, licznymi pogromami ludności prawosławnej w Azji Mniejszej. Zawarty w 1923 r. w Lozannie traktat pokojowy zatwierdzał wymianę ludności między Grecją a Turcją według kryterium wyznawanej religii. W 1924 proklamowano republikę, a w 1935 przywrócono monarchię. W obydwu wojnach światowych Grecy walczyli po stronie aliantów. W okresie II wojny światowej, po kapitulacji w 1941 roku, większość terytorium Grecji administrowana była przez faszystowski rząd kolaboracyjny.
W wyniku plebiscytu z 1 września 1946 ustrój Grecji określono jako monarchię konstytucyjną. W 1967 władzę w wyniku zamachu stanu przejęli wojskowi, których rządy w 1974 obalono zastępując władzą wybraną demokratycznie.
Od 1952 Grecja należy do NATO, (z przerwą od 1974r kiedy Grecja wystąpiła z NATO w proteście przeciw napaści Turcji na Cypr do 1980 r. gdy wstąpiła ponownie) a od 1981 roku jest członkiem EWG.
Ustrój polityczny.
Państwo greckie jest demokracją parlamentarną. Na jego czele stoi prezydent wybierany przez parlament na pięcioletnią kadencję. Władzę ustawodawczą sprawuje parlament oraz prezydent. Parlament składa się z 300 posłów wybieranych na czteroletnią kadencję.
Według nowych zasad 250 mandatów (miejsc) w parlamencie przyznawanych jest w systemie proporcjonalnym, a pozostałych 50 przypada partii, która uzyskała największą liczbę głosów. Aktualnie (9/2014) międzypartyjne konsultacje wskazują na możliwość likwidacji tego uprzywilejowania.
Władzę wykonawczą sprawuje rząd oraz prezydent. Premier wyznaczany jest przez prezydenta państwa, który powołuje na to stanowisko przewodniczącego partii mającej w parlamencie absolutną większość lub większość względną. Rząd musi otrzymać w parlamencie wotum zaufania.
Obecna sytuacja polityczna.
W czerwcu 2012 po powtórnych wyborach (po majowych nie utworzono rządu), premierem został Andonis Samaras z Nowej Demokracji tworząc koalicyjny rząd z PASOK-iem i Demokratyczną Lewicą (DIMAR). W czerwcu 2014, po uprzednim wycofaniu się DIMAR, krajem kierowały jedynie te same dwie partie, na przemian sprawujące władzę od 1974 r.
Po przedterminowych wyborach w styczniu 2015 roku, wygranych przez Koalicję Radykalnej Lewicy (SYRIZA), premierem został Alexis Tsipras, tworząc rząd koalicyjny z Niezależnymi Grekami. 5 lipca 2015 odbyło się referendum, w którym Grecy większością prawie 62% opowiedzieli się przeciwko zagranicznej pomocy finansowej na warunkach „trojki”.
22 stycznia 2020 prezydentem Grecji została Ekaterini Sakielaropulu, jest pierwszą kobietą na stanowisku prezydenta w Grecji.
System prawny.
W Grecji obowiązuje system prawa typu kontynentalnego. Na jego kształt znaczny wpływ wywarły prawo niemieckie i francuskie. Jego podstawą jest Konstytucja Grecji z 1975.
Podział administracyjny.
Grecja jest podzielona na 13 regionów ("περιφέρεια" – "periféreia", l.mn. "περιφέρειες" – "periféreies"), które dzielą się na 54 departamenty ("νομός" – "nomós", l. mn. "νομοί" – "nomoí").
W myśl uchwalonej już ustawy, z dniem 1 stycznia 2011 liczba gmin i departamentów uległa redukcji o ok. 40%. Zapewnić ma to budżetowi państwa oszczędność rzędu 1,8 mld euro rocznie.
Siły zbrojne Grecji.
Grecja dysponuje trzema rodzajami sił zbrojnych: wojskami lądowymi, marynarką wojenną oraz siłami powietrznymi. Uzbrojenie sił lądowych Grecji składało się w 2014 roku z: 1244 czołgów oraz 3571 opancerzonych pojazdów bojowych. Marynarka wojenna dysponowała 31 okrętami obrony przybrzeża, 13 fregatami, 4 okrętami obrony przeciwminowej oraz 8 okrętami podwodnymi. Greckie siły powietrzne wyposażone były (2014) m.in. w 224 myśliwce, 205 samolotów transportowych, 156 samolotów szkolno-bojowych, 202 śmigłowce wielozadaniowe oraz 29 śmigłowców szturmowych.
Wojska greckie liczyły (2014) 177,6 tys. żołnierzy zawodowych oraz 280 tys. rezerwistów. Według rankingu potencjału militarnego ("Global Firepower") (2014) greckie siły zbrojne zajmowały (2014) 57. miejsce na świecie, z rocznym budżetem na cele obronne w wysokości 6,5 mld dolarów (USD).
W 2020 wojsko greckie liczy sobie 200 tysięcy aktywnego personelu i 550 tysięcy rezerwistów.
Demografia.
Jako Grecy określa się 98% stałych mieszkańców (tzn. z pominięciem imigrantów ekonomicznych). Odmienność językowa lub kulturowa nie jest w Grecji tożsama z przynależnością do innego narodu. Jedynie ułamek ludności słowiańskojęzycznej oraz cała ludność używająca w domach języka tureckiego nie uważa się za Greków. Główne mniejszości narodowościowe, językowe lub kulturowe to Cyganie (ok. 300 tys.), Turcy i Pomacy (stanowiący łącznie ok. 100–130 tys. wyznawców islamu), Arumuni zwani Wlachami, Macedończycy, Albańczycy. Liczna jest grupa ludności słowiańskojęzycznej lub ludności dwu- i trójjęzycznej, identyfikująca się jako rdzenni Grecy. Według raportu Komitetu Helsińskiego, spośród ok. 200 tysięcy ludności słowiańskojęzycznej, rdzennie tubylczej, zamieszkałej w greckiej prowincji Macedonia, najwyżej 30 tysięcy osób identyfikuje się nie jako Grecy, lecz jako Macedończycy. Jako Grecy zdecydowanie określa się ludność używająca języka wołoskiego oraz Arvanici, tj. lud pochodzenia albańskiego, zamieszkały w całej Grecji od kilku stuleci, większa część Cyganów oraz tzw. Pontowie, jak określają się liczni rosyjskojęzyczni repatrianci z Rosji i ZSRR. Gęstość zaludnienia kraju wynosi 82 osób/km². Według danych meldunkowych, w miastach mieszka 63% ludności. Jednak w Grecji nie ma obowiązku zmiany meldunku przez obywatela w wypadku zmiany miejsca zamieszkania. Toteż w gminach zameldowania znacznej części mieszkańców Grecji są tylko przechowywane dane osób, w rzeczywistości tam już niezamieszkałych. Większe zespoły miejskie składają się ze spójnych urbanistycznie, lecz odrębnych administracyjnie jednostek, gmin miejskich, prawnie określanych także jako „miasto”. Do największych z nich należą (dane ze spisu powszechnego 2021):
Cudzoziemcy w Grecji.
Według danych służby statystycznej Eurostat, w 2008 roku Grecję zamieszkiwało legalnie około 906 tysięcy cudzoziemców, do których należeli głównie Albańczycy, stanowiący 63,7% ogółu legalnie przebywających w Grecji cudzoziemców. W rzeczywistości liczby te należy powiększyć jeszcze o co najmniej milion niezameldowanych w Grecji, jednak też zamieszkałych już na stałe imigrantów nielegalnych.
Nielegalni imigranci to głównie ludność pochodzenia azjatyckiego.
Religia.
Większa część stałych mieszkańców Grecji jest wyznawcami prawosławnego Autokefalicznego Kościoła Greckiego, na którego czele stoi metropolita Aten i całej Hellady (obecnie arcybiskup Hieronim II). Kościół podzielony jest administracyjnie na 81 diecezji, w których znajduje się 9000 kościołów i 300 monasterów (nie licząc autonomicznego okręgu Świętej Góry Athos).
Dane z 2017 według "Pew Research Center:"
Wśród imigrantów dominującą religią jest islam.
Gospodarka.
Spośród państw UE Grecja ma najgorszy wskaźnik wolności gospodarczej. W rankingu ogólnoświatowym nie mieści się nawet w pierwszej setce państw.
W latach 1981–2008 nastąpiły w Grecji znaczne przeobrażenia gospodarcze i rozpoczął się szybki rozwój gospodarczy. W miejsce rolno-surowcowej, z silnie zaznaczonym udziałem przemysłu, uformowała się gospodarka o większym udziale usług (głównie transport, obsługa turystów, handel, finanse), a z zanikającym, często przenoszonym za granicę przemysłem – proces dezindustrializacji, trwający także obecnie.
Większą rolę odegrało tu członkostwo (od 1981) i pomoc Wspólnoty Europejskiej. W latach od 1985 do 1991 Grecja otrzymała 2,5 mld dolarów amerykańskich w ramach tzw. planu integracyjnego; średni roczny przyrost produktu krajowego brutto wynosił w latach 1987–1990 1,6%. Usługi wytwarzają 78,3% produktu krajowego brutto, przemysł – 18%, rolnictwo – 3,6% (2011). Produkt krajowy brutto na 1 mieszkańca w 2007 roku wyniósł $27 360, czyli ok. 93% średniej unijnej. Największe bezrobocie w UE.
Od roku 2008 zaznaczył się gwałtowny spadek PKB przekraczający 25%. W greckiej gospodarce utrzymuje się wciąż problem niemożności osiągnięcia płynności płatniczej przez budżet państwa i przedsiębiorstwa. Toteż w okresach przedwyborczych, dodatkowe miliardy w banknotach dowożone są zza granicy, co jest mechanizmem dopuszczonym przez systemy zabezpieczeń strefy euro, przewidzianym dla potrzeb ad hoc zapewnienia płynności.
1 stycznia 2002 r. Grecja weszła do strefy euro. Od roku 2002 nastąpiło załamanie bilansu handlu zagranicznego, z -20 mld w 2001, do -65 mld USD w roku 2008. Poprawa rozpoczęła się w 2009, w warunkach głębokiej recesji i szybkiego spadku siły nabywczej ludności.
Mocnymi punktami gospodarki są turystyka i eksploatacja pierwszej w świecie (pod względem wartości jednostek) floty handlowej. Ponadto Grecy eksploatują liczne jednostki tzw. tanich bander.
Turystyka:
W 2015 roku kraj odwiedziło 23,599 mln turystów (7,1% więcej niż w roku poprzednim), wytwarzając przychód na poziomie 15,673 mld dolarów.
Sport.
Najpopularniejsze sporty w Grecji to piłka nożna (zwycięstwo drużyny narodowej na EURO 2004 i drugie miejsce Panathinaikosu w PEMK 1970/71) oraz koszykówka (wicemistrzostwo świata w 2006 r. po pokonaniu USA oraz wielokrotne zwycięstwa greckich klubów w Eurolidze). Dużą popularnością cieszą się też piłka wodna, lekkoatletyka czy siatkówka.

</doc>
<doc id="1744" url="https://pl.wikipedia.org/wiki?curid=1744" title="Gaz doskonały">
Gaz doskonały

Gaz doskonały, gaz idealny – abstrakcyjny, matematyczny model fizyczny gazu, spełniający następujące warunki:
Założenia te wyjaśniły podstawowe właściwości gazów. Po odkryciu własności cząstek w mechanice kwantowej, zastosowano te założenia też do cząstek kwantowych. Powyższe założenia prowadzą do następujących modeli:
Gaz doskonały należy odróżnić od płynu idealnego.
Klasyczny gaz doskonały.
Gaz taki w mechanice klasycznej opisuje równanie Clapeyrona (równanie stanu gazu doskonałego), przedstawiające zależność między ciśnieniem gazu "p", jego objętością "V", temperaturą "T" i licznością "n" wyrażoną w molach:
lub
Gaz doskonały to model, słuszny w pełni jedynie dla bardzo rozrzedzonych gazów. W rzeczywistych gazach wzrost ciśnienia powoduje, że zmniejszają się odległości między cząsteczkami oraz powoduje pojawianie się oddziaływań międzycząsteczkowych. Oddziaływania te odgrywają coraz większą rolę gdy maleje temperatura gazu zbliżając się do temperatury skraplania. W bardzo wysokich temperaturach zderzenia przestają być sprężyste. Model ten może być jednak stosowany w praktyce do niemalże wszystkich gazów w warunkach zbliżonych do normalnych. Dla gazów rzeczywistych przy dużych gęstościach i ciśnieniach niezbędne jest stosowanie równań uwzględniających te efekty (zob. równanie Van der Waalsa i wirialne równanie stanu).
Od gazu doskonałego należy odróżnić model o podobnie brzmiącej nazwie: płyn idealny.
Termodynamiczne funkcje stanu.
Wzory określające niektóre termodynamiczne funkcje stanu dla jednoatomowego gazu doskonałego:
Inne związki dla gazu doskonałego.
gdzie:

</doc>
<doc id="1745" url="https://pl.wikipedia.org/wiki?curid=1745" title="GusGus">
GusGus

GusGus – zespół muzyczny z Reykjavíku w Islandii, założony w 1995 z inicjatywy Sigurðura Kjartanssona i Stefána Árni Þorgeirssona.
Tworzy muzykę elektroniczną z elementami trip-hopu, house, techno i jazzu. Ich koncerty mają formę multimedialnego happeningu, gdzie muzyce towarzyszą efekty świetlne, tańce na scenie czy projekcje multimedialnych filmów.

</doc>
<doc id="1746" url="https://pl.wikipedia.org/wiki?curid=1746" title="Geografia">
Geografia

Geografia – nauka przyrodnicza i społeczna zajmująca się badaniem powłoki ziemskiej (przestrzeni geograficznej), jej zróżnicowaniem przestrzennym pod względem przyrodniczym i społeczno-gospodarczym, a także powiązaniami pomiędzy środowiskiem przyrodniczym a działalnością społeczeństw ludzkich. Nazwa geografia (gr. γεωγραφία) pochodzi od słów "ge" – „ziemia” i "grapho" – „piszę”. Za twórcę terminu uważa się Eratostenesa z Cyreny. Ze względu na różnorodność przedmiotu geografii i urozmaiconej metodologii częste są dyskusje wokół jej definicji oraz zakresu badawczego; proponuje się używanie w miejsce dotychczasowej nazwy „geografia” terminu „nauki geograficzne”.
Definicja według Stanisława Leszczyckiego:
"Geografia jest nauką o przestrzennym zróżnicowaniu struktur fizycznogeograficznych i społeczno-ekonomiczno-geograficznych i ich wzajemnym powiązaniu".
Przedmiot badań.
Przedmiotem badań geografii jest środowisko geograficzne obejmujące zarówno przyrodę, jak i gospodarkę. Geografia ma więc dwa przedmioty badań: środowisko naturalne oraz człowieka i jego działalność. Jest więc nauką dualistyczną.
Miejsce w systemie nauk.
Ze względu na swój dualizm (podwójny przedmiot badań) geografia należy zarówno do nauk przyrodniczych (geografia fizyczna) oraz do nauk społeczno-ekonomicznych (geografia społeczno-ekonomiczna); równocześnie poszczególne działy geografii fizycznej i społeczno-ekonomicznej wykazują ścisłe związki z innymi pokrewnymi gałęziami wiedzy. Działy geografii fizycznej wiążą się z naukami przyrodniczymi: geomorfologia z geologią i fizyką, klimatologia z fizyką, hydrografia z hydrologią i chemią, glacjologia z fizyką, geografia gleb z gleboznawstwem i biologią itd. Podobnie działy geografii społeczno-ekonomicznej wykazują powiązania z naukami społecznymi i ekonomicznymi, i tak np.: geografia osadnictwa z historią i urbanistyką, geografia ludności z demografią i statystyką itp. Geografia jest nauką syntetyzującą, problemową; w ujęciu przedmiotowym jest monotematyczna (formułująca prawidłowości na podstawie powtarzalności zjawisk) w ujęciu regionalnym jest idiograficzna (dająca opis przestrzenny zjawisk).
Metody badań.
Podstawą tej nauki jest badanie przestrzennych zależności, a najczęściej stosowanym narzędziem jest mapa.
Działy geografii.
Podstawowe działy geografii.
Geografię dzieli się na geografię ogólną i geografię regionalną. Według innego podziału można ją podzielić na geografię fizyczną i geografię ekonomiczną. 
1. Geografia fizyczna zajmująca się środowiskiem przyrodniczym, jego cechami, przestrzennym zróżnicowaniem tych cech oraz zjawiskami je kształtującymi. W skład geografii fizycznej wchodzą:
2. Geografia społeczno-ekonomiczna (geografia człowieka, antropogeografia) zajmująca się wszelkimi aspektami działalności człowieka, przestrzennymi strukturami życia społecznego i gospodarczego oraz poszczególnych działów gospodarki. W dziale geografii społeczno-ekonomicznej wyróżnia się:
3. Geografia regionalna zajmująca się kompleksowym opisem środowiska geograficznego regionów. Łączy zagadnienia geografii fizycznej i społeczno-ekonomicznej dotyczące danego regionu świata.
Historia geografii.
Greccy uczeni jako pierwsi zajmowali się geografią jako nauką i filozofią. Największy wkład wnieśli Tales z Miletu, Herodot, Eratostenes, Arystoteles, Strabon i Ptolemeusz. Rzymianie w czasie swoich podbojów rozwinęli sztukę rysowania map.
W średniowieczu do rozwoju tej dziedziny przyczynili się Arabowie (między innymi Al-Idrisi, Ibn Battuta czy Ibn Khaldun), którzy przejęli dziedzictwo naukowe Greków i Rzymian, aktywnie je rozwijając. W tym samym czasie horyzont geograficzny rozszerzył Marco Polo. Wiek XV i XVI to epoka wielkich odkryć geograficznych. Napływ szczegółowych danych wymagał lepszych fundamentów teoretycznych oraz lepszych technik kartograficznych, czego wyrazem są "Geographia generalis" Vareniusa i mapa świata Merkatora.
W XVIII i XIX wieku geografia staje się oddzielną dziedziną nauki i zostaje wprowadzona do programów uniwersyteckich. W ciągu ostatnich dwustu lat nastąpił znaczny rozwój nauk geograficznych oraz stosowanych w nich metod i narzędzi badawczych.
W rozwoju geografii znaczącą rolę odegrały organizacje skupiające badaczy i miłośników geografii. Pierwsze stowarzyszenie geograficzne w historii świata Akademię Argonautów założył w 1684 roku włoski franciszkanin oraz geograf Vincenzo Maria Coronelli. Należało do niego m.in. trzech Polaków: król polski Jan III Sobieski, marszałek wielki koronny Stanisław Herakliusz Lubomirski oraz kanclerz wielki koronny Jan Wielopolski. W XIX wieku powstały także brytyjskie Królewskie Towarzystwo Geograficzne czy amerykańskie Narodowe Towarzystwo Geograficzne, a w Polsce Polskie Towarzystwo Geograficzne istniejące od 1918 roku.
Tradycje najstarszej katedry geografii w Polsce, która została założona w 1849 roku i była kierowana przez Wincentego Pola, kontynuuje Instytut Geografii i Gospodarki Przestrzennej Uniwersytetu Jagiellońskiego w Krakowie.

</doc>
<doc id="1747" url="https://pl.wikipedia.org/wiki?curid=1747" title="GNU Scientific Library">
GNU Scientific Library

GNU Scientific library – biblioteka funkcji obliczeniowych i naukowych dla C i C++ dostępna na zasadach GPL. Biblioteka jest częścią Projektu GNU.
Przykład użycia.
Poniższy przykładowy program oblicza wartość funkcji Bessela dla argumentu 5:
int main(void)
 double x = 5.0;
 double y = gsl_sf_bessel_J0(x);
 printf("J0(%g) = %.18e\n", x, y);
 return 0;
Program musi być skonsolidowany z biblioteką GSL:
Wynik pracy programu jest pokazany poniżej (powinien być poprawny dla podwójnej precyzji):
Możliwości.
Biblioteka zawiera ponad tysiąc funkcji, dotyczących:
Wsparcie dla C++.
Biblioteka GSL może być używana w C++, ale nie może używać wskaźników do metod lecz tylko wskaźników do zwykłych funkcji. Zamiast tego, programiści C++ mogą użyć statycznych funkcji przekierowujących do właściwej metody klasy. Adres używany do statycznych metod w C++ jest kompatybilny z GSL. Dostępne są gotowe wrappery C++ do GSL.

</doc>
<doc id="1748" url="https://pl.wikipedia.org/wiki?curid=1748" title="GPL">
GPL



</doc>
<doc id="1749" url="https://pl.wikipedia.org/wiki?curid=1749" title="George W. Bush">
George W. Bush

George Walker Bush (wym. ; ur. 6 lipca 1946 w New Haven) – amerykański polityk, 43. prezydent Stanów Zjednoczonych, 46. gubernator Teksasu. Jest synem 41. prezydenta, George’a H.W. Busha.
Po ukończeniu studiów na Uniwersytecie Yale w 1968 oraz w Harvard Business School w 1975 pracował na stanowiskach kierowniczych w przedsiębiorstwach naftowych. Był współwłaścicielem klubu Texas Rangers, a następnie sprawował urząd gubernatora stanu Teksas (1995–2000). W 2000 roku uzyskał nominację Partii Republikańskiej i wygrał w kontrowersyjnych wyborach prezydenckich. Objął urząd prezydenta Stanów Zjednoczonych 20 stycznia 2001, kilka miesięcy przed zamachami z 11 września. W ich konsekwencji George W. Bush ogłosił strategię walki w ramach wojny z terroryzmem, zakładającej operacje wojskowe w Afganistanie (od 2001) oraz inwazję na Irak (2003). W 2004 został wybrany na drugą kadencję, która upłynęła w 2009.
Życiorys.
Młodość i wykształcenie.
Wychowywał się w Midland i Houston w Teksasie, z czwórką rodzeństwa: Jeb, Neil, Marvin i Dorothy. Młodsza siostra Robin zmarła w 1953 w wieku trzech lat na białaczkę. George Bush uczęszczał do Phillips Academy w Andover w stanie Massachusetts, gdzie zajmował się m.in. grą w baseball. Następnie studiował na Uniwersytecie Yale, gdzie uzyskał licencjat ("Bachelor") z historii w 1968.
W maju 1968 został przyjęty do służby w lotnictwie Gwardii Narodowej Teksasu ("Texas Air National Guard"), mimo uzyskania w teście zdolności najniższego akceptowalnego wyniku 25%. Po szkoleniu służył w bazie Ellington w Houston, latając na myśliwcach Convair F-102 Delta Dagger. Zdaniem jego krytyków podczas służby cieszył się przywilejami związanymi z pozycją jego ojca, nie został też wysłany na wojnę w Wietnamie, w przeciwieństwie do wielu młodych Amerykanów w tym okresie. W maju 1972 uzyskał przeniesienie do Gwardii Narodowej Alabamy, w celu pracy w kampanii wyborczej do Senatu. Brak dowodów w dokumentacji, aby pełnił tam aktywną służbę i uczęszczał na ćwiczenia, a w sierpniu 1972 został zawieszony w lataniu z powodu niepodejścia do testów medycznych. W maju 1973 powrócił do Houston, w październiku 1973 został zwolniony ze służby przed sześcioletnim terminem w celu odbycia studiów w Harvard Business School.
Praca zawodowa i początki kariery politycznej.
Po ukończeniu Harvard Business School ze stopniem magistra (MBA) Bush podjął pracę w przemyśle naftowym w Teksasie, zajmując stanowiska kierownicze w kilku spółkach. W 1977 ożenił się z Laurą Welch, po czym osiedlili się w Midland w Teksasie. Bush przeszedł wówczas za żoną z Kościoła Episkopalnego do Zjednoczonego Kościoła Metodystycznego.
W 1978 startował w wyborach do Izby Reprezentantów, lecz przegrał z Kentem Hance’em. W 1988 rodzina Bushów przeniosła się do Waszyngtonu, w celu pracy dla kampanii prezydenckiej ojca George’a. Po powrocie do Teksasu zainwestował w 1989 w drużynę baseballową Texas Rangers, w której sprzedaż udziałów w 1998 przyniosła mu zysk ponad 14 milionów dolarów.
W 1995 wygrał wybory na gubernatora Teksasu.
Wybór George’a W. Busha na prezydenta w 2000 roku wzbudził w części społeczeństwa amerykańskiego kontrowersje związane z wątpliwościami co do poprawności procesu liczenia głosów. George W. Bush otrzymał mniej głosów wyborców niż jego kontrkandydat Al Gore, jednak wygrał wybory uzyskując większość głosów Kolegium Elektorów Stanów Zjednoczonych. Podobna sytuacja miała wcześniej miejsce w 1828, 1876 i ponownie w 1888, gdy prezydentem został Benjamin Harrison, który zdobył o 100 tysięcy głosów mniej niż jego konkurent, prezydent Grover Cleveland. W listopadzie 2004 Bush wygrał wybory stosunkowo niewielką przewagą, pokonując kandydata demokratów Johna Kerry’ego i został wybrany ponownie na prezydenta na następne 4 lata.
Kontrowersje podczas wyborów w roku 2000.
George W. Bush po długotrwałych sporach sądowych ze swoim konkurentem, wiceprezydentem w administracji Clintona, Alem Gore’em został wybrany na prezydenta Stanów Zjednoczonych, pomimo tego, że Al Gore uzyskał więcej głosów wyborców (50 999 tys.) niż Bush (50 456 tys.), ale mniej głosów elektorskich. Zadecydowały o tym wysoce dyskusyjne wyniki wyborów na Florydzie. Spory uciął ostatecznie Federalny Sąd Najwyższy, który nakazał zaprzestanie ponownego przeliczania głosów jako niezgodnego z 14. poprawką do Konstytucji.
Polityka Busha.
George W. Bush wywodzi się z prawicowego skrzydła Partii Republikańskiej i startował do wyborów pod hasłem „powrotu do starych wartości”. Jego prezydentura była pod wyraźnym wpływem neokonserwatystów (polityka zagraniczna i gospodarka) oraz paleokonserwatystów (polityka społeczna), co wywołało niezadowolenie części społeczeństwa o poglądach lewicowych. Społeczeństwo amerykańskie jest podzielone, lecz jego większa część popierała tę politykę, co udowodniła, wybierając G. Busha na drugą kadencję. Na zmianę nastrojów, a w szczególności na zmęczenie wojną w Iraku, wskazują wybory w połowie kadencji („midterm elections”) w listopadzie 2006, po których Partia Demokratyczna uzyskała większość w obydwu izbach Kongresu, jak i na stanowiskach gubernatorów stanowych.
Prezydenturę Busha charakteryzowała kontynuacja polityki jego ojca oraz Ronalda Reagana.
Nominacje sędziowskie.
George W. Bush mianował dwóch sędziów do Sądu Najwyższego: Johna Robertsa (mianowany we wrześniu 2005 na stanowisko prezesa) i Samuela Alito (w 2006).
Stosunki z Polską.
Trzykrotnie (w 2001, 2003 i 2007) złożył oficjalne wizyty w Polsce.
8 czerwca 2007 przybył z 4-godzinną wizytą do Polski. Swoim samolotem wylądował w Gdańsku, gdzie Bushów przywitała polska para prezydencka. Nieformalne rozmowy prezydentów (bez krawatów – protokół dyplomatyczny definiuje to jako rozmowę nieformalną) odbyły się w prezydenckim ośrodku na Mierzei Helskiej.
Bush zaoferował Polsce modernizację polskich sił zbrojnych w zamian za instalację elementów "tarczy antyrakietowej".
Polska jest sojusznikiem Amerykanów w walce z terroryzmem. Władze RP w 2002 i 2003 wysłały kontyngenty wojskowe do Afganistanu i Iraku.
W kwestii wiz, które muszą kupować Polacy, by móc polecieć do Stanów Zjednoczonych, mówił wielokrotnie, że ta sprawa leży w gestii Kongresu. Jednak podczas konferencji prasowej z premierem Donaldem Tuskiem powiedział: "Ja rozumiem, że to frustrująca sprawa. Gdybym mieszkał w Polsce i chciał przyjechać do Ameryki, też byłbym sfrustrowany".
Wojna z terroryzmem.
Po zamachu terrorystycznym z 11 września administracja prezydencka postanowiła doprowadzić do rozbicia Al-Ka’idy. Pierwszym etapem wojny była zbrojna interwencja w Afganistanie, jednak nie pokonano całkowicie talibów i cały czas dochodzi tam do zamachów i walk.
Stany Zjednoczone dążyły też (podobnie jak obecnie) do rozbicia Al-Ka’idy w Somalii, gdzie przy ich wsparciu wojska etiopskie wyparły ze stolicy (Mogadiszu) na przełomie 2006 i 2007 bojówkarzy Unii Trybunałów Islamskich, oskarżanych o współpracę z Al-Ka’idą. Amerykanie twierdzą, że podczas nalotu w maju 2008 zabili szefa somalijskiej Al-Ka’idy.
Wojna w Iraku.
Mimo braku poparcia ONZ Stany Zjednoczone wraz ze swoimi sojusznikami (w tym Polską) dokonały interwencji w Iraku (druga wojna w Zatoce Perskiej). Oficjalnym powodem była chęć zniszczenia broni masowego rażenia będącej rzekomo w posiadaniu dyktatora oraz chęć zaprowadzenia demokracji w tym kraju. Podbój okazał się militarnym sukcesem. W ciągu miesiąca zdobyto Bagdad, a notowania Busha były wtedy najwyższe w historii. W przeciągu następnych miesięcy aresztowano większość członków dawnego reżimu z Saddamem Husajnem na czele. W czasie działań wojennych w dużym stopniu Amerykanom pomogli kurdyjscy rebelianci, wyzwalając miasto Mosul. Mimo wszystkich sukcesów militarnych tej operacji, dużych składów broni masowego rażenia do tej pory nie znaleziono. Według raportu National Ground Intelligence Center wydziału do spraw wywiadu Departamentu Obrony Stanów Zjednoczonych znaleziono ponad 500 sztuk broni chemicznej wyprodukowanej przed rokiem 1991, zazwyczaj były to pojedyncze egzemplarze i niewielkie składy amunicji chemicznej zagubione podczas wojny iracko-irańskiej (1980-1988). Komisja senacka dowiodła, że gabinet Busha nie posiadał rzetelnych dowodów na istnienie tej broni, choć twierdził z uporem, że je ma. Od początku inwazji zginęło około 600 tysięcy Irakijczyków, dwa miliony opuściło kraj, a cztery miliony wyemigrowało. Armia amerykańska straciła 3700 żołnierzy, sama zaś wojna kosztowała ten kraj ponad 400 miliardów dolarów.
W grudniu 2006 publicznie przyznał, że nie wygrywa wojny w Iraku. Stany Zjednoczone wydały na prowadzenie wojny wiele miliardów dolarów, co budzi sprzeciw opinii publicznej. George Bush zamierzał wysłać do Iraku jeszcze 25 000 żołnierzy, mimo sprzeciwu ekspertów i opinii publicznej.
Jednak od 2007 dzięki nowej taktyce amerykańskiej armii spadła liczba zamachów na amerykańskich żołnierzy i irackich cywilów.
Bliski Wschód.
Prezydent Bush i jego administracja popierali dążenia Palestyńczyków do utworzenia własnego państwa, lecz również działania Izraela zmierzające do zlikwidowania baz Hamasu. Bush popierał umiarkowany Fatah i administrację palestyńska z prezydentem Mahmudem Abbasem. Jesienią 2007 z inicjatywy Busha w Anapolis odbyła się międzynarodowa konferencja dotycząca przyspieszania palestyńsko–izraelskich rozmów pokojowych.
W lutym 2008 prezydent Bush złożył wizytę w Izraelu i Autonomii Palestyńskiej, by zachęcić przywódców obu krajów do dalszego dialogu. W marcu 2008 została złożona publiczna opinia prezydenta Busha, że do końca trwania jego kadencji zostanie zawarty trwały pokój między Palestyną a Izraelem. Pod koniec kadencji zauważono wzmożone zaangażowanie Busha w politykę Bliskiego Wschodu.
Rosja.
Stosunki z Rosją pogorszyły się zwłaszcza po rozpoczęciu przez Stany Zjednoczone negocjacji z Polską i Czechami w sprawie instalacji elementów tak zwanej tarczy antyrakietowej. Władze Rosji argumentują wciąż, że tarcza godzi w rosyjską rację stanu.
Oba kraje poróżniła także kwestia rozszerzenia NATO na Gruzję i Ukrainę. Prezydent Bush podczas wizyty w Kijowie poparł starania obu krajów o członkostwo w NATO.
Stosunki z Europą Zachodnią.
Wojna w Iraku bardzo poróżniła Stany Zjednoczone z Francją i Niemcami, które były przeciwne interwencji wojsk amerykańskich w Iraku. Głównym sojusznikiem Busha był premier Tony Blair. W czasie drugiej kadencji stosunki USA z tymi państwami się polepszyły – głównie dzięki zmianie władzy w Niemczech (2005) i we Francji (2007). Zarówno Nicolas Sarkozy, jak i Horst Köhler i Angela Merkel byli bardziej proamerykańscy niż ich poprzednicy.
Wielka Brytania była za prezydentury Busha i nadal pozostaje głównym sojusznikiem Amerykanów w Europie.
Chiny.
Stosunki chińsko-amerykańskie w okresie prezydentury Busha były dość dobre.
Prezydent Bush był dość powściągliwy w 2008 w komentowaniu tłumieniu przez komunistyczne władze w Pekinie tybetańskich protestów. Zachęcał Chiny do dialogu z Dalajlamą.
Bush zapowiedział, że będzie w Pekinie podczas inauguracji igrzysk olimpijskich.
W październiku 2007 spotkał się z Dalajlamą i uczestniczył w ceremonii nadania mu Złotego Medalu Kongresu.
Iran.
Bush i jego administracja były przeciwnikami irańskiego programu atomowego. Dotychczasowe negocjacje nic nie dały, lecz co pewien czas obie strony wychodziły z propozycją dalszych rozmów. Za prezydentury Obamy, Iran wciąż argumentuje, że prowadzi badania wyłącznie w celach pokojowych, zaś USA nadal widzi w tym poważne niebezpieczeństwo i chęć budowy przez Iran broni jądrowej.
Kryzys finansowy 2007–2009.
Administracja Busha zwróciła się we wrześniu 2008 do Kongresu o zgodę na wykupienie za 700 miliardów USD długów hipotecznych, grożących pogrążeniem amerykańskiego sektora finansowego, a tym samym całej gospodarki, w zupełnym chaosie. Został on przyjęty (z modyfikacjami) przez Kongres.
Wybory w 2004.
Bush ponownie został wybrany na prezydenta w wyniku wyborów w 2004. Jego konkurentem był John Kerry. Bush w skali kraju zdobył 3 miliony głosów więcej niż Kerry.
Wybory 2008.
W wyborach prezydenckich 2008 Bush poparł kandydaturę Johna McCaina, który był jego konkurentem o nominację w 2000. Bush nie wziął udziału w konwencji delegatów Partii Republikańskiej we wrześniu 2008 z powodu huraganu Gustav. Po zwycięstwie Baracka Obamy pogratulował mu zwycięstwa.
Po prezydenturze.
W 2010 wydano jego autobiografię "„Decision points” (Kluczowe decyzje)".
W styczniu 2011 Amnesty International wezwała rządy krajów afrykańskich, które wizytował Bush, do aresztowania tego ostatniego. Organizacja argumentowała, że George W. Bush za swojej prezydentury aprobował stosowanie tortur (między innymi podtapianie).
Po opuszczeniu urzędu nie udzielał się publicznie. Dopiero w lipcu 2013 wziął w Dallas udział w uroczystości nadania obywatelstwa dwóm imigrantom, co miało być wyrazem jego poparcia dla projektu nowej reformy imigracyjnej.
Po drugiej kadencji Bush zajął się malowaniem obrazów. Początkowo było to obiektem żartów, z czasem jednak jego prace uzyskały akceptację krytyków sztuki.
W 2018 otrzymał honorowe obywatelstwo Wilna.
Życie prywatne.
Od 5 listopada 1977 żonaty z Laurą Welch. Mają córki bliźniaczki Barbarę i Jennę urodzone 25 listopada 1981. 10 maja 2008 Jenna wyszła za mąż za Henry’ego Hagera, syna przewodniczącego Partii Republikańskiej w Wirginii.
Uważa się za człowieka głęboko wierzącego. Należy do kościoła metodystycznego. Religia pomogła mu wyrwać się z alkoholizmu w 1986.
George Bush został dwukrotne wybrany „Człowiekiem Roku” tygodnika „Time”. Pierwszy raz w roku 2000, a drugi w 2004. Jego ojciec – prezydent George H.W. Bush został wybrany raz – w 1990.
Upamiętnienie w popkulturze.
W 2018 filmie "Vice" w reżyserii Adama McKaya w postać Busha wcielił się Sam Rockwell.

</doc>
<doc id="1750" url="https://pl.wikipedia.org/wiki?curid=1750" title="Giorgione">
Giorgione

Giorgione, właśc. Giorgio Barbarelli da Castelfranco lub Zorzi di Castelfranco (ur. w 1478 lub 1479 w Castelfranco Veneto, zm. przed 25 października 1510 w Wenecji) – włoski malarz, przedstawiciel renesansu. 
O życiu i twórczości Giorgionego wiemy bardzo mało. Jest jednym z najbardziej zagadkowych malarzy czasów nowożytnych. Wiadomo, że urodził się w Castelfranco Veneto i był uczniem Giovanni Belliniego. Miał wpływ na innych malarzy, z którymi się zetknął, m.in. Tycjana i Sebastiana del Piombo. Zmarł przedwcześnie w okresie zarazy.
Był jednym z głównych przedstawicieli i twórców dojrzałego renesansowego malarstwa weneckiego, realizującym po mistrzowsku jego założenia kolorystyczne. Wspaniały mistrz koloru i światła. Przedstawiciel koloryzmu weneckiego, obserwator chwilowych stanów natury i świata.
W swoich obrazach Giorgione realizuje nową koncepcję przedstawiania wyglądów natury, opartą na kolorze, którą rozwiną i uzupełnią jego wielcy następcy, jak Tycjan, Veronese czy Tintoretto, nie naruszając jej generalnych założeń. Polegała ona na wyrażeniu wszystkich najistotniejszych cech struktury wizualnej przedmiotu, a więc modelunku bryły i jej umieszczenia w przestrzeni i atmosferze, światła i głębi przestrzennej, za pomocą układu barw o różnym natężeniu barw i różnej temperaturze. 
Giorgione, a za nim inni wenecjanie, za ten unifikujący element uznali kolor. Płaszczyzna obrazu zostaje pokryta układem plam barwnych, układy linearne stają się uzupełnieniem kolorystycznej struktury, a w bardziej skrajnych przypadkach stają się zbędne. Taki sposób kształtowania opiera się w większym stopniu na wrażeniach kolorystyczno-świetlnych niż motywie. Obserwując malarstwo Tycjana widać, jak wzbogacił środki kolorystycznego kształtowania, eliminując stopniowo rolę linii i nieopartego na kolorze modelunku światłocieniowego, ograniczając przez to iluzjonizm przedstawienia. Wreszcie ten sposób ukształtowania określa się jako ton lub tonację obrazu. W malarstwie Giorgionego te wszystkie założenia zostały z grubsza zrealizowane. Nie doszedł jednak do tak radykalnych rozwiązań, jak Tycjan w ostatnich obrazach swego długiego życia.
Do czasów współczesnych nie zachowały się żadne podpisane i datowane prace Giorgionego. Duże rozbieżności wzbudza już liczba przypisywanych mu prac (od 5 do 40). Tworzył obrazy o tematyce religijnej: "Maria z Dzieciątkiem i świętymi Liberiuszem i Franciszkiem", tzw. "Madonna z Castelfranco", "Judyta", mitologicznej: "Śpiąca Wenus" (ukończona przez Tycjana), "Wenus odpoczywająca", portrety i alegoryczne, oraz trudne do interpretacji kompozycje figuralne w pejzażu: "Burza", "Trzech filozofów", "Koncert wiejski". W obrazach Giorgionego krajobraz nie stanowi jedynie tła sceny, ale tworzy wszechogarniającą przestrzeń; zgodnie z duchem renesansu, człowiek w jego obrazach pozostaje w idealnej harmonii z naturą. Obrazy Giorgionego cechuje harmonijne zespolenie przedstawionych postaci z krajobrazem, miękki modelunek, liryzm i nastrojowość sugerujące treści symboliczne i alegoryczne. Dążył do symbiozy malarstwa, muzyki i poezji. Jego nowatorstwo, oprócz ujęcia krajobrazu, objawiło się w także w przedstawieniu kobiecego aktu. W obrazie "Śpiąca Wenus" naga kobieta, po raz pierwszy w nowoczesnym malarstwie, stała się głównym i jedynym tematem dzieła malarskiego.
Autorstwo wielu dzieł Giorgionego jest sporne, przyjmuje się pogląd o istnieniu kręgu Giorgiona (tzw. "giorgioneschi") złożonego z wielbicieli i naśladowców artysty.

</doc>
<doc id="1751" url="https://pl.wikipedia.org/wiki?curid=1751" title="Giotto di Bondone">
Giotto di Bondone

Giotto di Bondone, właśc. "Angiolo di Bondone", zdrobniale "Angiolotto" (ur. ok. 1266, zm. 8 stycznia 1337 we Florencji) – malarz i architekt włoski, tercjarz franciszkański.
Do najważniejszych jego dzieł należą: freski w kaplicy Scrovegnich w Padwie, w kościele Santa Croce we Florencji i w bazylice św. Franciszka w Asyżu.
Życiorys.
Zgodnie z przekazami Lorenza Ghibertiego i Giorgia Vasariego, Cimabue miał zobaczyć rysunki owiec wykonane przez Giotta, gdy ten był jeszcze młodym pastuszkiem, i przyjąć go do swojego warsztatu. Choć trudno stwierdzić, na ile prawdziwa jest ta legenda, to za fakt uznaje się, iż Cimabue był mistrzem Giotta. Według innej legendy Cimabue wyszedł ze swojej pracowni zostawiając w niej swój obraz. Giotto dla żartu namalował małą muchę na nosie jednej z figur namalowanych przez Cimabue. Po powrocie Cimabue próbował odgonić muchę ze swojego obrazu, ponieważ była namalowana perfekcyjnie i mistrz myślał, że jest prawdziwa. Cimabue w latach 1288–1292 wykonywał freski w bazylice św. Franciszka w Asyżu. Wraz z mistrzem pojawił się i jego uczeń. Przy freskach pracowało wielu artystów m.in. z Rzymu i dzięki temu Giotto mógł zdobyć gruntowne wykształcenie. Utalentowany młody malarz był coraz częściej dopuszczany do pracy przy freskach i dzisiaj badacze wiodą spór o autorstwo poszczególnych malowideł. Z pewnością rola Giotta w malowaniu fresków stopniowo wzrastała, ale przypisywanie mu całości dzieła to uproszczenie wynikające z jego późniejszej sławy.
Niewykluczone, że młody Giotto wraz z mistrzem odbył podróże także do Pizy i Rzymu; przypisuje mu się fragmenty fresków z Bonifacym VIII w bazylice na Lateranie i medaliony z prorokami z Santa Maria Maggiore (ok. 1290). Po powrocie do Florencji Giotto ożenił się z Ciutą di Lapo del Pela. Mieli ośmioro dzieci: czterech synów i cztery córki.
We wczesnym okresie twórczości wykonał niemal 6-metrowy krucyfiks dla kościoła Santa Maria Novella we Florencji, "poliptyk z Badii", "Madonnę z Dzieciątkiem" i "Stygmatyzację św. Franciszka".
Według legendy papież Benedykt XI poszukując artystów, wysłał do Giotta posłańca, który zlecił artyście namalowanie jakiegoś dzieła dla papieża, które potwierdzałoby jego kunszt. Giotto namalował jednym ruchem idealny okrąg i wręczył go posłańcowi. Posłaniec myślał, że Giotto kpi z niego, ale papież po zobaczeniu okręgu Giotta zrozumiał, że tylko wspaniały artysta może namalować jednym ruchem idealny okrąg bez żadnych dodatkowych przyrządów. Według jednej z interpretacji okręgi namalowane przez Rembrandta na obrazie Autoportret z dwoma kołami nawiązują do legendy o okręgu Giotta.
W latach 1303–1305 Giotto pracował w Padwie malując freski kaplicy Scrovegnich (dell'Arena). Wówczas po raz pierwszy pojawił się w źródłach pisanych; tym samym te freski to pierwsze chronologicznie dzieło, które bezdyskusyjnie mu przypisano. Kaplica poświęcona jest Marii i Jezusowi. Na ścianach bocznych umieszczono po trzy cykle obrazów, jeden nad drugim. Górny prawy cykl to sześć obrazów z życia Joachima i Anny, rodziców Marii. Górny lewy to sześć obrazów o życiu Marii do momentu Zwiastowania. Środkowy cykl to dwadzieścia dwa obrazy z życia Jezusa, usytuowane po obu stronach kaplicy. Natomiast cykle dolne to alegorie siedmiu cnót (strona prawa) i siedmiu grzechów głównych (strona lewa). Wszystkie cykle wiąże ze sobą fresk "Sąd Ostateczny" namalowany na ścianie nad wejściem do kaplicy. Freski w prezbiterium kaplicy Scrovegnich powstały po roku 1317 i wykonali je prawdopodobnie uczniowie Giotta.
Giottowi często przypisuje się autorstwo cyklu fresków świętego Franciszka w bazylice św. Franciszka w Asyżu, jednak część badaczy sprzeciwia się tej opinii. Brak jest źródeł, które jednoznacznie potwierdziłyby taką atrybucję (wspominają one tylko, że Giotto pracował w Asyżu albo nawet wymieniają bazylikę pod tym wezwaniem, ale bez podania konkretnego miejsca). Giorgio Vasari w połowie XVI w. jako pierwszy przypisał ten cykl Giottowi i do dziś tę opinię powtarza większość publikacji. Cykl został wykonany w górnym kościele, w dolnym pasie przedstawień. Łącznie namalowanych jest 28 scen, opartych na biografii św. Franciszka "Legenda Maior" pióra św. Bonawentury.
W latach 1310–1320 Giotto prowadził prace malarskie we florenckiej bazylice Santa Croce, dekorując freskami cztery kaplice. Od roku 1328 pracował w Neapolu na zlecenie Roberta Andegaweńskiego, króla Neapolu. Dekorował freskami kaplicę pałacową i stworzył cykl obrazów "Sławni ludzie" w wielkiej sali pałacu królewskiego. Po powrocie do Florencji w 1334 roku otrzymał stanowisko nadzorcy budowy miasta i katedry. Zaprojektował dzwonnicę, Ponte Carraia (most na rzece Arno), zmodernizował fortyfikacje.
Do jego uczniów należeli m.in. Taddeo Gaddi i Maso di Banco.
Wkrótce umarł w pełni sławy. Cennino Cennini, włoski malarz i teoretyk malarstwa napisał o nim: "sztukę malarską zamienił z greckiej w łacińską i uczynił ją nowoczesną; jego dzieło było doskonalsze niż czyjekolwiek gdziekolwiek indziej." Giottem zachwycali się krytycy i malarze prawie każdej epoki: nazareńczycy, prerafaelici, malarze z Pont-Aven z Gauguinem na czele. Wszyscy doceniali skalę barw, harmonię koloru, zrównoważoną kompozycję i metafizyczny spokój malowanych postaci.

</doc>
<doc id="1752" url="https://pl.wikipedia.org/wiki?curid=1752" title="GNU Compiler Collection">
GNU Compiler Collection

GNU Compiler Collection (GCC) – zestaw kompilatorów o otwartym kodzie źródłowym rozwijany w ramach Projektu GNU. Rozpowszechniany jest na licencji GPL oraz LGPL.
GCC jest podstawowym kompilatorem w systemach uniksopodobnych, przy czym szczególnie ważną rolę odgrywa w procesie budowy jądra Linuksa.
Historia.
Początkowo skrótowiec GCC oznaczał GNU C Compiler, ponieważ był to kompilator wyłącznie do języka C.
Pierwsza wersja kompilatora o numerze 1.0 została opublikowana 23 maja 1987 przez Richarda Stallmana.
Znaczącym wydarzeniem w historii rozwoju GCC było wydanie wersji 2.95 w lipcu 1999 – pierwszej po zintegrowaniu z projektem EGCS.
Kompilatory dostępne w GCC.
W skład GCC wchodzą kompilatory następujących języków programowania:
a także eksperymentalnie
Istnieje również frontend języka D dla GCC – gdc.
Środowisko pracy.
Kompilatory wchodzące w skład GCC mogą być uruchamiane na wielu różnych platformach sprzętowych i systemowych. Za ich pomocą można generować kod wynikowy przeznaczony dla różnych procesorów i systemów operacyjnych oraz dokonywać tzw. kompilacji skrośnej.
Lista kilku najważniejszych architektur sprzętowych, na których uruchomiono GCC:
Poniżej zestawiono systemy operacyjne umożliwiające uruchomienie GCC:
Kompilatory GCC (w szczególności kompilator C) służą do kompilacji wielu jąder systemów operacyjnych, takich jak Linux, Hurd, jądro FreeBSD oraz wielu systemów eksperymentalnych.
Budowa i działanie GCC.
Program "gcc" (wywoływany podczas kompilacji np. z linii poleceń) odpowiada za przetworzenie argumentów, uruchomienie odpowiedniego kompilatora właściwego dla języka programowania w jakim zakodowano plik z kodem źródłowym, wykonanie programu asemblera dla tak otrzymanego wyniku oraz uruchomienie konsolidatora (linkera) w celu uzyskania pliku wykonywalnego.
Przykładowo dla pliku napisanego w C zostaną wykonane następujące programy: preprocesor cpp, kompilator cc1, asembler as oraz konsolidator collect2 (dostępny zazwyczaj jako program ld). Należy przy tym zwrócić uwagę, iż program as wchodzi w skład pakietu oprogramowania binutils. Również pliki nagłówkowe biblioteki standardowej języka C nie są częścią GCC.
Kompilator GCC składa się z 3 głównych części: front endu, middle endu oraz back endu.
front end.
Dla każdego języka programowania obsługiwanego przez GCC istnieje oddzielny front end. Dzięki temu względnie łatwo można dodawać kompilatory do nowych języków. Plik z kodem źródłowym poddawany jest procesowi analizy składniowej za pomocą ręcznie zakodowanego parsera. W efekcie tego działania powstaje reprezentacja programu zwana AST (ang. abstract syntax tree), która jest następnie przetwarzana do postaci w pełni niezależnej od pierwotnie użytego języka programowania GENERIC lub GIMPLE.
middle end.
Na tym etapie kompilator dokonuje optymalizacji kodu polegającej na:
Reprezentacja kodu zamieniana jest z postaci GIMPLE do innej zwanej RTL (ang. Register Transfer Language).
back end.
Ta część GCC odpowiada za wygenerowanie kodu asemblera przeznaczonego dla konkretnej architektury sprzętowej, a z niego kodu obiektowego. Ponieważ na tym etapie kompilator ma wiele informacji na temat docelowej platformy może dokonać kolejnych optymalizacji kodu np. uwzględniając budowę procesora, zestaw jego rozkazów czy specyficzne rozszerzenia.
Rozszerzenia języka C.
GCC zawiera wiele rozszerzeń ponad to, co określają standardy ANSI i ISO.
Są to m.in.:
Zmienne etykietowe.
void foo (int nr)
 static void * labels [] = {&amp;&amp;label0, &amp;&amp;label1};
 goto *labels [nr];
label0:
 printf("Code 0\n");
 return;
label1:
 printf("Code 1\n");
 return;
int main()
 foo(0);
 foo(1);
 return 0;
Inline Assembler w C/C++.
GCC umożliwia użycie asemblera w kodzie. Nie są to jednak pojedyncze instrukcje, tylko całe bloki razem ze zdefiniowanymi specjalnym systemem interfejsem między asemblerem a C/C++. Dzięki temu GCC może o wiele lepiej optymalizować kod.
W poniższym przykładzie program drukuje najpierw i=1, później i=2. GCC sam dokonuje alokacji rejestrów oraz przeniesienia między rejestrami a zmienną i na stosie.
int main()
 int i=0;
 asm("movl $1, %0" : "=g" (i));
 printf("i = %d\n", i);
 asm("addl $1, %0" : "+g" (i));
 printf("i = %d\n", i);
 return 0;
Makra o zmiennej liczbie argumentów.
Oprócz funkcji o zmiennej liczbie argumentów, deklarowanych np. int printf (const char *, ...); (gdzie ... oznacza zero lub więcej argumentów), GCC umożliwia tworzenie makr o zmiennej liczbie argumentów. Deklaruje je się tak samo jak funkcje:
Aby użyć listy argumentów, należy użyć definiowanego wtedy makra __VA_ARGS__:
Zamiast __VA_ARGS__, zostaną wklejone argumenty, w formie:
Jednak, jak widać, jeśli makru printf podamy zero argumentów, po drugim argumencie funkcji fprintf zostanie sam przecinek. Będzie to błędem składniowym. Aby tego uniknąć, wymyślono jeszcze jedno rozszerzenie
Dzięki zastosowaniu operatora sklejania ##, jeśli podane zostanie zero argumentów o zmiennej liczbie, nie będzie zbędnego przecinka.

</doc>
<doc id="1753" url="https://pl.wikipedia.org/wiki?curid=1753" title="Glacjologia">
Glacjologia

Glacjologia – gałąź hydrologii, zajmująca się badaniem lodowców, ich formami, właściwościami fizycznymi i chemicznymi oraz zachodzącymi w nich procesami. Badania glacjologów dotyczą również rozwoju mas lodowych, wzajemnego oddziaływania między lodowcami a środowiskiem, a także pomiaru masy lodowców i ich przemieszczania się.

</doc>
<doc id="1755" url="https://pl.wikipedia.org/wiki?curid=1755" title="Glin">
Glin

Glin (w technice: aluminium; Al, ) – pierwiastek chemiczny, metal z bloku p układu okresowego.
Jedynym izotopem stabilnym jest 27Al.
Glin jest trzecim najpowszechniej występującym pierwiastkiem w skorupie ziemskiej. Od jego symbolu (oraz symbolu krzemu) wywodzi się dawna nazwa najbardziej zewnętrznej warstwy globu – sial.
Historia.
Sole i tlenki glinu znane były od zarania dziejów. Uwodniony, mieszany siarczan tego pierwiastka, ałun, był używany jako środek antyseptyczny przez starożytnych Greków. Istnienie tego pierwiastka i nazwę zasugerował Louis-Bernard Guyton de Morveau w 1761 r. W 1807 podobną sugestię wyraził sir Humphry Davy, który zaproponował współczesną nazwę (aluminium). Istnieją kontrowersje na temat tego, kto pierwszy wyodrębnił ten pierwiastek w stanie czystym. Według jednych źródeł był to Friedrich Wöhler w 1827 r. wg innych Hans Christian Ørsted w 1825 r. Amerykanin Charles Martin Hall i Francuz Paul-Louis Toussaint Héroult w 1886 opracowali produkcję glinu na skalę przemysłową. Niezależnie od siebie opracowali metodę otrzymywania aluminium w procesie elektrolizy stopionej mieszaniny kriolitu i boksytu, obecnie znanym jako proces Halla-Heroulta.
Właściwości chemiczne.
Glin w stanie czystym szybko utlenia się na powietrzu, ulegając pasywacji. Pierwotnie pokrywa się warstwą o grubości kilku nm. Pod wpływem wilgoci zewnętrzna warstwa tej powłoki ulega częściowej hydrolizie i składa się z i . Natomiast wewnętrzną warstwę tworzy , częściowo uwodniony do Al(O)OH. Stanowi ona ścisłą powłokę chroniącą metal przed dalszą korozją w zwykłych warunkach. Jest ona odporna na działanie roztworów wodnych o pH 4–9.
Łatwo roztwarza się w rozcieńczonych roztworach mocnych kwasów (np. HCl) i zasad (np. NaOH lub KOH) wypierając wodór, np. :
Jego reaktywność wobec kwasu siarkowego opisywana jest różnie:
W stężonym kwasie azotowym ulega silnej pasywacji, dzięki czemu jest odporny na jego działanie i jest wykorzystywany w przemyśle do wytwarzania zbiorników do jego transportu. Z kolei z chlorowanymi węglowodorami reaguje gwałtownie. Także w wysokiej temperaturze () utlenia glin szybko.
W związkach występuje na III stopniu utlenienia, bardzo rzadko również na I i II.
Właściwości fizyczne.
Jest srebrzystobiałym metalem o niskiej gęstości, bardzo dobrej kowalności i dużej plastyczności. Jest łatwy w odlewaniu i obróbce, podczas której nie tworzy iskier. Wykazuje dobre przewodnictwo elektryczne, jest paramagnetyczny. W postaci czystej jego właściwości mechaniczne są słabe, które jednak można znacząco poprawić poprzez niewielkie ilości dodatków stopowych. Cienkie powłoki naparowanego glinu są trwałymi, bardzo dobrymi zwierciadłami dla światła widzialnego i promieniowania cieplnego (czysty glin odbija do 99% światła widzialnego i do 95% podczerwieni).
Zastosowanie.
Stopy aluminium.
Ze względu na swoje właściwości, takie jak mała gęstość i odporność na korozję, stopy glinu z miedzią i magnezem zwane duraluminium znalazły wiele zastosowań i są używane do wyrobu szerokiej grupy produktów – od części karoserii i silników samochodów, przez poszycia i elementy konstrukcyjne samolotów, po części statków kosmicznych. Tak zwane aluminium utwardzane dyspersyjnie jest wykorzystywane w produkcji koszulek elementów paliwowych i konstrukcyjnych rdzeni niektórych badawczych reaktorów jądrowych. Stopów aluminium z manganem i magnezem używa się do produkcji puszek do napojów (stopy 3004 lub 3104 na ścianki oraz 5182 na wieczka).
Aluminium słabo pochłania neutrony termiczne (ok. 20 fm²), przez co wykorzystywane jest w technice reaktorowej na koszulki elementów paliwowych.
Czysty glin.
Próżniowe napylenie glinu na powierzchnię szkła lub przezroczystych tworzyw sztucznych wykorzystywane jest do produkcji luster.
Pył glinowy.
Pył glinu używany jest w hutnictwie do otrzymywania metali z ich tlenków w procesie aluminotermii. Stosowana w tym procesie mieszanina glinu oraz tlenków metali jest znana pod nazwą termit. Termitu używa się do spawania rur i szyn kolejowych, a także do produkcji broni zapalającej. Jest także stosowany w materiałach wybuchowych np. amonal.
Jest też składnikiem farb metalicznych odpowiedzialnym za charakterystyczny połysk.
W syntezie chemicznej pył aluminium stosowany jest w reakcjach uwodorniania i jako zamiennik cynku w reakcji Reformatskiego.
Stosowany jest również w przemyśle spożywczym, jako barwnik metaliczny. Używany jest przy srebrnych dekoracjach ciast i tortów. Parlament Europejski uznał, że dodawanie aluminium powinno być zakazane, ponieważ istnieją przesłanki, że ma związek z chorobą Alzheimera, choć do tej pory nie udało się tego jednoznacznie udowodnić.
Folia aluminiowa.
Folie aluminiowe o różnej grubości stosowane są do pakowania (m.in. żywności) oraz do różnorodnych celów w technikach laboratoryjnych. Folia aluminiowa jest także wykorzystywana jako tzw. lustro lub ekran cieplny (odbijający promieniowanie podczerwone) do zapobiegania utraty ciepła. W tym celu stosuje się albo samą folię aluminiową (np. o grubości 0,05 mm), albo połączoną trwale z materiałem termoizolacyjnym.
Związki.
Najważniejsze związki glinu to tlenek glinu i amfoteryczny wodorotlenek glinu. Glin tworzy też wodorek, a tetrahydroglinian litu LiAlH4 jest powszechnie stosowanym w chemii organicznej silnym środkiem redukującym. Duże znaczenie przemysłowe mają też aluminoksany, a zwłaszcza MAO (metylowy aluminoksan), z którego produkuje się sita molekularne, oraz powszechnie wykorzystuje jako stałe podłoże dla wielu katalizatorów. Glina i kaolin, powszechnie wykorzystywane przy produkcji ceramiki, to złożone mieszaniny glino-krzemianów.
Znaczenie biologiczne.
Znaczenie dla fauny.
Wodorowęglan glinu Al(HCO3)3, ortofosforan glinu AlPO4 oraz krzemian glinu Al2(SiO3)3 są stosowane jako leki przy nadkwasocie.
Glin jest całkowicie asymilowany przez wątrobę i nie wydalany na zewnątrz, nie wykazując przy tym typowych cech toksycznych. Dlatego też większość źródeł zalicza go do metali obojętnych i z tego względu w pewnych określonych warunkach dopuszczony jest do użytkowania w gastronomii. Jednak w przypadku termicznej obróbki żywności, przy bezpośrednim kontakcie z wodą, glin wykazuje wysoką rozpuszczalność i w nadmiernych ilościach przenika do pożywienia. Z tego powodu w Polsce już w latach 80. systematycznie wycofywano z użytku naczynia aluminiowe i obecnie jego znaczenie jest marginalne. Nadmiar glinu nadmiernie obciąża wątrobę, a przyjmowanie dużych dawek tego pierwiastka, zwłaszcza w okresie dzieciństwa, skutkuje upośledzeniem funkcji i mniejszą wydajnością tego organu w późniejszych latach. Ponadto należy wspomnieć, że glin łatwo asymiluje się ze związkami wapnia łatwo przyswajalnego do związków trudno przyswajalnych. Dlatego też należy ograniczać jego spożycie w okresie wzrostu i rozwoju układu kostnego. Nie jest również wskazane, aby w nadmiarze spożywały go osoby w trakcie leczenia złamań i cierpiące na odwapnienie kości.
Znaczenie dla flory i gleb.
Glin, podobnie jak krzem, nie jest pierwiastkiem niezbędnym dla życia roślin. Mało tego, w dużych ilościach może być toksyczny zarówno dla roślin, jak i dla zwierząt zjadających roślinę zawierającą glin. Obecność glinu w glebie związana jest z obecnością jonów H+. Aby pozbyć się glinu z gleby, najczęściej stosuje się równolegle neutralizacje pH oraz sadzenie roślin, które pobierają glin z gruntu w większych ilościach.
Wytwarzanie.
W skorupie ziemskiej występuje w znacznych ilościach, 8,2% wagowo. Wytwarzane jest z boksytu w następujących po sobie procesach:

</doc>
<doc id="1757" url="https://pl.wikipedia.org/wiki?curid=1757" title="System informacji geograficznej">
System informacji geograficznej

System informacji geograficznej () – system informacyjny służący do wprowadzania, gromadzenia, przetwarzania oraz wizualizacji danych geograficznych, którego jedną z funkcji jest wspomaganie procesu decyzyjnego.
Każdy system GIS składa się z: bazy danych geograficznych, sprzętu komputerowego, oprogramowania oraz twórców i użytkowników GIS. W przypadku, gdy system informacji geograficznej gromadzi dane opracowane w formie mapy wielkoskalowej (tj. w skalach 1:5000 i większych), może być nazywany systemem informacji o terenie ().
Związki między geografią i GIS.
W książce „Introduction to human geography” prof. Stuart Sweeney przedstawia fundamentalne koncepcje w geografii:
GIS są efektem rewolucji w geografii dokonującej się w ciągu ostatnich kilkunastu lat, jak również oczywiście wynikiem gwałtownego rozwoju informatyki i metod zarządzania bazami danych (zbiorami informacji). Powstanie GIS jest wynikiem połączenia prac prowadzonych w różnych dziedzinach: geografii, kartografii, geodezji, informatyce, elektronice.
Systemy GIS znajdują praktyczne zastosowanie w wielu dziedzinach. Stąd bierze się różnorodność terminów określających systemy przetwarzające informacje geograficzne, jak system informacyjny bazy danych geograficznych, system danych geograficznych, system informacji przestrzennej. Każde z tych określeń przybliża w pewien sposób funkcje realizowane przez poszczególne systemy. W praktyce najczęściej spotykane są systemy specjalizowane, ukierunkowane na wąską grupę zastosowań, jednakże istnieją również wielozadaniowe GIS ogólnego zastosowania.
Struktura danych.
Pomimo różnorodności celów przetwarzania, we wszystkich GIS punktem wyjścia są dane związane z lokalizacją obiektów geograficznych. Opisy obiektów geograficznych zasadniczo składają się z dwóch części, zawierających dwa różne rodzaje danych:
Dane przestrzenne mogą zawierać informacje zarówno o kształcie i lokalizacji bezwzględnej poszczególnych obiektów w wybranym układzie odniesienia, jak również o ich rozmieszczeniu wzajemnym względem innych obiektów (topologia), te z kolei dzielą się na:
oraz
Dane opisowe (zwane także danymi nieprzestrzennymi lub atrybutowymi) – opisujące cechy ilościowe lub jakościowe obiektów geograficznych nie związane z ich umiejscowieniem w przestrzeni.
Uzupełnieniem informacji o obiektach świata rzeczywistego reprezentowanych w bazie danych jest symbolika, tj. graficzny opis postaci, w jakiej obiekty te mają być przedstawiane użytkownikowi.
Istotnym składnikiem GIS jest cyfrowa geograficzna baza danych. Zawiera ona opis poszczególnych obiektów geograficznych. Baza danych przestrzennych jest zazwyczaj ściśle zintegrowana z pozostałymi modułami funkcjonalnymi GIS, tzn. dostęp do niej jest możliwy tylko poprzez GIS. Alternatywnym rozwiązaniem jest usytuowanie jej na zewnątrz systemu. Wówczas stanowi ona odrębny system, komunikujący się z GIS poprzez dostęp do wspólnych zbiorów danych. Często stosowane są rozwiązania, w których dane o lokalizacji (rozszerzone o identyfikatory) obiektów geograficznych wraz z ich opisem graficznym przechowywane są przez wewnętrzną bazę danych, natomiast dane atrybutowe przez bazę zewnętrzną względem GIS. Rolę tę z powodzeniem może spełniać dowolny system zarządzania baz danych ogólnego zastosowania. Połączenie pomiędzy poszczególnymi typami danych opisujących konkretny obiekt geograficzny zapewnione jest dzięki istnieniu unikalnego identyfikatora, nadawanego obiektowi w procesie wprowadzania danych.
Struktura funkcjonalna GIS.
Geograficzny system informacyjny składa się z kilku grup programów (modułów) realizujących odrębne funkcje. Są to:
Moduły funkcjonalne typowego GIS.
Wprowadzanie danych.
Źródłem danych wejściowych dla GIS mogą być wszystkie informacje, zebrane w dowolnej formie: mapa, ortofotomapa (zdjęcie lotnicze), obraz satelitarny, ankiety statystyczne, dokumenty z pomiarów geodezyjnych i obserwacji terenowych, jak również wszelkiego rodzaju informacje zapisane w postaci cyfrowej.
Dane wprowadzać można za pomocą skanerów. Umożliwia natychmiastową wektoryzację danych, z drugiej zaś strony metoda ta jest wysoce czasochłonna, a co za tym idzie – droga. Innym rozwiązaniem jest wektoryzacja map i dokumentów po uprzednim ich zeskanowaniu. Wektoryzacja jednak jest procesem bardzo złożonym. Żadna metoda nie pozwala jednak na pełną automatyzację.
Atrybuty nieprzestrzenne w bazie danych geograficznych, to zbiór nazw i liczb, cech jakościowych lub ilościowych obiektów. Dla przykładu, droga może być wprowadzona do geograficznej bazy danych jako ciąg punktów (w przypadku wykorzystywania rastrowego formatu zapisu danych) bądź jako macierz (ciąg wektorów). Ponadto droga ta może charakteryzować się ustalonym sposobem prezentacji graficznej w systemie, określonym przykładowo przez takie cechy, jak kolor, grubość, rodzaj linii. Sposób prezentacji graficznej obiektu może w pewnym stopniu wyrażać część przyporządkowanych mu atrybutów nieprzestrzennych (i tak np. dla drogi – jej grubość lub kolor mogą odpowiadać gęstości ruchu na niej lub rodzajowi nawierzchni). Niemniej jednak w przypadku wprowadzenia większej liczby cech składających się na atrybuty nieprzestrzenne obiektu, wskazane jest ich wyodrębnienie. Pozwala to na uproszczenie przetwarzania danych.
Źródłem tych danych mogą być raporty i rocznik statystyczny, książka adresowa, słowniki nazw geograficznych itp. Akwizycja i rejestracja tych danych jest także procesem czasochłonnym; przyczyną tego jest ich duża ilość. W GIS często importuje się dane nieprzestrzenne z innych systemów.
Zarządzanie bazą danych.
Dostęp do zbiorów danych zapisanych w postaci cyfrowej zapewnia system zarządzania bazą danych. Oferuje on między innymi procedury dopisywania, wyszukiwania, aktualizacji i porządkowania danych. W zależności od przyjętego logicznego modelu danych, baza może mieć różną strukturę: hierarchiczną, sieciową, relacyjną, lub może być zorientowana obiektowo. Niezależnie jednak od sposobu konstrukcji bazy danych, jej zasadniczymi jednostkami są zazwyczaj rekordy składające się z pól. Rekordy te reprezentują poszczególne obiekty geograficzne lub kartograficzne, natomiast ich pola odpowiadają atrybutom. Głównym celem stawianym przed systemem zarządzania geograficzną bazą danych jest umożliwienie szybkiego dostępu do danych.
Można wyobrazić sobie, że zbiór obiektów jednej klasy tworzy podkład (warstwę) mapy. W ten sposób model mapy analogowej w zapisie cyfrowym wygląda, jak gdyby nałożono na siebie szereg folii, podkładów o różnym zakresie tematycznym. Rozdział obiektów na poszczególne warstwy dokonywany jest w procesie rejestracji danych przestrzennych.
Czym innym jest jednak fizyczna alokacja i rozmieszczenie plików w pamięci komputera, a czym innym jej pojęciowa logiczna konstrukcja ułatwiająca użytkownikowi dostęp do żądanych informacji. Przekładnię między tymi dwoma aspektami – technicznym i logicznym – zapewniają odpowiednie procedury zarządzania systemem bazy danych. Tak jak tematyczna organizacja bazy operuje pojęciem warstwy (podkładu, pokrycia) zawierającego obiekty, tak organizacja danych przestrzennych według lokalizacji operuje pojęciem regionu (strony). Baza danych jest wówczas dzielona na części, w których umieszczone są obiekty geograficzne sąsiadujące ze sobą na powierzchni Ziemi. Strony te mogą się dodatkowo dzielić.
Wyprowadzanie i obrazowanie danych.
Wyprowadzanie danych polega na ich przedstawianiu w formie zrozumiałej dla użytkownika lub w formie umożliwiającej ich transfer do innego systemu przetwarzania. Najczęściej wykorzystywaną formą prezentacji danych w geograficznych systemach informacyjnych jest ich wyświetlenie na monitorze w postaci graficznej przypominającej mapę. Użytkownik dokonuje wyboru obiektów, które mają zostać wyświetlone. Kryterium wyboru obiektów może być m.in. ich lokalizacja lub wartość atrybutów. W trakcie wyświetlania mapy cyfrowej możliwa jest zmiana sposobu prezentacji graficznej poszczególnych obiektów lub ich grup. Ponadto zazwyczaj dostępne są takie operacje, jak powiększanie i pomniejszanie fragmentu mapy, zmiana kolorów, zmiana usytuowania napisów opisujących obiekty na mapie. Do zaawansowanych technik wizualizacji zaliczyć należy możliwość prezentacji trójwymiarowej.
Procedury prezentacji umożliwiają w większości GIS uzyskanie trwałej kopii obrazowanych danych w postaci mapy. Najczęściej wykorzystywanymi w tym celu urządzeniami są ploter oraz drukarka (mozaikowa lub laserowa).
Zastosowania GIS.
Szeroką grupę zastosowań GIS stanowi wszelkiego typu ewidencja – gruntów, budynków, a ogólnie rzecz biorąc, wszelkiego rodzaju zasobów. Szczegółowe informacje tego typu wykorzystują urbaniści, geodeci, konstruktorzy. Zastosowanie warstwowej organizacji map umożliwia łatwą modyfikację jedynie wybranych obiektów, bez konieczności przerysowywania całej mapy. Komputerowa ewidencja własności gruntów z powodzeniem może zastąpić tradycyjną, prowadzoną za pomocą rejestrów i map geodezyjnych (katastralnych).
Inną grupę zastosowań stanowi wykorzystanie GIS do przetwarzania informacji o lokalizacji wszelkiego rodzaju zjawisk, zwłaszcza tych cechujących się znaczną zmiennością w czasie. GIS są bardzo wygodnym narzędziem w rejestracji poziomów emisji wszelkiego rodzaju zanieczyszczeń. Dla potrzeb monitoringu środowiska naturalnego akwizycja danych dla GIS może być prowadzona z wykorzystaniem zdalnych czujników i urządzeń pomiarowych sterowanych komputerowo. W tej grupie zastosowań mieści się również wykorzystanie GIS do analizy i obrazowania danych o charakterze statystycznym, takich jak np. zagrożenie przestępczością, występowanie chorób, struktura użytkowania gruntów.
GIS mogą również być bardzo wygodnym narzędziem do przetwarzania danych o infrastrukturze technicznej terenu, tj. o sieciach wodociągowych, gazowniczych, energetycznych, liniach komunikacyjnych. Dane tego typu wymagają częstych modyfikacji. Ponadto wymagana jest ich duża dokładność i aktualność. GIS umożliwiają spełnienie tych wymagań. Ten obszar zastosowań związany jest z technologią zwaną z , czyli w skrócie .

</doc>
